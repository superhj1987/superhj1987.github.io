<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: architecture | 后端技术杂谈]]></title>
  <link href="http://www.rowkey.me/blog/categories/architecture/atom.xml" rel="self"/>
  <link href="http://www.rowkey.me/"/>
  <updated>2022-06-07T06:54:58+00:00</updated>
  <id>http://www.rowkey.me/</id>
  <author>
    <name><![CDATA[Mr.H]]></name>
    <email><![CDATA[superhj1987@126.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[架构简明指南2022最新版]]></title>
    <link href="http://www.rowkey.me/blog/2022/06/04/arch-usage/"/>
    <updated>2022-06-04T11:29:34+00:00</updated>
    <id>http://www.rowkey.me/blog/2022/06/04/arch-usage</id>
    <content type="html"><![CDATA[<p>《Clean Architecture》一书中对于软件架构目的的解释：</p>

<blockquote><p>The goal of software architecture is to miminize the human resources required to build and maintain the required system.</p></blockquote>

<p>即：软件架构的目的就是将构建和维护系统需要的人力成本降到最低。</p>

<p>因此，可以得出架构设计的关键思维就是判断和取舍（程序设计的关键思维是逻辑和实现），即如何选择技术、组合技术使得需要的人力资源最少。</p>

<p>需要注意的一点是，脱离业务谈架构是不合理的，技术架构及其演进都是业务目标驱动的。</p>

<!--more-->


<h2>架构六步思考法</h2>

<blockquote><p>笔者对美团总架构师夏华夏一次分享提出的架构六步思考法的理解。</p></blockquote>

<p><img src="http://www.rowkey.me/post_images/arch-six-think.png" width="450"/></p>

<p>这里尤其需要注意的一点是在面对问题时，首先要试图将未知问题转化为已知问题，而不是创造新问题。</p>

<h2>架构手段</h2>

<p>架构的目的就是解决复杂度，主要包括高性能、高可用以及可扩展三方面。此外，分布式系统是架构工作中面对的典型复杂系统，对于其中常见的问题有一些常用应对手段。</p>

<h3>高可用</h3>

<blockquote><p>高可用=系统构建在多机=分布式系统</p></blockquote>

<ul>
<li>冗余：同城多活或者异地多活</li>
<li>降级：需要对各个关键节点建立降级预案。能够在超出预估流量时，保证大部分用户的服务是正常的。包括一个请求经过的多有节点。以轮训实现的直播系统为例：</li>
</ul>


<table>
<thead>
<tr>
<th>节点 </th>
<th> 手段 </th>
<th> 说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>客户端     </td>
<td> 拉取频率降级 </td>
<td> 服务端实时修改轮训时间</td>
</tr>
<tr>
<td>.        </td>
<td> 防雪崩策略 </td>
<td> 轮训出错后，自动指数级增大轮训时间</td>
</tr>
<tr>
<td>.        </td>
<td> 点赞消息合并 </td>
<td> 在客户端合并，减少服务端处理消息数目</td>
</tr>
<tr>
<td>Nginx      </td>
<td> 接口限流 </td>
<td> 针对接口，限制QPS</td>
</tr>
<tr>
<td>业务容器   </td>
<td> 拉取条数自动降级 </td>
<td> 可在线修改每种消息类型的返回条数</td>
</tr>
<tr>
<td>.        </td>
<td> 上行频率降级 </td>
<td> 可降级点赞、评论的频率限制</td>
</tr>
<tr>
<td>Kafka      </td>
<td>  容灾队列 </td>
<td> Kafka故障时写入容灾队列</td>
</tr>
<tr>
<td>消息处理BG </td>
<td> 自动丢弃消息 </td>
<td> 非重要消息可以视情况丢弃</td>
</tr>
<tr>
<td>.        </td>
<td> 处理延迟降级 </td>
<td> 根据延迟大小，采用加锁串行和不加锁并行处理策略</td>
</tr>
</tbody>
</table>


<ul>
<li>全链路业务监控：对请求链路上的所有结点都加入监控。包括客户端的APM、错误日志、JVM监控、QPS、状态码、延时、服务器资源的基础监控（带宽、CPU、内存、IO）等。示例如下：</li>
</ul>


<table>
<thead>
<tr>
<th>节点 </th>
<th> 监控内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>客户端     </td>
<td> APM</td>
</tr>
<tr>
<td>Nginx      </td>
<td> 错误码监控和报警；访问QPS、接口耗时分布、带宽</td>
</tr>
<tr>
<td>业务应用   </td>
<td> 错误日志、QPS、状态码、延时；JVM；依赖服务的QPS、状态码、延时</td>
</tr>
<tr>
<td>Kafka      </td>
<td> 消息堆积</td>
</tr>
<tr>
<td>消息处理BG </td>
<td> 错误日志；JVM；消息处理数目，消息处理延时</td>
</tr>
<tr>
<td>基础资源   </td>
<td> 带宽、CPU利用率、内存、磁盘</td>
</tr>
</tbody>
</table>


<h3>高性能</h3>

<blockquote><p>分布式系统的副产品</p></blockquote>

<ul>
<li>数据库集群</li>
<li>缓存架构</li>
<li>负载均衡</li>
<li>NoSQL：不局限于关系型数据库，在合适的场景下选择NoSQL数据库会带来性能的提升</li>
<li>异构索引：分区情况下，为提升未按拆分键进行查询的场景的性能，通过构建异构索引表。先通过查询异构索引表得到目标记录的主键，然后再根据记录主键查询，从而避免全库全表扫描</li>
</ul>


<p><strong>低延迟方案</strong>[系统响应性能提升]</p>

<ul>
<li><strong>异步</strong>：队列缓冲、异步请求。</li>
<li><strong>并发</strong>：利用多CPU多线程执行业务逻辑。</li>
<li><strong>就近原则</strong>：缓存、梯度存储。</li>
<li><strong>减少IO</strong>：合并细粒度接口为粗粒度接口、频繁的覆盖操作可以只做最后一次操作。这里一个需要特别注意的地方: <strong>代码中尽量避免在循环中调用外部服务，更好的做法是使用粗粒度批量接口在循环外面只进行一次请求。</strong></li>
<li><strong>分区</strong>：频繁访问的数据集规模保持在合理的范围。</li>
</ul>


<p><strong>高吞吐方案</strong></p>

<ul>
<li>分层调用：接入层、逻辑层、数据层，通过Proxy或者Router对逻辑层做集群管理</li>
<li>异步并发</li>
</ul>


<h3>可扩展</h3>

<ul>
<li>分层架构/简洁架构：单向依赖，职责清晰。</li>
<li>SOA：面向服务架构，服务粒度较大。</li>
<li>微内核：可插拔式架构，适用于客户端。</li>
<li>微服务：适用于复杂的大型系统，细粒度服务。</li>
</ul>


<p><strong>系统扩展思路</strong></p>

<ul>
<li>通过克隆扩展->高可用</li>
<li>通过拆分不同的东西来扩展->垂直扩展</li>
<li>拆分类似的东西来扩展->水平扩展</li>
</ul>


<h3>分布式系统</h3>

<ol>
<li><p>海量请求问题</p>

<p> 本质即如何达到高吞吐、低延迟、高可用，上文已经讲述。</p></li>
<li><p>大量服务器管理</p>

<ul>
<li>故障恢复和可扩展性：分布式目录服务、消息队列服务、分布式事务系统</li>
<li>运维便利性：自动部署工具、集中日志分析系统、全链路监控</li>
</ul>
</li>
<li><p>开发效率</p>

<ul>
<li>复杂通信编程：微服务框架、异步编程工具</li>
<li>大量模块分工：Iaas/Paas/Saas云服务</li>
</ul>
</li>
</ol>


<h2>架构原则</h2>

<p><img src="//images/blog_images/arch-spec.png" alt="" /></p>

<ul>
<li><strong>避免过度设计</strong>：简单的架构就是最好的架构。最简单的方案最容易实现和维护，也可以避免浪费资源。但方案中需要包括扩展。</li>
<li><strong>冗余设计</strong>：对服务、数据库的做结点冗余，保证服务的高可用。通过数据库主从模式、应用集群来实现。</li>
<li><strong>多活数据中心</strong>：为了容灾，从根本上保障应用的高可用性。需要构建多活的数据中心，以防止一个数据中心由于不可控因素出现故障后，引起整个系统的不可用。</li>
<li><strong>无状态设计</strong>：API、接口等的设计不能有前后依赖关系，一个资源不受其他资源改动的影响。无状态的系统才能更好地进行扩展。如果非得有状态，则要么客户端管理状态，要么服务端用分布式缓存管理状态。</li>
<li><strong>可回滚</strong>：对于任何业务尤其是关键业务，都具有恢复机制。可以使用基于日志的WAL、基于事件的Event sourcing等来实现可回滚。</li>
<li><strong>可禁用/自我保护</strong>：具有限流机制，当上游的流量超过自身的负载能力时，能够拒绝溢出的请求。可以通过手动开关或者自动开关（监测异常流量行为），在应用前端挡住流量。限流算法包括：令牌桶（支持突发流量）、漏桶（匀速流量）、计数器以及信号量（限制并发访问的数量）。此外永远不要信赖第三方服务的可靠性，依赖于第三方的功能务必有服务降级措施以及熔断管理，如：对于每一个网络操作，都需要设置超时时间，超过这个时间就放弃或者返回兜底响应。</li>
<li><strong>问题可追踪</strong>：当系统出现问题时，能够定位请求的轨迹、每一步的请求信息等。分布式链路追踪系统即解决的此方面的问题。</li>
<li><strong>可监控</strong>：可监控是保障系统能够稳定运行的关键。包括对业务逻辑的监控、应用进程的监控以及应用依赖的CPU、硬盘等系统资源的监控。每一个系统都需要做好这几个层面的监控。</li>
<li><strong>故障隔离</strong>：将系统依赖的资源(线程、CPU)和服务隔离开来能够使得某个服务的故障不会影响其他服务的调用。通过线程池或者分散部署结点可以对故障进行隔离。此外，为不同的用户提供单独的访问通道，不仅仅能够做故障隔离，也有利于做用户权限控制。</li>
<li><strong>成熟可控的技术选型</strong>：使用市面上主流、成熟、文档、支持资源多的技术，选择合适的而非最火的技术实现系统。如果面对自研和开源技术的选择，需要考虑契合度：如果功能需求契合度很高，那么选择开源即可；如果开源技术是需求的子集或者超集，那么要衡量吃透这个开源技术的成本和自研的成本那个高。</li>
<li><strong>梯级存储</strong>：内存->SSD硬盘->传统硬盘->磁带，可以根据数据的重要性和生命周期对数据进行分级存储。</li>
<li><strong>缓存设计</strong>：隔离请求与后端逻辑、存储，是就近原则的一种机制。包括客户端缓存（预先下发资源）、Nginx缓存、本地缓存以及分布式缓存。</li>
<li><strong>异步设计</strong>：对于调用方不关注结果或者允许结果延时返回的接口，采用队列进行异步响应能够很大程度提高系统性能；调用其他服务的时候不去等待服务方返回结果直接返回，同样能够提升系统响应性能。异步队列也是解决分布式事务的常用手段。</li>
<li><strong>前瞻性设计</strong>：根据行业经验和对业务量的预判，提前把可扩展性、后向兼容性、容量预警设计好。以防止超过系统容量后造成各种问题影响服务。</li>
<li><strong>水平扩展</strong>：相比起垂直扩展，能够通过堆机器解决问题是最优先考虑的问题，系统的负载能力也才能接近无限扩展。此外，基于<strong>云计算</strong>技术根据系统的负载自动调整容量能够在节省成本的同时保证服务的可用性。</li>
<li><strong>小步构建和发布</strong>：快速迭代项目，快速试错。不能有跨度时间过长的项目规划。</li>
<li><strong>自动化</strong>：打包、测试的自动化称为持续集成，部署的自动化称为持续部署。自动化机制是快速迭代和试错的基础保证。</li>
</ul>


<h2>技术选型原则</h2>

<ol>
<li>是否是生产级别、成熟的产品。<strong>生产级、可运维、可治理、成熟稳定</strong>的技术是首选。技术是有生命周期的，需要保持对新技术的敏感度，但切忌不要在技术的早期就开始使用。版本号、用的公司数量、文档完善度、运维支持能力（日志、命令行、控制台、故障检测恢复能力）都是成熟度的体现。</li>
<li>新技术的引入一定要坚持<strong>少即是多</strong>的原则，能不引入新技术尽量不要引入新技术。毕竟新技术的引入既有学习成本，又有维护成本。并且对于一个公司来说技术栈越多，那么学习和维护成本就越高，技术栈知识无法共享，技术体系无法建立，会严重影响研发效率和业务规模化能力。如果到了必须要引入的地步，一定要有严格的技术评审流程。</li>
<li>在引入一项新技术之前，要充分调研<strong>了解新技术的先决条件</strong>，不能盲目引入。对于确实需要引入但是目前还不满足先决条件的，需要做好阶段性规划，先打好基础，再适时引入新技术。</li>
<li><strong>不要盲目跟风大公司</strong>。很多时候适合大公司的技术并不适合小公司。毕竟大公司有充足的人力、资源和时间，这是小公司无法相比的。</li>
<li><strong>技术是带有文化特性的</strong>。在国外流行的技术，在国内未必流行。在选型的时候，尽量采用在国内有文化基础，已经落地开花的技术。此外，不同公司流行的技术文化也不相同，需要考虑自己公司的业务模式、已有技术生态和开发人员技能等。</li>
<li><strong>使用能掌控的技术</strong>。需要根据业务规模、团队规模和人员水平，经过综合评估对技术进行分析，以决定是否引入。</li>
<li>对于关键技术一定要找到<strong>合适的人</strong>来使用和研发。交给不合适的人，不仅无法解决问题，反而会制造更多的问题。</li>
<li><strong>抵制技术的宗教信仰</strong>，技术没有绝对的好坏优劣，只有合适与不合适、使用场景等。</li>
<li><strong>实践出真知</strong>。对新技术的引入一定要在仔细研究其文档的基础上跑样例、做压力测试，甚至通读其源码，经过一些试点项目验证后再逐渐扩大使用规模。</li>
<li>对于某些复杂、重量级技术的落地是有生命周期的，务必要<strong>通盘考虑，制定落地计划，分阶段推进</strong>技术的落地（引入、定制改造、小规模试点再到逐步扩大生产规模）。</li>
<li><strong>自研、开源、购买</strong>的选择。如果不是最擅长也提供不了差异化竞争优势的技术在成本允许的情况下直接采用开源或者购买即可；处在关键链路上的核心技术，一定要有定制或者自研的能力。此外，创业公司尽量采用开源技术或者购买云服务，而随着公司业务规模的增长，那么逐渐需要有定制和自研能力。</li>
</ol>


<p>此外，对于开源技术还需要注意：</p>

<ol>
<li><strong>是否是一线互联网公司落地产品</strong>。例如阿里开源的很多软件都是其在内部经过生产环境验证过的，形成了闭环的。而很多第三方软件服务商则仅仅是开源，并没有自身的需求，因此需要社区一起使用反馈从而形成闭环，这也就意味着你要和他一起踩坑形成闭环。</li>
<li><strong>是否有背书的大公司或者组织</strong>。例如Google一开始推出的K8S并不具有优势，然而由于Google的强大号召力和背书能力，因此促使大批用户使用从而形成了闭环，使得K8S目前基本垄断了容器PAAS市场。同样的，Apache下的开源项目绝大多数也是可以值得信赖的。</li>
<li><strong>开源社区是否活跃</strong>。Github上的stars的数量是一个重要指标，同时会参考其代码和文档更新频率（尤其是最近几年），这些指标直接反应开源产品的生命力。</li>
</ol>


<h2>数据设计原则</h2>

<ul>
<li>注意存储效率

<ul>
<li>减少事务</li>
<li>减少联表查询</li>
<li>适当使用索引</li>
<li>考虑使用缓存</li>
</ul>
</li>
<li>避免依赖于数据库的运算功能(函数、存储器、触发器等)，将负载放在更容易扩展的业务应用端</li>
<li>数据统计场景中，实时性要求较高的数据统计可以用Redis；非实时数据则可以使用单独表，通过队列异步运算或者定时计算更新数据。此外，对于一致性要求较高的统计数据，需要依靠事务或者定时校对机制保证准确性。</li>
<li>索引区分度法则：辨识度超过20%的属性，如果有查询需求，就应该建立索引。</li>
<li>对于数值型数据，可以使用保序压缩方式在保证顺序不变的前提下减少字符串长度。如：进行36进制转化即一种保序压缩方式。</li>
<li>大量数据的去重计数如果允许误差可以选择基数估计算法（Hyperhyperlog、Loglogcount）或者布隆过滤器。</li>
</ul>


<h2>系统稳定性原则</h2>

<ul>
<li>灰度发布，尽量减少影响的范围</li>
<li>慢查询review</li>
<li>防御式编程，不要相信任何人和服务：自动熔断，手动降级</li>
<li>SOP(标准操作流程)：工具化、自动化</li>
<li><strong>例行巡检！！！</strong>：DB、调用链、P90响应时间</li>
<li>容量规划：见下面“容量规划”部分</li>
</ul>


<p>SOP示例：</p>

<table>
<thead>
<tr>
<th>需求管理 </th>
<th> 项目开发 </th>
<th>  测试  </th>
<th>  发布上线 </th>
<th> 监控报警 </th>
<th> 故障处理</th>
</tr>
</thead>
<tbody>
<tr>
<td>Task管理、技术评审、<strong>上下游依赖变化review</strong> </td>
<td> 分支管理、交叉代码Review、代码规范、日志规范、代码静态检查 </td>
<td> 单元测试、冒烟测试、回归测试、办公室测试、线上压测 </td>
<td> 线上发布、验证业务效果 </td>
<td> 业务指标监控、系统监控dashboard </td>
<td> 第一时间向上级反馈、及时周知业务方：问题、影响范围、解决方案、预计恢复时间、线上服务降级</td>
</tr>
</tbody>
</table>


<h2>系统容量规划</h2>

<p>需要对系统/关键模块做好评估、量化，以防止超出容量时不至于压垮服务器，仍然能够服务于大部分用户。</p>

<p><img src="//post_images/capacity-plan.png" alt="" /></p>

<ol>
<li>根据流量模型、历史数据、预测算法预估未来某一个时间点的业务量：QPS、每日数据量等。</li>
<li>评估单点最大承载量（数据库的单点承载数据量、应用服务器的单点承载并发量）【通过性能测试】，根据业务量计算需要部署的结点数目，做1.5倍部署（DID原则）。</li>
<li>性能压测验证整个系统的负载能力。</li>
<li>设计达到容量预估值时的预警、限流、快速恢复措施以及后续扩展方案。</li>
</ol>


<p>PS: 在容量预估中，机器数目的计算遵循DID原则：20倍设计、3倍实施/实现、1.5倍部署。即需要部署1.5倍的可承载预估业务流量的机器数目。</p>

<h2>架构隐患分析</h2>

<h3>FEMA方法分析表格</h3>

<ul>
<li>功能点：用户角度</li>
<li>故障模式：系统会出现的故障，量化描述</li>
<li>故障影响：功能点会受到什么影响</li>
<li>严重程度：对业务的影响程度。致命/高/中/低/无</li>
<li>故障原因：故障出现的原因</li>
<li>故障概率：某个具体故障原因发生的概率</li>
<li>风险程度：综合考虑严重程度和故障概率，严重程度 × 故障概率</li>
<li>已有措施：故障发生时的应对措施。包括检测告警、容错、自恢复等。</li>
<li>规避措施：降低故障发生概率而做的事情，包括技术手段和管理手段。</li>
<li>解决措施：此问题的彻底解决办法</li>
<li>后续规划：后续改进计划，包括技术手段、管理手段，可以是规避措施，也可以是解决措施。风险程度越高的隐患解决的优先级越高</li>
</ul>


<h3>FEMA方法分析示例</h3>

<table>
<thead>
<tr>
<th>功能点 </th>
<th> 故障模式 </th>
<th> 故障影响 </th>
<th> 严重程度 </th>
<th> 故障原因 </th>
<th> 故障概率 </th>
<th> 风险程度 </th>
<th> 已有措施 </th>
<th> 规避措施 </th>
<th> 解决措施 </th>
<th> 后续规划</th>
</tr>
</thead>
<tbody>
<tr>
<td>登录 </td>
<td> 用户中心MySQL响应时间超过5秒 </td>
<td> 用户登录缓慢 </td>
<td> 高 </td>
<td> MySQL中有慢查询 </td>
<td> 高 </td>
<td> 高 </td>
<td> 慢查询监测 </td>
<td> 杀死慢查询进程；重启MySQL </td>
<td> 无 </td>
<td> 优化慢查询语句</td>
</tr>
<tr>
<td>刷新资讯列表 </td>
<td> Redis无法访问 </td>
<td> 当Redis无法访问，那么基于Redis的画像、内容等都无法响应，会影响100%的用户 </td>
<td> 高 </td>
<td> Redis服务宕机 </td>
<td> 低 </td>
<td> 中 </td>
<td> 无 </td>
<td> 无 </td>
<td> 无 </td>
<td> 依赖于UCloud的Redis服务会有风险，需要自建Redis分摊风险</td>
</tr>
</tbody>
</table>


<h2>架构重构的原则</h2>

<p>一个系统的架构是随着业务而不断演化的，因此不可避免地会留下很多技术债。如果一味地不去管，那么总有一天技术债会爆发出来造成意想不到的破坏。因此很多时候对架构的重构是必须的。其需要遵循的原则如下：</p>

<ul>
<li>确定重构的目的和必要性：为了业务需要；有无其他备选方案</li>
<li>定义“重构完成”的界限</li>
<li>渐进式重构</li>
<li>确定当前的架构状态</li>
<li>不要忽略数据</li>
<li>管理好技术债务</li>
<li>远离那些虚荣的东西</li>
<li>做好面对压力的准备</li>
<li>了解业务</li>
<li>做好面对非技术因素的准备</li>
<li>能够掌握代码质量</li>
</ul>


<h2>架构改造实施模式</h2>

<ul>
<li>拆迁者模式：根据业务需求，对架构进行重新设计，也就是一次性重写所有代码。此种模式成本大，不能很好地支撑持续交付，架构改造风险也较大。</li>
<li>绞杀者模式：保持遗留系统不变，新的功能重新开发为新的系统/服务，逐渐替代掉遗留系统。适用于庞大难以更改的遗留系统。</li>
<li>修缮者模式：将旧的待改造的部分/模块进行隔离（通过增加中间层解决，中间层可以是抽象类、接口等），通过迭代，在原有遗留系统内部对其进行逐步改造，改造的同时要保证与其他部分仍能协同工作。</li>
</ul>


<h2>在线数据迁移</h2>

<p>在线数据迁移指将正在提供线上服务的数据，从一个地方迁移到另一个地方，迁移过程中服务不受影响。这种场景是系统演进过程中都会出现的问题，包括业务演进的需要和性能扩展的需要。典型的步骤如下：</p>

<p><img src="//post_images/data-mig.png" alt="" /></p>

<ol>
<li>上线双写：在业务系统里写代码，同时向新旧数据存储写入数据。此步骤完成后，需要进行一致性验证，包括存储维度和业务维度。前者指对比原数据存储和新数据存储中的数据对比，后者指从用户看到的数据维度进行对比。</li>
<li>历史数据迁移：将历史数据从旧存储迁移到新存储。包括离线和在线两种。离线是编写批量处理程序或者依靠数据存储的同步机制从旧存储查询历史数据（开启双写以前的数据）插入到新存储中。在线指的是依赖数据存储的同步机制在线同步数据，如MySQL的binlog、MongoDB的OpLog。此过程，建议在部分数据迁移后就进行一致性验证，通过后再全量数据迁移。</li>
<li>切读：通过灰度的方式逐渐切换请求到新系统上，灰度可以通过在代码中埋入开关来逐步的放大读新系统的请求量。一般的流程：预发布/Tcpcopy环境(验证代码运行正常)->办公室环境/线上环境uid白名单（内部用户，验证功能正常）->线上环境百分比0.1%、1%、%10%（进一步验证功能正常以及性能和资源压力）->线上环境全量。此过程建议持续一到两周。</li>
<li>清理：数据迁移验证通过后，清理业务系统的双写代码和开关代码等逻辑代码、旧存储的数据和配套系统以及旧的资源等。</li>
</ol>


<p>某些情况下，可以先做历史数据搬迁，然后再写入新数据。需要谨慎的处理搬迁这段时间里产生的新数据，一般使用 queue 缓存写入的方式，称为“追数据”。</p>

<p>此外，如果是单一功能的在线数据迁移，可以参考Redis Cluster数据重分配的实现机制。</p>

<ol>
<li>离线程序迁移数据，并维护数据迁移状态：未迁移、迁移中、迁移完成。</li>
<li>业务代码做统一控制。发生数据读写时，如果数据状态是迁移中，那么阻塞等待至迁移完毕再执行后续操作；如果数据的状态是迁移完成或者是新数据则直接执行后续逻辑；如果数据状态是未迁移，那么就主动发起迁移或者等待离线迁移完成。</li>
</ol>


<h2>其他</h2>

<ul>
<li>系统设计流程四部走：识别复杂度->设计备选方案->评估和选择备选方案->详细方案设计</li>
<li>讨论技术方案时，以是否合理为依据，而不要以工作量少为依据。</li>
<li>对遗留系统进行垂直分离成本比较大时，可以考虑直接clone项目部署单独的集群，用服务地址分隔开，不同的接口走不同的服务地址。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[中台简谈]]></title>
    <link href="http://www.rowkey.me/blog/2019/11/23/middle-talk/"/>
    <updated>2019-11-23T11:29:34+00:00</updated>
    <id>http://www.rowkey.me/blog/2019/11/23/middle-talk</id>
    <content type="html"><![CDATA[<blockquote><p>面对新事物，先接纳，再判断。不要轻易就否定，即使经过自己的思考后确实没啥价值，这期间的思考过程也是一种知识梳理和思维锻炼。</p></blockquote>

<p>2019年技术圈最火的一个词非“中台”莫属了。联想到公司已经在持续做的平台化，其实会让人感到混乱。平台和中台有啥区别？有了中台，那么前台和后台又指的什么？本文是自己在调研中台概念中沉淀出来的一些思考。</p>

<!--more-->


<h2>中台是什么</h2>

<p>一种特殊形式的平台，抽象业务/系统的共性，支撑业务快速发展，是<strong>企业级共享能力平台</strong>。其核心在于对业务、数据、技术的抽象，对服务能力进行复用。解决重复开发、数据分散、试错成本高的问题。中台突出的是规划控制和协调的能力，前台强调的是创新和灵活多变。</p>

<ul>
<li>业务中台：多个前台业务应用共享的需求，关注如何支撑在线主营业务。一般来说业务中台由多个中心系统组成。</li>
<li>数据中台: 专用的数据处理平台，用技术连接大数据计算存储能力，用业务连接数据应用场景能力的平台。</li>
<li>技术中台：整合和包装了云基础设施以及在其上的各种技术中间件，并在此基础上建设和封装了简单易用的能力接口，提供了基础设施重用的能力。</li>
<li>研发中台：关注应用研发效率的管理平台，为应用的开发提供了流程、质量管控和持续交付的能力。</li>
<li>移动中台：平台级的移动端开发支持</li>
<li>AI中台：数据中台之上，模型的训练、发布，智能服务的构建自动化，统一的元数据管理体系，模型的全生命周期管理。</li>
<li>组织中台：与中台技术架构相匹配的组织架构</li>
</ul>


<p>众多的可复用能力只是中台的形，<strong>核心的业务数据和业务流程</strong>才是中台存在的本质。</p>

<h2>为什么要有中台</h2>

<p><strong>传统的烟囱式架构面临的问题</strong></p>

<ul>
<li>重复性建设对人力物力的浪费</li>
<li>系统间交互的集成和协作成本昂贵</li>
<li>不利于业务沉淀和持续发展</li>
</ul>


<p><strong>共享服务带来的优势</strong></p>

<ul>
<li>提高研发效能，赋予业务快速创新和试错能力</li>
<li>打通数据，真正发挥大数据的威力，共享数据价值</li>
<li>中台组织结构提升组织效能</li>
</ul>


<h2>怎么实现中台战略</h2>

<h3>思路的改变</h3>

<ul>
<li>提升自己的研发效率->提升别人的研发效率</li>
<li>从代码->需求，到代码->组件->需求，到代码->组件->可配置->需求</li>
<li>业务逻辑和平台逻辑分离，业务逻辑和业务逻辑隔离</li>
<li>集中配置，分布式运行</li>
</ul>


<h3>总体架构</h3>

<p><img src="//post_images/middle-office-arch.png" alt="" /></p>

<h3>建设思路</h3>

<ol>
<li><p>中台化改造</p>

<blockquote><p>对已有平台的中台化改造</p></blockquote>

<ul>
<li>平台不断对于自身治理演进、打破技术边界、逐渐拥抱业务、容纳业务、具备更强的业务属性的过程。</li>
<li>通过业务抽象以及可配置化和白屏化（给平台产品做一个配置界面实现自助式服务，没有UI要求，一般是一个白页面加一些配置项）的改造升级</li>
<li>技术平台->技术中台：对于技术平台的治理、安全、可用性和自助式的产品化包装，打造自助服务平台，关注业务的用户使用体验，让业务可以更快速更方便体验更好的使用企业内部的技术能力</li>
</ul>
</li>
<li><p>中台化：利用平台化的思维和手段梳理、识别、沉淀与复用企业级核心能力的过程。根据业务演进逐渐积累而成，<strong>分阶段逐步实施</strong>。多于一个前台业务需要某一种能力，那么此能力即可沉淀为中台能力。切忌大而全的建设中台。</p>

<ul>
<li><strong>资源集中管理->能力抽象->灵活性</strong></li>
<li><strong>共享服务：普通的服务能力->组件化服务，并提供良好的服务治理支持</strong>

<ul>
<li>找到一个合适的服务化对象：API as service，存量API升级成服务化平台的组件服务</li>
<li>建设对象服务化的基础设施：Product as Service，封装API服务</li>
<li>服务化实施阶段: Solution as Service</li>
<li>增强服务和基础设施实现服务的精细治理</li>
</ul>
</li>
</ul>
</li>
<li><p>运营</p>

<ul>
<li>运营前置：制定迭代计划及接入计划。中台产品推广、前台（用户）接入计划以及接入后的运营支持</li>
<li>根据用户分层制定SLA：不同的需求响应机制、不同的沟通管理机制、不同的服务质量控制机制、不同的问题处理及升级机制</li>
</ul>
</li>
<li><p>演进</p>

<ul>
<li>各种中台的逐渐建设</li>
<li>共享服务中心的不断增加</li>
</ul>
</li>
</ol>


<h3>建设要点</h3>

<ul>
<li>在“工具”与“完全为业务服务”之间寻找平衡点，既需要满足业务的需求，又不能过度参与业务。</li>
<li>重视中台的运营、持续治理以及演进</li>
<li>拆分整体应用形成业务组件：抽象程度越高，中台对业务的适配度越高。</li>
<li>可配置，自助白屏化</li>
<li>足够灵活的扩展点，支持定制化扩展</li>
<li>服务文档化</li>
<li>开放体系：对内对外</li>
</ul>


<h2>参考资料</h2>

<ul>
<li>《中台战略》</li>
<li>《企业IT架构转型之道》</li>
<li>极客时间《说透中台》</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据传输之RESTful]]></title>
    <link href="http://www.rowkey.me/blog/2019/09/28/restful/"/>
    <updated>2019-09-28T11:29:34+00:00</updated>
    <id>http://www.rowkey.me/blog/2019/09/28/restful</id>
    <content type="html"><![CDATA[<p>REST，全称表现层状态转移（Representational State Transfer）, 指的是资源在网络中以某种表现形式进行状态转移，是一种架构风格。其描述的是在网络中Client和Server的一种交互形式。简单来说就是用HTTP URL来定位资源，用HTTP的各种method来描述操作。其关键的三个概念如下：</p>

<ul>
<li>Resource: 资源，主要指的是数据。</li>
<li>Representational：数据的表现形式，如JSON、XML、HTML等。</li>
<li>State Transfer：状态变化, 通过HTTP method来描述。</li>
</ul>


<p>REST经常被用来规范API的设计以及数据传输的格式，可以统一给各种客户端提供接口,包括Web、iOS、Android和其他的服务。REST不需要显式的前端页面，只需要按照格式返回数据即可。符合REST风格的API称为RESTful API，符合RESTFul规范的架构称为RESTful架构。如下图所示：</p>

<p><img src="//post_images/restful/restful.png" alt="" /></p>

<!--more-->


<h2>一. 操作</h2>

<p>RESTful是基于HTTP协议的，其主要依赖于HTTP协议的几种method来表示CRUD（create、read、update和delete,即数据的增删查改）操作：</p>

<ul>
<li>GET: 从服务器上获取资源</li>
<li>POST: 创建新的资源</li>
<li>PUT： 更新服务器资源</li>
<li>DELETE： 删除服务器资源</li>
</ul>


<p>这里需要注意两点：</p>

<ul>
<li>GET、PUT和DELETE应该是幂等的，即相同的数据和参数下，执行一次或多次产生的效果是一样的。</li>
<li>对于POST和PUT操作，应该返回最新的资源，删除操作则一般不必要。</li>
<li>所有的操作都是无状态的，即所有的资源，都可以通过URL定位，这个定位与其他资源无关，也不会因为其他资源的变化而改变。</li>
</ul>


<p>除了上述方法之外，还有一个PATCH方法也用于更新资源的部分属性，但并用的并不多，用POST即可。</p>

<p>此外，HTTP 1.1的几个头部也是应该注意的：</p>

<ul>
<li>Accept: 客户端要求服务器返回什么样表现形式的数据。RESTFul API需要根据此头部返回合适的数据。</li>
<li>If-Match: 在对资源做更新和删除操作时，客户端提供If-Match头，值为服务端上次对此资源返回的Etag, 服务端对比Etag如果一致才做更新和删除，否则返回412。</li>
<li>If-None-Match: 和If-Match相反，如果不匹配上次的Etag才返回数据，匹配的话则返回304，多用于Get请求。</li>
<li>If-Modified-Since：值为时间，如果请求的部分在指定时间之后被修改则请求成功，未被修改则返回304，多用于Get请求。</li>
</ul>


<h2>二. 返回码</h2>

<p>HTTP本身已经提供了很多StatusCode来表示各种状态。RESTFul接口需要遵循这些定义，返回合适的状态码和数据。当然，如果是内部使用，统一返回200，在返回数据里自定义一套status code也是可以的。</p>

<p>HTTP的状态码大体分为几个区间：</p>

<ul>
<li>2XX：请求正常处理并返回。</li>
<li>3XX：重定向，请求的资源位置发生变化。</li>
<li>4XX：客户端发送的请求有错误。</li>
<li>5XX：服务器端错误。</li>
</ul>


<p>在自己设计返回码的时候最好也遵循此范围设计，以下是其中几个常用的状态码：</p>

<ul>
<li>200：表示请求成功。</li>
<li>301：资源已经永久移动到新的地址，新的URL会在响应头中返回。</li>
<li>302：资源临时被移动到新的地址，新的URL会在响应头中返回。</li>
<li>304：表明资源未改变。主要配合请求头中的If-None-Match和If-Modified-Since使用。</li>
<li>400：错误请求，表示请求中有语法错误。</li>
<li>401：请求的资源需要认证，请求没有提供认证信息或者认证错误。</li>
<li>403：资源被禁止访问。</li>
<li>404：资源不存在。</li>
<li>502：错误的网关，通常是作为代理的服务器无法收到远程服务器的正确响应。</li>
<li>503：服务不可用。</li>
</ul>


<h2>三. 资源</h2>

<p>资源是RESTful API的核心，其以URI（统一资源标识符）标识，而URL则不仅能够标识一个资源，还能够定位资源。RESTful中使用HTTP URL标识并定位一个资源。原则上只使用名词来指定资源，而且推荐使用复数。以对记事的CRUD API的设计为例：</p>

<ul>
<li>获取所有记事列表：GET /api/notes?page=1&amp;per_page=20</li>
<li>获取某人的所有记事列表：GET /api/users/{uid}/notes</li>
<li>获取标记为星的记事：GET /api/users/{uid}/notes?star=1</li>
<li>创建记事：POST /api/notes</li>
<li>删除某一个记事：DELET /api/notes/{note_id}</li>
<li>更新某一个记事：PUT /api/notes/{note_id}</li>
</ul>


<p>可知：</p>

<ul>
<li>资源分为单个资源和资源集合，尽量使用复数来表示资源，单个资源通过添加ID等标识符来表示。</li>
<li>资源使用嵌套结构，类似于目录路径的方式，可以体现出之间的关系。</li>
<li>一个资源可以有不同的URL，如上可以获取所有的记事列表，也可以获取某人的所有记事列表。</li>
<li>对于GET方法，一定不能设计为可以改变资源的操作。如get /api/deleteNote?id=xx。</li>
<li>URL是对大小写敏感的，尽量使用小写字母，单词间用下划线连接。</li>
<li>使用Query参数来控制返回结果，如上面返回星标记事的接口。此外，像排序方向、排序使用的字段都是可以放在query参数中的。</li>
<li>分页参数使用Query参数（page、per_page）控制，在返回数据中返回当前页、下一页、上一页、总页数等分页相关信息。</li>
</ul>


<p>如果需要区分版本号，可以放在路径中，如/api/v2/**，也可以放在header的Accept字段或者Query参数中:</p>

<pre><code>Accept: version=2.0;...
</code></pre>

<p>对于一些很难设计为CRUD操作的URL, 如登录、送礼物等，有以下处理方式：</p>

<ul>
<li>使用POST，如POST /api/login。</li>
<li>把动作转换成资源: 登录就是创建了一个Session或者Token，那么就可以设计为 POST /api/sessions。</li>
</ul>


<p>此外，对于数据的提交格式和返回格式，目前以JSON格式为主，其可读性、紧凑性、多语言支持都较好；数据提交的方式也应该使用application/JSON的内容格式并在body里放置JSON数据。</p>

<pre><code>...
Content-type: application/json
Accept: application/json
...

{
    'title':'xxx',
    'content':'xxx'
    ...
}
</code></pre>

<h2>四. 安全性</h2>

<p>HTTP本身是对数据不做任何安全处理的，因此建议首先从根本上使用HTTPS加强数据的安全性。此外，这里的安全性还要保证数据的完整性；保证接口的授权访问，保证接口只提供给授权过的应用访问以及过滤掉不必要的请求；保证数据的授权访问，只允许资源拥有者删除、更新自己的资源。</p>

<h3>数据的完整性</h3>

<p>数据完整性主要是指在对数据进行修改时，要保证要修改的数据和服务器数据是一致的。可以通过Etag这个HTTP中的头部字段来解决。</p>

<p>Etag表示的是资源的唯一版本号, 请求资源时，RESTful api应该把资源数据以及资源的Etag一起返回。api请求方修改资源时应该提交If-Match头，这样服务器通过对比Etag可以防止数据被错误修改，类似于并发中CAS的原理。但是要绝对保证数据的完整性，还得需要配合严格的并发控制才能做到。</p>

<h3>接口访问控制</h3>

<p>接口访问控制可以保证接口的授权访问，拒绝不合法的请求。可以通过以下几种方式：</p>

<ul>
<li>在Request headers中添加特殊的标识符，如果不含有此header的请求直接拒绝。这可以做简单的接口访问控制。</li>
<li>过滤Requst query和body, 做白名单验证，即只允许出现哪些参数，如果有非法参数，可以抛弃或者直接拒绝请求。</li>
</ul>


<p>上面只是比较简单的接口访问控制策略，无法彻底拒绝未授权的请求。我们可以通过为每一个授权应用分配app_secret（私有的，不公开），访问时对请求进行签名验证的方式实现更为严格的接口访问控制，这种方法也叫做HMAC。请求签名生成的一个例子如下：</p>

<pre><code>app_sign = MD5(METHOD &amp; PATH &amp; timestamp &amp; app_secret)
</code></pre>

<p>其中，METHOD指的是此次请求的方法，PATH指的URL中的path部分，timestamp是请求时间戳，app_secret是分配请求方的私钥，此外还有一个分配给请求方的app_id。这样，app_id、timestamp、app_sign随着请求一起发送（可以作为query参数也可以作为header），服务器接收到请求后使用同样的算法计算出app_sign进行对比，如果相同则正常请求，否则返回401 Unauthorized。由此既可以保证接口的授权访问，还能够基于时间戳防止重放攻击。当然，app_sign的生成算法可以加入更多的因子，如request_body、query等。但需要注意的是这个算法越复杂，对接口的性能影响就越大，需要做权衡。</p>

<h3>数据的授权访问-OAuth</h3>

<p>数据的授权访问其实也是接口访问控制的一部分。主要关注点在于对资源的操作权限做控制。基于HTTP做授权访问的核心就是验证一个请求是否是合法用户发起的，主要的有HTTP Basic Auth、OAuth。其中Basic Auth会把用户的用户名和密码直接暴露在网络中并不安全，因此RESTful api主要使用OAuth做数据的授权访问控制。</p>

<p>OAuth2.0的验证流程如下图所示：</p>

<p><img src="//post_images/restful/oauth.png" alt="" /></p>

<ul>
<li>得到授权码code。</li>
<li>使用授权码换取access_token和refesh_token，通常refresh_token比access_token有效期长。</li>
<li>使用access_token获取用户openid。</li>
<li>使用access_token和用户openid调用用户授权接口。</li>
<li>使用refresh_token获取新的access_token。</li>
</ul>


<p>当然，如果是提供给内部应用的API，可以做适当简化，比如用户登录直接返回access_token，凭借此access_token调用授权接口即可。</p>

<h2>五. 限流</h2>

<p>RESTful api应该有限流机制，否则会造成API被滥用甚至被DDOS攻击。可以根据不同的授权访问做不同的限流，以减少服务器压力。</p>

<p>限流的情况可以通过下面几个头部字段返回给请求方：</p>

<ul>
<li>X-RateLimit-Limit: 用户每个小时允许发送请求的最大值。</li>
<li>X-RateLimit-Remaining：当前时间窗口剩下的可用请求数目。</li>
<li>X-RateLimit-Rest: 时间窗口重置的时候，到这个时间点可用的请求数量就会变成 X-RateLimit-Limit 的值。</li>
</ul>


<p>对于未登录的用户根据IP或者设备ID来限流，对于登录用户根据用户标识。对于超过流量的请求，返回403 forbiden或者429 Too many requests都可以。</p>

<h2>六. 超文本API</h2>

<p>RESTful还有一个非常关键的特性就是超文本API（Hypermedia API），指的是服务器需要在每一个API接口的返回结果中都要提供与下一步操作相关的资源链接, 客户端借助这些实现表现层状态转移。这种设计也被称为 HATEOAS（Hypermedia as the Engine of Application State）。</p>

<p>除此之外，这样做还能够让客户端和服务端解耦，客户端只需要依次遍历返回结果中的超链接就能完成一系列业务逻辑；当服务端做了业务逻辑改动后，也只需要修改服务器返回的资源链接即可。</p>

<h2>七. 编写文档</h2>

<p>RESTful API一般是对接第三方的，因此，文档说明是非常必要的。因此对每一个接口都详细的说明参数含义、数据返回格式和字段意义并举出实际的例子都是非常关键的。</p>

<p>Java Web开发中，我们可以使用Swagger UI + Spring Fox来基于注释生成RESTful API文档。</p>

<h2>八. RESTful API实现</h2>

<p>Spring MVC、Jersey、Play Framework等主流的Web开发框架都支持RESTful的接口编写。这里我们以Spring MVC为例。</p>

<pre><code>@RequestMapping(value = "/api/notes/{noteId}", method = RequestMethod.GET, headers = "Accept=application/json")
@ResponseBody
public UserNote getUserNoteInfo(@PathVariable long noteId) {

   return ...;
}
</code></pre>

<p>此外，OAuth的实现可以使用Spring Security OAuth, 其基于Spring Secutiry实现了OAuth服务。不过，Spring Security OAuth使用稍显复杂，完全可按照OAuth2.0的流程使用Spring MVC + Redis进行实现。</p>

<blockquote><p>本文节选自《Java工程师修炼之道》一书。</p></blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[微服务杂谈]]></title>
    <link href="http://www.rowkey.me/blog/2019/05/30/msa/"/>
    <updated>2019-05-30T11:29:34+00:00</updated>
    <id>http://www.rowkey.me/blog/2019/05/30/msa</id>
    <content type="html"><![CDATA[<p>这几年在Java工程师招聘时，会看到很多人的简历都写着使用了Spring Cloud做微服务实现，使用Docker做自动化部署，并且也会把这些做为自己的亮点。而比较有趣的这其中以小公司出来的人为绝大多数，大的公司出来的人简历上倒是很少提这些东西。</p>

<p>对于我自己来说，从15年就开始关注这一块，看过马丁.福勒最开始的关于微服务的论文、也看过不少对微服务的论证的英文文章和书，也研究过Spring Cloud、Sofa等开源实现以及Service mesh。考虑到我们公司研发团队人力不足、基础设施不完善，当初是没有推行微服务的。但随着看到上述的那种简历越来越多，有时候我也会疑问：难道真的不用微服务就落后了吗？公司的同事如果不掌握这些就真的没有竞争力了吗。而随着最近公司业务的逐步提升，研发人员越来越多，借着在梳理公司的微服务落地计划时，也梳理了一下微服务的相关知识点，也是本文的主要内容。</p>

<p>开篇之前先声明我对微服务的几点态度:</p>

<blockquote><ol>
<li>架构模式有很多，微服务不是唯一的选择也不是什么银弹。国内很多中小公司引入微服务都是在盲目追新，也能看出做此种技术选型的工程师基础架构素质的不足。</li>
<li>“你必须长的足够高才能使用微服务”。微服务基础设施，尤其是容器技术、自动化部署、自动化测试这些不完备，微服务形同虚设，不会带来什么质的提升。</li>
<li>微服务架构的关键不在于具体的实现，而在于如何合理地划分服务边界以及组织架构是否相匹配。不考虑研发团队的规模和组成就盲目上微服务是不良的技术选型。</li>
<li>Spring Boot是Spring全家桶的上层封装，并不是什么崭新的技术，也不是什么值得觉得成为自己杀手锏的技术。</li>
<li>Spring Cloud中Spring Cloud Netflix的组件是经过生产环境验证的，其他的则建议慎重选择。</li>
</ol>
</blockquote>

<!--more-->


<h2>微服务是什么</h2>

<p>微服务起源于2005年Peter Rodgers博士在云端运算博览会提出的微Web服务(Micro-Web-Service)，根本思想类似于Unix的管道设计理念。2014年，由Martin Fowler 与 James Lewis共同提出了微服务的概念，定义了微服务架构风格是一种通过一套小型服务来开发单个应用的方法，每个服务运行在自己的进程中，并通过轻量级的机制进行通讯（HTTP API）。关键的三点是<strong>small、automated以及lightweight</strong>。</p>

<p>对比SOA，微服务可以看做是SOA的子集，是轻量级的SOA，粒度更细的服务，独立进程、数据分离，更注重<strong>敏捷、持续交付、DevOps以及去中心化实践</strong>。其共同的<strong>架构原理</strong>：</p>

<ul>
<li>单一职责</li>
<li>关注分离：控制与逻辑相分离</li>
<li>模块化和分而治之</li>
</ul>


<p><strong>特点</strong>：</p>

<ul>
<li>用服务进行组件化</li>
<li>围绕业务能力进行组织</li>
<li>是产品而非项目</li>
<li>端点智能化和哑管道: 控制逻辑都在端点，管道仅仅是传输</li>
<li>全自动化部署</li>
<li>语言和数据的去中心化控制</li>
<li>面向失败设计</li>
<li>渐进式设计</li>
</ul>


<p>综合来看，其优缺点如下：</p>

<p><strong>优点</strong>：</p>

<ul>
<li>模块的强边界</li>
<li>独立部署</li>
<li>技术选型的多样性</li>
</ul>


<p><strong>缺点</strong>：</p>

<ul>
<li>分布式带来编程复杂度，远程调用的消耗</li>
<li>舍弃强一致性，实现最终一致性</li>
<li>操作复杂性要求有一个成熟的运维团队或者运维基础设施</li>
</ul>


<h2>为什么要采用微服务</h2>

<p>是否选择微服务取决于你要设计的系统的复杂度。微服务是用来把控复杂系统的，但是随之而来的就是引入了微服务本身的复杂度。需要解决包括自动化部署、监控、容错处理、最终一致性等其他分布式系统面临的问题。即使已经有一些普遍使用的解决方案，但是仍然是有不小的成本的。</p>

<p><img src="//post_images/msa/productivity.png" alt="" /></p>

<p>生产力和复杂度的关系如图所示，可见系统越复杂，微服务带来的收益越大。此外，无论是单体应用还是微服务，团队的技能都需要能够把控住。</p>

<p>马丁.福勒的一个观点是：除非管理单体应用的成本已经太复杂了（太大导致很难修改和部署），否则都不要考虑微服务。大部分应用都应该选择单体架构，做好单体应用的模块化而不是拆分成服务。</p>

<p>因此，<strong>系统一开始采用单体架构，做好模块化，之后随着系统变得越来越复杂、模块/服务间的边界越来越清晰，再重构为微服务架构是一个合理的架构演化路径。</strong></p>

<p><strong>四个可以考虑上微服务的情况</strong>：</p>

<ol>
<li>多人开发一个模块/项目，提交代码频繁出现大量冲突。</li>
<li>模块间严重耦合，互相依赖，每次变动需要牵扯多个团队，单次上线需求太多，风险大。</li>
<li>主要业务和次要业务耦合，横向扩展流程复杂。</li>
<li>熔断降级全靠if-else。</li>
</ol>


<p><strong>微服务的三个阶段</strong>：</p>

<ol>
<li>微服务1.0：仅使用注册发现，基于SpringCloud或者Dubbo进行开发。</li>
<li>微服务2.0：使用了熔断、限流、降级等服务治理策略，并配备完整服务工具和平台。</li>
<li>微服务3.0：Service Mesh将服务治理作为通用组件，下沉到平台层实现，应用层仅仅关注业务逻辑，平台层可以根据业务监控自动调度和参数调整，实现AIOps和智能调度。</li>
</ol>


<h2>微服务架构</h2>

<h3>先决条件</h3>

<ul>
<li>快速的环境提供能力：依赖于云计算、容器技术，快速交付环境。</li>
<li>基本的监控能力：包括基础的技术监控和业务监控。</li>
<li>快速的应用部署能力：需要部署管道提供快速的部署能力。</li>
<li>Devops文化：需要具有良好的持续交付能力，包括全链路追踪、快速环境提供和部署等，还需要快速的反应能力（对问题、故障的快速响应），开发和运维的协同工作。</li>
</ul>


<p>此外，根据康威定律和逆康威定律（技术架构倒逼组织架构改进），组织架构也是一个很关键的因素。对应于微服务架构，组织架构需要遵循以下原则：</p>

<ol>
<li>一个微服务由一个团队维护，团队成员以三人为宜。</li>
<li>单个团队的任务和发展是独立的，不受其他因素影响。</li>
<li>团队是功能齐全、全栈、自治的，扁平、自我管理。</li>
</ol>


<h3>基础设施</h3>

<p>微服务的推行需要依赖于很多底层基础设施，包括提供微服务的编译、集成、打包、部署、配置等工作，采用PaaS平台解决微服务从开发到运行的全生命周期管理，同时提供异构环境管理、容器资源隔离与互通、服务伸缩漂移、服务升级与回退、服务熔断与降级、服务注册与发现。</p>

<ol>
<li><p>最基本的基础设施</p>

<ul>
<li>进程间通讯机制：微服务是独立进程的，需要确定之间的通讯方式。</li>
<li>服务发现+服务路由: 提供服务注册中心，服务提供者和消费者通过服务发现获取服务的信息从而调用服务，实现服务的负载均衡等。</li>
<li>服务容错：微服务架构中，由于服务非常多，往往是一个服务挂了，整个请求链路的服务都受到影响，因此需要服务容错，在服务调用失败的时候能够处理错误或者快速失败，包括熔断、fallback、重试、流控和服务隔离等。</li>
<li>分布式事务支持：随着业务拆分为服务，那么有时候不可避免的就是跨服务的事务，即分布式事务的问题。原则是尽量避免分布式事务，如果无法避免那么可以使用消息系统或者CQRS和Event Sourcing方案来实现最终一致性。如果需要强一致性，则有两阶段提交、三阶段提交、TCC等分布式事务解决方案。</li>
</ul>
</li>
<li><p>提升外部服务对接效率和内部开发效率</p>

<ul>
<li>API网关: 负责外部系统的访问，负责跨横切面的公共层面的工作，包括安全、日志、权限控制、传输加密、请求转发、流量控制等。典型的网关功能即对外暴露一个域名xx.com，根据第一级目录做反向路由xx.com/user，xx.com/trade。每一级目录，如user、trade对应一个服务的域名。此外，API网关也可以有服务编排的功能（不推荐）。</li>
<li>接口框架: 规范服务之间通讯使用的数据格式、解析包、自解释文档，便于服务使用方快速上手等。</li>
</ul>
</li>
<li><p>提升测试和运维效率</p>

<ul>
<li>配置中心: 运行时配置管理能够解决动态修改配置并批量生效的问题。包括配置版本管理、配置项管理、节点管理、配置同步等。</li>
<li>持续交付：包括持续集成、自动化部署等流程。目的就是小步迭代，快速交付。

<ul>
<li>持续集成：这一部分并非是微服务特定的，对于之前的单体应用，此部分一般来说也是必要的。主要是指通过自动化手段，持续地对代码进程编译构建、自动化测试，以得到快速有效的质量反馈，从而保证代码的顺利交付。自动化测试包括代码级别的单元测试、单个系统的集成测试、系统间的接口测试。</li>
<li>自动化部署：微服务架构，节点数动辄上百上千，自动化部署能够提高部署速度和部署频率，从而保证持续交付。包括版本管理、资源管理、部署操作、回滚操作等功能。而对于微服务的部署方式，包括<strong>蓝绿部署、滚动部署以及金丝雀部署</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><p>进一步提升运维效率</p>

<ul>
<li>服务监控: 微服务架构下节点数目众多，需要监控的机器、网络、进程、接口等的数量大大增加，需要一个强大的监控系统，能够提供实时搜集信息进行分析以及实时分析之上的预警。包括监控服务的<strong>请求次数、响应时间分布、最大/最小响应值、错误码分布</strong>等</li>
<li>服务跟踪：跟踪一个请求的完整路径，包括<strong>请求发起时间、响应时间、响应码、请求参数、返回结果</strong>等信息，也叫做全链路跟踪。通常的服务监控可以和服务监控做在一起，宏观信息由服务跟踪呈现，微观单个服务/节点的信息由服务监控呈现。服务跟踪目前的实现理论基本都是Google的Dapper论文。</li>
<li>服务安全：内网之间的微服务调用原则上讲应该是都可以互相访问写，一般并不需要权限控制，但有时候限于业务要求，会对接口、数据等方面有安全控制的要求。此部分可以以配置的方式存在于服务注册中心中，和服务绑定，在请求时由做为服务提供者的服务节点进行安全策略控制。配置则可以存储在配置中心以方便动态修改。</li>
</ul>
</li>
</ol>


<p>在微服务数量很少的情况下，以上基础设施的优先级自上而下降低。否则，仅仅依赖人工操作，则投入产出比会很低。</p>

<p>还需要提到的是Docker容器技术。虽然这个对于微服务并不是必须的，但是容器技术<strong>轻量级、灵活、与应用依存、屏蔽环境差异</strong>的特性对于持续交付的实现是至关重要的，即使对于传统的单体应用也能够给其带来交付效率的大幅提升。</p>

<h3>架构设计模式</h3>

<p>在引入微服务之后，传统的单体应用变为了一个一个服务，之前一个应用直接提供接口给客户端访问的架构不再适用。微服务架构下，针对不同设备的接口做为BFF层（Backend For Frontend），也叫做用户体验适配层，负责聚合、编排微服务的数据转换成前端需要的数据。服务之间的调用则在允许的情况下（允许延迟）尽可能使用异步消息传递方式，如此形成<strong>面向用户体验的微服务架构设计模式</strong>。如下图所示：</p>

<p><img src="//post_images/msa/msa-arch.png" alt="" /></p>

<p><strong>Client -> API Gateway -> BFF（Backend For Frontend） -> Downstream Microservices</strong></p>

<ul>
<li>后台采用微服务架构，微服务可以采用不同的编程语言和不同的存储机制。</li>
<li>前台采用BFF模式对不同的用户体验（如桌面浏览器，Native App，平板响应式Web）进行适配。</li>
<li>BFF、API Orchestration Layer，Edge Service Layer，Device Wrapper Layer是相同的概念。</li>
<li>BFF不能过多，过多会造成代码逻辑重复冗余。</li>
<li>可以将网关承担的功能，如Geoip、限流、安全认证等跨横切面功能和BFF做在同一层，虽然增加了BFF层的复杂性，但能够得到性能优势。</li>
</ul>


<h3>服务拆分</h3>

<p>微服务架构最核心的环节，主要是对服务的<strong>横向拆分</strong>。服务拆分就是讲一个完整的业务系统解耦为服务，<strong>服务需要职责单一，之间没有耦合关系，能够独立开发和维护</strong>。</p>

<p>服务拆分不是一蹴而就的，需要在开发过程中不断地理清边界。在完全理清服务之前，尽量推迟对服务的拆分，尤其是对数据库的拆分。</p>

<p><strong>拆分方法</strong>如下：</p>

<ul>
<li>基于业务逻辑拆分</li>
<li>基于可扩展拆分</li>
<li>基于可靠性拆分</li>
<li>基于性能拆分</li>
</ul>


<p>其中，对于无法修改的遗留系统，采用绞杀者模式：在遗留系统外面增加新的功能做成微服务方式，而不是直接修改原有系统，逐步的实现对老系统替换。</p>

<p><strong>拆分过程需要遵守的规范</strong>如下：</p>

<ul>
<li>先少后多、先粗后细（粒度）</li>
<li>服务纵向拆分最多三层，两次调用：Controller、组合服务、基础服务</li>
<li>仅仅单向调用，禁止循环调用</li>
<li>串行调用改为并行调用或者异步化</li>
<li>接口应该幂等</li>
<li>接口数据定义严禁内嵌，透传</li>
<li>规范化工程名</li>
<li>先拆分服务，等服务粒度确定后再拆分数据库。</li>
</ul>


<h3>微服务框架</h3>

<p>上面讲述了微服务架构的众多基础设施，如果每一个基础设施都需要自己开发的话是非常巨大的开发工作。目前市面上已经有不少开源的微服务框架可以选择。</p>

<ol>
<li><p>Spring Boot</p>

<p> Spring Boot是用来简化新Spring应用的初始搭建以及开发过程的。其虽然不是微服务框架，但其设计的初衷本质就是微应用的底层框架，因此非常适合用于微服务基础设施的开发以及微服务的应用开发。尤其对于Spring技术栈的团队来说，基于Spring Boot开发微服务框架和应用是自然而然的一个选择。</p></li>
<li><p>Dubbo&amp;&amp;Motan</p>

<p> Dubbo阿里开源的服务治理框架。其出现在微服务理念兴起之前，可以看做是SOA框架的集大成之作。但其仅仅包含了微服务基础设施的部分功能，诸如熔断、服务跟踪、网关等都没有实现。</p>

<ul>
<li>服务发现 ：服务发布、订阅、通知</li>
<li>高可用策略 ：失败重试（Failover）、快速失败（Failfast）、资源隔离 - 负载均衡 ：最少活跃连接、一致性 Hash、随机请求、轮询等</li>
<li>扩展性 ：支持 SPI 扩展（service provider interface）</li>
<li>其他 ：调用统计、访问日志等</li>
</ul>


<p>Motan则是微博开源的类似Dubbo的RPC框架，与Dubbo相比更轻量级。</p></li>
<li><p>Spring Cloud</p>

<p> Spring Cloud是基于Spring Boot实现的微服务框架，也可以看做一套微服务实现规范。基本涵盖了微服务基础设施的方方面面，包括配置管理、服务发现、断路器、智能路由、微代理、控制总线、全局锁、决策竞选、分布式会话和集群状态管理等。其基于Spring生态，社区支持非常好。但其很多组件都没有经过生产环境验证，需要慎重选择。</p>

<p> Spring Cloud Netflix是Spring Cloud的一个子项目，是Spring对Netflix OSS的集成实现。基于Netflix的大规模使用，其中的已经被广泛使用的组件包括：</p>

<ul>
<li>Eureka： 服务注册和服务发现</li>
<li>Ribbon：弹性而智能的进程间和服务通讯机制，客户端负载均衡</li>
<li>Hystrix： 熔断器，在运行时提供延迟和容错的隔离</li>
<li>Zuul: 服务网关</li>
</ul>


<p>此外，另一个子项目Spring Cloud Alibaba则是Alibaba开源的基于Spring Boot的微服务框架，主要是对阿里云服务的支持。</p></li>
<li><p>Service Mesh</p>

<p> 上述的微服务框架都是侵入式的，服务化的过程都需要进行代码改造。Service Mesh则是下一代微服务架构，最明显的特征就是无入侵。采用sidecar模式来解决系统架构微服务化后的服务间通信和治理问题。如下图所示：</p>

<p> <img src="//post_images/msa/sm.png" alt="" /></p>

<p> 目前主流的开源实现包括：</p>

<ul>
<li>Linkerd和Envoy：以 sidecar 为核心，关注如何做好proxy，并完成一些通用控制平面的功能。缺乏对这些sidecar的管理和控制。</li>
<li>Istio和Conduit：目前最为流行的Service Mesh实现方案，集中在更加强大的控制平面(sidecar被称为数据平面)功能。前者由Google和IBM合作，并使用了Envoy作为sidecar部分的实现；后者则是Linkerd作者的作品。相比起来，Istio有巨头背景，功能强大，但可用性和易用性一直不高，Conduit则相对简单、功能聚焦。</li>
</ul>


<p>限于Service Mesh带来的性能延迟的开销以及sidecar对分布复杂性的增加，其对大规模部署(微服务数目多)、异构复杂(交互协议/开发语言类型多)的微服务架构带来的收益会更大。</p></li>
<li><p>Sofastack</p>

<p> 蚂蚁金服开源的构建金融级分布式架构的一套中间件。包括微服务开发框架、RPC框架、服务注册中心、全链路追踪、服务监控、Service Mesh等一整套分布式应用开发工具。</p>

<p> 特别值得一提的是SOFAMesh。其是对下一代微服务架构Service Mesh的大规模落地方案实践，基于 Istio改进和扩展而来，应该是国内最为成熟的开源Service Mesh方案。</p></li>
</ol>


<p>此外，需要提到<strong>Kubernetes(K8s)</strong>，其本身提供了部分的微服务特性支持（通过域名做服务发现），对代码无侵入。但服务调用、熔断这些都需要自己实现。</p>

<p>综上，目前公司技术团队技术栈是Spring，并且已有服务的实现都是基于Dubbo，因此选择Spring Cloud Netflix做为基础的微服务框架，对其中不成熟或者缺乏的组件，选择业界更为成熟的组件替代即可。</p>

<p><img src="//post_images/msa/msa-basic.png" alt="" /></p>

<ul>
<li>API网关：Zuul</li>
<li>服务注册中心：Dubbo</li>
<li>配置中心：disconf</li>
<li>服务监控&amp;&amp;全链路追踪：CAT</li>
<li>服务开发框架：Spring Boot</li>
<li>日志监控、告警：ELK + Elasalert</li>
<li>流量控制：Sentinel</li>
<li>消息队列：Kafka</li>
</ul>


<h2>参考资料</h2>

<ul>
<li><a href="https://www.ben-morris.com/whats-so-bad-about-monoliths-anyway/">What’s so bad about monoliths anyway…?!</a></li>
<li><a href="https://martinfowler.com/articles/microservices.html">Microservice</a></li>
<li><a href="https://martinfowler.com/bliki/MicroservicePremium.html">MicroservicePremium</a></li>
<li><a href="https://martinfowler.com/articles/microservice-trade-offs.html">Microservice Trade-Offs</a></li>
<li><a href="https://martinfowler.com/bliki/MicroservicePrerequisites.html">MicroservicePrerequisites</a></li>
<li><a href="https://martinfowler.com/bliki/MonolithFirst.html">MonolithFirst</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzI2MzM3MzkyMg==&amp;mid=2247486544&amp;idx=1&amp;sn=999be8b4f06150b96d9a46ada6bb9ded&amp;chksm=eabd995dddca104bd3c6262d491572f7be9b2a763a43a388f66bd0e90e4bd60e5037727107e4&amp;mpshare=1&amp;scene=1&amp;srcid=0201lT7ZBVBGmTki8bYnmDgl%23rd">服务怎么拆？</a></li>
<li><a href="https://www.thoughtworks.com/insights/blog/bff-soundcloud">BFF@SoundCloud</a></li>
<li><a href="http://www.importnew.com/28798.html">Service Mesh 及其主流开源实现解析</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[缓存这些事]]></title>
    <link href="http://www.rowkey.me/blog/2019/02/25/cache/"/>
    <updated>2019-02-25T11:29:34+00:00</updated>
    <id>http://www.rowkey.me/blog/2019/02/25/cache</id>
    <content type="html"><![CDATA[<p><strong>最新版本可见:<a href="https://github.com/superhj1987/pragmatic-java-engineer/blob/master/book/chapter5-datastore/cache.md">https://github.com/superhj1987/pragmatic-java-engineer/blob/master/book/chapter5-datastore/cache.md</a></strong></p>

<p>缓存是为了弥补持久化存储服务如数据库的性能缓慢而出现的一种将数据存储在内存中，从而大大提高应用性能的服务。如缓存五分钟法则所讲：如果一个数据频繁被访问，那么就应该放内存中。这里的缓存就是一种读写效率都非常高的存储方案，能够应对高并发的访问请求，通常情况下也不需要持久化的保证。但相对其他存储来说，缓存一般是基于内存的，成本比较昂贵，因此不能滥用。</p>

<p>缓存可以分为：本地缓存和分布式缓存。</p>

<!--more-->


<h2>本地缓存</h2>

<p>本地缓存指的是内存中的缓存机制，适用于尺寸较小、高频的读取操作、变更操作较少的存储场景。在Java开发中常用的本地缓存实现有：</p>

<ol>
<li><p>ConcurrentHashMap</p>

<p> 这是JDK自带的线程安全map实现，适合用户全局缓存。其get、put的操作比较简单，不用赘述。如果想要实现缓存的失效、淘汰策略则需要自定义实现。</p></li>
<li><p>LinkedHashMap</p>

<p> LinkedHashMap也是JDK的实现。其简单的用途是一个可以保持插入或者访问顺序的HashMap，但其实其配置好是可以当做LRU cache的。这里的LRU即least recently used, 指的是固定容量的缓存，当缓存满的时候，优先淘汰的是最近未被访问的数据。</p>

<pre><code class="`"> int cacheSize = 1000; //最大缓存1000个元素

 LinkedHashMap cache = new LinkedHashMap&lt;String, String&gt;(16, 0.75f, true) {
     @Override
     protected boolean removeEldestEntry(Map.Entry&lt;String, String&gt; eldest) {
         return size() &gt; cacheSize;
     }
 };
</code></pre>

<p> 需要注意的是，LinkedHashMap是非线程安全的，如果是全局使用，需要做并发控制。</p></li>
<li><p>Guava Cache</p>

<p> Guava Cache来自于Google开源的Guava类库中，是一个实现的比较完全的本地缓存，包括缓存失效、LRU都做了支持。</p>

<pre><code class="`"> final int MAX_ENTRIES = 1000; //最大元素数目
 LoadingCache&lt;String, String&gt; cache = CacheBuilder.newBuilder()
     .maximumSize(MAX_ENTRIES)
     .concurrencyLevel(Runtime.getRuntime().availableProcessors())//并行度
     .expireAfterWrite(2, TimeUnit.SECONDS) //写入2秒后失效
     .build(new CacheLoader&lt;String, String&gt;() { 
         @Override
         public String load(String key) throws Exception {
             return ...;//异步加载数据到缓存
         }

         @Override
         public ListenableFuture&lt;String&gt; reload(String key, String oldValue) throws Exception {
             return ...;
         }
     }); 

 //Using the cache
 String value= cache.getUnchecked("testKey");
</code></pre>

<p> 上面的load方法是第一次加载对应的key的缓存时调用的方法,重载此方法可以实现单一线程回源，而reload方法的重载，则可以在后台定时刷新数据的过程中，依然使用旧数据响应请求，不会造成卡顿，这里默认的实现是load方法的代理，是同步的，建议重新用异步方式实现。此外，里面并行度指的是允许并行修改的线程数，此值建议根据当前机器的CPU核数来设置。</p>

<p> 上述的例子中使用了基于maximumSize和基于时间expireAfterWrite的缓存剔除，除此之外，还可以通过：</p>

<ol>
<li><p>基于权重的缓存剔除</p>

<pre><code class="`"> CacheBuilder.newBuilder()
     .maximumWeight(10000)             
     .weigher(new Weigher&lt;String, Object&gt;() {  
         @Override  
         public int weigh(String key, Object value) {  
             return key.length();  
         }  
     })
     .build();
</code></pre>

<p> 这样当cache中put一个key时，都会计算它的weight值并累加，当达到maximumWeight阀值时，会触发剔除操作。</p></li>
<li><p>制定key和value使用的引用类型来做缓存剔除</p>

<pre><code class="`"> CacheBuilder.newBuilder().weakKeys();
 CacheBuilder.newBuilder().weakValues();
 CacheBuilder.newBuilder().softValues();
</code></pre></li>
</ol>


<p> 还需要指明的一点是，Guava Cache中的缓存失效并非立即生效的，通常是延迟的, 在各种写入数据时都去检查并cleanUp。</p>

<p> 此外，Guava Cache还提供了asMap视图，可以获取保存数据使用的ConcurrentMap形式。使用此视图时需要注意读写操作会重置相关缓存项的访问时间，包括asMap().get()方法和Cache.asMap().put()方法，但asMap().containsKey()方法和遍历asMap().entrySet()除外。</p>

<p> 这里还需要提到的一点是，缓存框架Caffeine使用Java8对Guava进行了重写，包括驱逐策略、过期策略和并发机制，使得缓存性能得到了显著提升，并且使用上可以兼容Guava的API。如果是在Java8上的开发，推荐直接使用Caffeine作为本地缓存实现。</p>

<pre><code class="`"> LoadingCache&lt;String, String&gt; cache = CaffeinatedGuava.build(
            Caffeine.newBuilder().maximumSize(MAX_ENTRIES),
            new CacheLoader&lt;String, String&gt;() { // Guava's CacheLoader
                @Override
                public String load(String key) throws Exception {
                    return "";
                }
            });
</code></pre></li>
<li><p>Ehcache</p>

<p> Ehcache是一个纯Java的进程内缓存框架，具有快速、精干等特点，是Hibernate中默认的CacheProvider，使用比较广泛，支持多级存储，可以将数据存储到磁盘上。其最新版本为3.x，但使用不多，且兼容性也不好，推荐使用其2.x版本即可。</p></li>
</ol>


<h2>分布式缓存</h2>

<p>分布式缓存指的是单独的缓存服务，独立部署，通过协议、接口等提供缓存服务。相比起本地缓存，能够支持更大的容量。</p>

<p>几年前最流行的分布式缓存软件是Memcached，但其支持的数据结构太少，现在已经基本被Redis所取代。Redis能够支持丰富的数据结构，基于事件驱动的单线程非阻塞IO也能够应对高并发的业务场景。这里主要针对Redis来讲述，Redis版本为3.2.10。</p>

<p>Redis是非常强大的，既可以作为数据库又可以作为缓存，还能当做队列。总体概括来讲，其有以下用途：</p>

<ol>
<li>最简单的String,可以作为Memcached的替代品，用作缓存系统。</li>
<li>使用SetNx可以实现简单的分布式锁(如果需要对锁设置失效期，建议使用SET key value [EX|PX] NX xx命令以保证原子性),也可参考Redis作者的RedLock算法实现分布式锁（<a href="http://redis.cn/topics/distlock.html%EF%BC%89%E3%80%82">http://redis.cn/topics/distlock.html%EF%BC%89%E3%80%82</a></li>
<li>使用List的pop和push功能可以作为阻塞队列/非阻塞队列。</li>
<li>使用SUBSCRIBE和PUBLISH可以实现发布订阅模型。</li>
<li>对数据进行实时分析，如可以累加统计等。</li>
<li>使用Set做去重的计数统计。</li>
<li>使用SortedSet可以做排行榜等排序场景。</li>
<li>使用getbit、setbit、bitcount做大数据量的去重统计，允许误差的情况下可使用HyperLogLog。</li>
<li>使用GEO可以实现位置定位、附近的人。</li>
</ol>


<p>以上场景基本上涵盖了Redis支持的各种存储结构：</p>

<ul>
<li>Key: 可以是任意类型，但最终都会存储为byte[]。</li>
<li>String: 简单的(key,value)存储结构，支持数据的自增、支持BitSet结构。</li>
<li>Hash：哈希表数据结构，支持对field的自增等操作。</li>
<li>List：列表，支持按照索引、索引范围获取元素以及pop、push等堆栈操作。</li>
<li>Set：集合，去重的列表。</li>
<li>SortedSet：有序集合。</li>
<li>HyperLogLog：可对大数据进行去重，有一定的误差率。</li>
<li>GEO：地理位置的存储结构，支持GEOHASH。</li>
</ul>


<h3>内存压缩</h3>

<p>Redis的存储是以内存为主的，因此如何节省内存是使用的时候一个非常关键的地方。毕竟一个String类型的存储即使key和value是简单的1字节，其占用空间也达到了差不多64字节（估算近似值，包括了dictEntry、redisObject、key、value以及内存对齐等）。</p>

<p>首先，key越短越好，可以采取编码或者简写的方式。如用户的笔记数目缓存key可以使用u:{uid}:n_count作为Key。同时,key的数量也要控制，可以考虑使用hash做二级存储来合并类似的key从而减少key的数量。</p>

<p>其次，value也是越小越好，尤其是存储序列化后的字节时，要选择最节省内存的序列化方式, 如Kryo、Protobuf等。</p>

<p>此外，Redis支持的数据结构的底层实现会对内存使用有很大的影响，如：缓存用户的头像时，可以根据用户ID做分段存储，每一段使用hash结构进行存储:</p>

<pre><code>//第一段 1-999
hset u:avatar:1 1 http://xxxx
hset u:avatar:1 2 http://xxxx

//第二段 1000-1999
hset u:avatar:2 1000 http://xxxx
hset u:avatar:2 1999 http://xxxx
</code></pre>

<p>这样，相比起使用String存储，hash底层会使用ziplist做存储，极大地节省内存使用。但这里需要注意的是Redis有一个hash-max-ziplist-entries的参数，默认是512，如果hash中的field数目超过此值，那么hash将不再使用ziplist存储，开始使用hashtable。但是，此值设置过大，那么在查询的时候就会变慢。从实践来看，此值设置为1000，hash分段大小也为1000，此时的修改和查询性能最佳。此外，还有一个hash-max-ziplist-value参数，默认是64字节，value的最大字符串字节大小如果大于此值，那么则不会使用ziplist。</p>

<p>除了hash之外，其他数据结构也有类似的内存编码变化，使用的时候也需要注意。如下所示：</p>

<table>
<thead>
<tr>
<th>数据结构 </th>
<th> 编码 </th>
<th> 条件</th>
</tr>
</thead>
<tbody>
<tr>
<td>hash</td>
<td> ziplist</td>
<td> 最大value大小 &lt;= hash-max-ziplist-value &amp;&amp; field个数 &lt;= hash-max-ziplist-entries</td>
</tr>
<tr>
<td>hash</td>
<td> hashtable </td>
<td> 最大value大小 > hash-max-ziplist-value || field个数 > hash-max-ziplist-entries</td>
</tr>
<tr>
<td>list</td>
<td> ziplist</td>
<td> 最大value大小 &lt;= list-max-ziplist-value &amp;&amp; field个数 &lt;= list-max-ziplist-entries</td>
</tr>
<tr>
<td>list</td>
<td> linkedlist</td>
<td> 最大value大小 > list-max-ziplist-value || 列表长度 > list-max-ziplist-entries</td>
</tr>
<tr>
<td>set</td>
<td> intset</td>
<td> 元素都为整数 &amp;&amp; 集合长度 &lt;= set-max-intset-entries</td>
</tr>
<tr>
<td>set</td>
<td> hashtable</td>
<td> 元素非整数类型 || 集合长度 > set-max-intset-entries</td>
</tr>
<tr>
<td>sortedSet </td>
<td> ziplist</td>
<td> 最大value大小 &lt;= zset-max-ziplist-value &amp;&amp; 集合长度 &lt;= zset-max-ziplist-entries</td>
</tr>
<tr>
<td>sortedSet </td>
<td> skiplist </td>
<td> 最大value大小 > zset-max-ziplist-value || 集合长度 > zset-max-ziplist-entries</td>
</tr>
</tbody>
</table>


<p>此外，对于list来说，Redis 3.2使用了新的数据结构quicklist来编码实现，废弃了list-max-ziplist-value和list-max-ziplist-entries配置，使用list-max-ziplist-size（负数表示最大占用空间或者正数表示最大压缩长度）和list-compress-depth（最大压缩深度）这俩参数进行配置。</p>

<p>还有一点需要注意的是内存碎片，所谓内存碎片指的是小的非连续的内存，这种内存无法得到充分使用，会造成浪费。我们可以通过info命令获取mem_fragmentation_ratio（used_memory_rss/used_memory）此值来观察内存碎片的程度。</p>

<ul>
<li>此值通常在1左右，越大表示表示存在（内部或外部的）内存碎片。</li>
<li>小于1时表示Redis的部分内存被换出到了交换空间，会降低操作性能。</li>
</ul>


<h3>Redis Lua</h3>

<p>一般情况下，Redis提供的各种操作命令已经能够满足我们的需求。如果需要一次将多个操作请求发送到服务端，可以通过Jedis客户端的pipeline接口批量执行。但如果有以下三种需求，就需要使用Redis Lua：</p>

<ul>
<li>需要保证这些命令做为一个整体的原子性。</li>
<li>这些命令之间有依赖关系、</li>
<li>业务逻辑除了Redis操作还包括其他逻辑运算。</li>
</ul>


<p>Redis从2.6后内置对Lua Script的支持，通过eval或者evalsha执行Lua脚本。其脚本的执行具有原子性，因此适用于秒杀、签到等需要并发互斥且有一些业务逻辑的业务场景。</p>

<pre><code>String REDIS_SCRIPT_GRAB_GIFT =
            "local giftLeft = tonumber(redis.call('get',KEYS[1])) or 0;" //读取礼物剩余数量
                    + "if(giftLeft &lt;= 0) then return 0; end;" //抢购失败
                    + "redis.call('decr',KEYS[1]);" //减少礼物数量
                    + "return 1;";

...
Object grabResutl = jedis.eval(REDIS_SCRIPT_GRAB_GIFT, Lists.newArrayList("test:gifts:" + giftId + ":left"),null);
...
</code></pre>

<p>使用Redis Lua需要注意的是：</p>

<ul>
<li>Lua脚本里涉及的所有key尽量用变量，从外面传入，使Redis一开始就知道你要改变哪些key，尤其是在使用redis集群的时候。</li>
<li>建议先用SCRIPT LOAD载入script，返回哈希值。然后用EVALHASH执行脚本，可以节省脚本传输的成本。</li>
<li>如果想从Lua返回一个浮点数，应该将它作为一个字符串（比如ZSCORE命令）。因为Lua中整数和浮点数之间没有什么区别，在返回浮点数据类型时会转换为整数。</li>
</ul>


<h3>数据失效和淘汰</h3>

<p>如果某些数据并不需要永远存在，可以通过Expire设置其失效时间，让其在这段时间后被删除。这里设置了失效时间之后可以通过SET 和 GETSET 命令覆写失效期或者使用PERSIST去掉失效期。需要注意的是如果一个命令只是更新一个带生存时间的 key 的值而不是用一个新的 key 值来代替它的话，那么生存时间不会被改变。如INCR、DECR、LPUSH、HSET等命令就不改变key的失效时间。此外，设置了失效期的key其ttl是大于0的，直至被删除会变为-2, 未设置失效期的key其ttl为-1。</p>

<p>和大部分缓存一样，过期数据并非立即被删除的。在Redis中，其采取的方式如下：</p>

<ul>
<li>消极方法：主动get或set时触发失效删除</li>
<li>积极方法：后台线程周期性（每100ms一次）随机选取100个设置了有效期的key进行失效删除，如果有1/4的key失效，那么立即再选取100个设置了有效期的key进行失效删除。</li>
</ul>


<p>这里需要注意的是当使用主从模式时，删除操作只在Master端做，在Slave端做是无效的。</p>

<p>此外，当对Redis设置了最大内存maxmemory, 那么当内存使用达到maxmemory后，会触发缓存淘汰。Redis支持以下几种淘汰策略：</p>

<ul>
<li>volatile-lru：从已设置过期时间的数据集中挑选最近最少使用的数据淘汰。</li>
<li>volatile-ttl：从已设置过期时间的数据集中挑选将要过期的数据淘汰。</li>
<li>volatile-random：从已设置过期时间的数据集中任意选择数据淘汰。</li>
<li>allkeys-lru：从数据集中挑选最近最少使用的数据淘汰。</li>
<li>allkeys-random：从数据集中任意选择数据淘汰。</li>
<li>noeviction：禁止驱逐数据。</li>
</ul>


<p>其中，volatile-lru是3.0版本之前的默认淘汰策略，之后的版本默认策略改成了noeviction。</p>

<p>为了配合LRU的淘汰策略，Redis的内部数据结构中有一个lru字段记录了对象最后一次被访问的时间。可以通过object idletime [key]来在不更新lru字段的情况下查看相应key的空闲时间。进一步的可以结合使用scan+object idletile [key]来查询哪些健长时间未被访问，以判定热点key和冷key。</p>

<p>这里需要注意的是Redis中为了节省内存占用使用了整数对象池（即共享整数对象），但当淘汰策略为LRU时，由于无法对对象池的同一个对象设置多个访问时间戳，因此不再会使用整数对象池。</p>

<h3>持久化</h3>

<p>Redis支持对内存中的数据进行持久化，包括两种实现方式：</p>

<ol>
<li><p>RDB</p>

<p> RDB是基于二进制快照的持久化方案，其在指定的时间间隔内（默认触发策略是60秒内改了1万次或300秒内改了10次或900秒内改了1次）生成数据集的时间点快照（point-in-time snapshot),从而实现持久化。基于快照的特性，使其会丢失一些数据，比较适用于对Redis的数据进行备份。此外，RDB进行时，Redis会fork()出一个子进程，并由子进程来遍历内存中的所有数据进行持久化。在数据集比较庞大时，由于fork出的子进程需要复制内存中的数据，因此这个过程会非常耗时，会造成服务器停止处理客户端，停止时间可能会长达一秒。</p>

<p> 可配置RDB对数据进行压缩存储，支持字符串的LZF算法和String形式的数字变回int形式。</p></li>
<li><p>AOF</p>

<p> AOF是基于日志的持久化方案，记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。这些命令全部以 Redis 协议的格式来保存（纯文本文件），新命令会被追加到文件的末尾。此外，为了避免AOF的文件体积超出保存数据集状态所需的实际大小，Redis在AOF文件过大时会fork出一个进程对AOF文件进行重写（将历史AOF记录中的命令合并替换成key-value的插入命令）。AOF这种方案，默认是每隔1秒进行一次fsync（将日志写入磁盘），因此与RDB相比，其最多丢失1秒钟的数据，当然如果配置成每次执行写入命令时 fsync（执行命令成功后进行aof，非常慢），甚至可以避免任何数据的丢失。但其文件的体积是明显大于RDB的，将日志刷到磁盘和从AOF恢复数据的过程也是慢于RDB的。</p></li>
</ol>


<p>如果想要保证数据的安全性，建议同时开启AOF和RDB，此时由于RDB有可能丢失文件，Redis重启时会优先使用AOF进行数据恢复。</p>

<p>此外，可以通过save或者bgsave命令来手动触发RDB持久化，通过bgrewriteaof触发aof重写。如此可以将rdb或者aof文件传到另一个Redis结点进行数据迁移。</p>

<p>需要注意的是，如果通过kill -9或者Ctrl+c来关闭redis,那么RDB和AOF都不会触发，会造成数据丢失，建议使用redis-cli shutdown或者kill优雅关闭Redis。</p>

<h3>分布式</h3>

<p>Redis对分布式的支持有三种：</p>

<ol>
<li><p>Master-Slave</p>

<p> 简单的主从模式，通过执行slaveof命令来启动，一旦执行， Slave会清掉自己的所有数据，同时Master会bgsave出一个RDB文件并以Client的方式连接Slave发送写命令给Slave传输数据（多个slave连接时，只要在master的bgsave完成之前，那么就不会多次bgsave）。2.8版本后，Redis提供了PSYNC协议，支持主备间的增量同步，类似于断点续传，不会每次连接Master都全量同步数据。</p>

<p> Redis提供了Redis Sentinel做上述方案的fail-over，能够对 Redis 主从复制进行监控，并实现主挂掉之后的自动故障转移。</p>

<p> 首先，Sentinel会在Master上建一个pub/sub channel，通告各种信息。所有Sentinel通过接收pub/sub channel上的+sentinel的信息发现彼此（Sentinel每5秒会发送一次<strong>sentinel</strong>:hello消息)。然后，Seneinel每秒钟会对所有Master、Slave和其他Sentinel执行ping，这些redis-server会响应+PONG、-LOADING或者-MASTERDOWN告知其存活状态等。如果一台Sentinel在30s中内没有收到Master的应答，会认为Master已经处于SDOWN状态同时会询问其他Sentinel此Master是否SDOWN,如果quonum台Sentinels认为Master已经SDOWN,那么认为Master是真的挂掉（ODOWN），此时会选出一个状态正常且与Master的连接没有断开太久的Slave作为新的Master。</p>

<p> Redis Sentinel提供了notify脚本机制可以接受任何pub/sub消息，以便于发出故障告警等信息；提供了reconfig脚本机制在Slave开始提升成Master、所有Slave都已指向新Master、提升被终止等情况下触发对此类脚本的调用，可以实现一些自定义的配置逻辑。</p></li>
<li><p>Redis Cluster</p>

<p> Redis 3.0后内置的集群方案。此方案没有中心节点的，每一个Redis实例都负责一部分slot（存储一部分key），业务应用需要通过Redis Cluster客户端程序对数据进行操作。客户端可以向任一实例发出请求，如果所需数据不在该实例中，则该实例引导客户端去对应实例读写数据。Redis Cluster的成员管理（节点名称、IP、端口、状态、角色）等，都通过节点之间两两通讯，基于Gossip协议定期交换并更新。是一种比较重的集群方案。</p>

<p> Redis的集群方案除了内置的Redis Cluster之外，很多公司都采用基于代理中间件的思路做了一些实现，Twemproxy、Codis是其中用的比较多的软件。相比起官方的集群方案，其使用方式和单点Redis是一模一样的，原有的业务改动很少（个别命令会不支持），且其数据存储和分布式逻辑是分离的便于扩展和升级。</p></li>
<li><p>客户端分片</p>

<p> 除了上述集群方案之外，在客户端做分片也是一种常用的Redis集群实现方式，不依赖于第三方分布式中间件，实现方法和代码都自己掌控，相比代理方式少了中间环节。但是此方式数据迁移、合并等都不够灵活，建议慎用。Jedis2.0开始就提供了ShardedJedis实现客户端分片，但实际应用并不多见。</p></li>
</ol>


<h3>使用提示</h3>

<h3>Redis数据操作</h3>

<ul>
<li>不同业务共用同一Redis实例时，务必使用前缀来区分各个key，以防止key冲突覆盖。</li>
<li>尽量减少字符串频繁修改操作如append，setrange, 改为直接使用set修改字符串，可以降低预分配带来的内存浪费和内存碎片化。</li>
<li>不要在大数据量线上环境中使用keys命令，很容易造成Redis阻塞。</li>
<li>缓存的失效时间不要集中在同一时刻，会导致缓存占满内存触发内存淘汰（占用CPU）或者直接导致缓存雪崩。</li>
<li>String类型在1KB（Redis官方测试）是一个吞吐量性能拐点，因此String类型的大小以1KB以内为宜（局域网环境下，1KB以内吞吐性能基本一致），最大不超过10KB。</li>
<li>SortedSet中元素的score使用双精度64位浮点数，取值范围为-(2<sup>53</sup>)到+(2<sup>53</sup>)。更大的整数在内部用指数形式表示，因此如果为分数设置一个非常大的整数，其本质是一个近似的十进制数。</li>
<li>尽量使用mset、hmset等做批量操作，以节省网络IO消耗。此外，lpush、rpush、sadd也支持一次输入多个value，同样可以节省网络IO。但需要注意单次请求操作的数量尽量控制在500以内，从而避免慢查询。</li>
<li>使用Redis的事务命令（multi、exec、discard）, 其事务级别类似于Read Committed，即事务无法看到其他事务未提交的改动。还可以使用watch对某一个key做监控，当key对应的值被改变时，事务会被打断，能够达到CAS的效果。但需要注意的是Redis的事务和关系型数据库的事务不同，并非严格的ACID事务，仅仅能达到Isolation。</li>
<li>在Java中使用Jedis的pipeline一次执行多条互相没有依赖关系的命令可以节省网络IO的成本，但pipeline和事务不同，其只是一种批量写批量读的多命令流水线机制，Redis服务器并不保证这些命令的原子性。</li>
<li>可以使用SortedSet做范围查询，如：使用日期作为score,那么就可以根据日期来查询。此外，还可以在范围数据中进行查询，例如：IP定位库的数据一般是某一段IP范围属于哪一个城市,那么可以使用SortedSet存储每一段范围的最小IP和最大IP做为score，城市做为memeber。当给定一个IP时，根据score先找出大于这个IP的最小值，再找出小于这个IP的最大值，如果两者对应的城市相同，即完成定位，否则，无法获取到位置信息。</li>
<li>使用List做队列时，如果需要ack, 可以考虑再使用一个SortedSet，每次队列中pop出一个元素则按照访问时间将其存储到SortedSet中，消费完后进行删除。</li>
<li>控制集合键数据（list、set、zset、hash）的元素个数在5000以内，防止造成大key的查询阻塞其他请求的处理。可以使用zsan、hsan、sscan进行渐进操作或者分拆key来处理。</li>
<li>当无法避免对大集合键数据（元素非常多）进行全量读取时，可以通过搭建多个slave来提升性能，也可以使用Memcached作为Redis前面全量读取的缓存，从而利用MC的多线程实现方式以及对二进制KV的高效读取来获得性能的提升。</li>
<li><p>对大集合键数据的删除避免使用del，会造成Redis阻塞。</p>

<ul>
<li>hash: 通过hscan命令，每次获取一部分字段，再用hdel命令，每次删除1个字段。</li>
<li>list： 使用ltrim命令每次删除少量元素。</li>
<li>set: 使用sscan命令，每次扫描集合中一部分元素，再用srem命令每次删除一个键。</li>
<li>zset: 使用zremrangebyrank命令,每次删除top 100个元素。</li>
</ul>
</li>
<li><p>在Java开发中一般选择直接使用Jedis即可。如果需要诸如分布式锁、主从等分布式特性或者应用层级的Redis操作封装（布隆过滤器、队列），可以选择使用Redisson库来操作Redis。此外，Spring Data Redis也是一种选择，在4.2.2中做过讲述。</p></li>
</ul>


<h3>配置与监控</h3>

<ul>
<li>可以通过monitor命令监测Redis上命令执行的情况。</li>
<li>使用redis-cli &ndash;bigkeys可以扫描出每种数据类型最大的key。</li>
<li>由于Redis自身单线程的原因，切忌慢查询会阻塞住整个Redis, 可以通过slowlog get来查看慢查询日志。</li>
<li>设置Redis最大内存，以防内存用爆。</li>
<li>使用redis-rdb-tools对rdb文件进行分析，如每条key对应value所占的大小，从而做量化分析。</li>
<li>可以使用Redis Sampler，统计Redis中的数据分布情况。</li>
<li>Redis的最大连接数默认为10000（通过命令CONFIG GET maxclients得到），可以在redis.conf配置（maxclients: 10000）。如果还是有限制，需要考虑修改系统的单个进程可打开的最大文件个数（ulimit -n）以及网络的并发连接数。</li>
<li>单点Redis的性能一般能够达到10万QPS左右。</li>
</ul>


<h2>缓存设计</h2>

<p>在使用缓存系统的时候，还需要考虑缓存设计的问题，重点在于缓存失效时的处理和如何更新缓存。</p>

<p>缓存失效是在使用缓存时不得不面对的问题。在业务开发中，缓存失效由于找不到整个数据，一般会出于容错考虑，从存储层再进行查询，如果有则放入缓存。如果查找的数据压根在存储层就不存在，缓存失去意义，还给后端服务带来了巨大的请求压力，会进一步引起雪崩效应。这种现象又称为缓存穿透。</p>

<p>目前常用的解决缓存穿透问题的方案如下：</p>

<ol>
<li>在底层存储系统之上加一层布隆过滤器，将所有可能存在的数据哈希到一个足够大的BitMap中，一个一定不存在的数据会被这个BitMap拦截掉，从而避免了对底层存储系统的查询压力。</li>
<li>如果数据在存储层查询也为空，那么对此空结果也进行缓存，但要设置合适的失效时间。</li>
</ol>


<p>更进一步的，解决缓存穿透的问题其实是和缓存的更新机制是相关的。缓存更新的常用三种模式如下：</p>

<ul>
<li>Cache Aside Pattern: 应用程序以数据库为准，失效则从底层存储更新，更新数据先写入数据库再更新缓存。是最常用的缓存更新模式。</li>
<li>Read/Write Through Pattern: 以缓存为准，应用只读写缓存，但是需要保证数据同步更新到了数据库中。</li>
<li>Write Behind Caching Pattern: 以缓存为准，应用只读写缓存，数据异步更新到数据库，不保证数据正确写回，会丢数据。可以采用Write Ahead Logging等机制避免丢数据。</li>
</ul>


<p>如上，在缓存失效时采用何种策略去更新缓存直接决定了能否解决缓存穿透的问题。Cache Aside Pattern中缓存失效则从底层存储更新无法避免缓存穿透的问题。基于以上三种模式采用下面更为细化的更新机制可以在一定程度上避免缓存穿透的问题：</p>

<ol>
<li>缓存失效时，用加锁或者队列的方式单线程/进程去更更新缓存并等待结果。</li>
<li>缓存失效时，先使用旧值，同时异步（控制为同时只有一个线程/进程）更新缓存，缓存更新失败则抛出异常。</li>
<li>缓存失效时，先使用旧值，同时异步（控制为同时只有一个线程/进程）更新缓存，缓存更新失败延续旧值的有效期。</li>
<li>数据写入或者修改时，更新数据存储后再更新缓存。缓存失效时即认为数据不存在。</li>
<li>数据写入或者修改时，只更新缓存，使用单独线程周期批量刷新缓存到底层存储。缓存失效时即认为数据不存在。此种方案不能保障数据的安全性，有可能会丢数据。</li>
<li>采用单独线程/进程周期将数据从底层存储放到缓存中（MySQL可以基于binlog增量更新缓存）。缓存失效时即认为数据不存在。此种方案无法保证缓存数据和底层存储的数据强一致性。</li>
</ol>


<p>如果一开始设计缓存结构的时候注意切分粒度，把缓存力度划分的细一点，那么缓存命中率相对会越高，也能在一定程度上避免缓存穿透的问题。</p>

<p>此外，还可以在后端做流量控制、服务降级或者动态扩展，以应对缓存穿透带来的访问压力。</p>

<blockquote><p>本文节选自《Java工程师修炼之道》一书。</p></blockquote>
]]></content>
  </entry>
  
</feed>
