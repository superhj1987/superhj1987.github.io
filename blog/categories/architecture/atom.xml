<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: architecture | 后端技术杂谈 | 飒然Hang]]></title>
  <link href="http://www.rowkey.me/blog/categories/architecture/atom.xml" rel="self"/>
  <link href="http://www.rowkey.me/"/>
  <updated>2018-11-02T11:54:40+08:00</updated>
  <id>http://www.rowkey.me/</id>
  <author>
    <name><![CDATA[飒然Hang]]></name>
    <email><![CDATA[superhj1987@126.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Java应用性能调优之调优准备]]></title>
    <link href="http://www.rowkey.me/blog/2018/10/31/profile-ready/"/>
    <updated>2018-10-31T22:29:34+08:00</updated>
    <id>http://www.rowkey.me/blog/2018/10/31/profile-ready</id>
    <content type="html"><![CDATA[<h2>调优概述</h2>

<p>实际的开发工作中，有时候会遇到程序突然变得响应缓慢或者进程消失的情况。这时候就需要对程序进行问题排查和调优，找出产生问题的根源，并进行优化。</p>

<p>一般来说，影响程序性能的因素主要有以下几个：</p>

<ul>
<li>硬件配置</li>
<li>操作系统</li>
<li>应用程序</li>
</ul>


<p>CPU、内存、硬盘等硬件配置从根本上决定了应用的性能，而对于网络应用，带宽也是影响其性能的关键因素。本章暂且抛开这些方面不讲，主要讲述针对操作系统和应用程序的调优方法。又可以分为：</p>

<ul>
<li>系统调优: 针对操作系统的配置调优。</li>
<li>Java调优：针对Java应用的调优，包括JVM和代码。</li>
<li>外部系统调优：针对应用程序依赖的外部系统如数据库、缓存、Web服务器等的调优。</li>
</ul>


<!--more-->


<p>共性的，无论是哪一个层面的调优，性能调优都可以分为以下三个步骤：</p>

<ul>
<li>性能监控：此步骤可以描述为&#8221;我并不知道我要做什么&#8221;，在系统没有出现问题以前，是没有调优的动机和需求的（当然凭借经验预判除外）。需要监控机制来发现、暴露系统的性能问题。这里一般依赖于系统级别或者业务级别的监控工具。</li>
<li>性能分析：此步骤可以描述为&#8221;我知道我要做什么&#8221;，当性能监控发现问题的时候，那么调优的动机就来了。这时你就知道该要去解决什么样的问题了。带着这个问题去做各个层面的分析。此步骤则需要一些性能分析工具。</li>
<li>性能调优：此步骤可以描述为“我知道我需要知道什么了”，经过性能分析，你最终会知道是什么原因造成了问题，该需要怎么做才能够使得性能提高。这一步通常是需要系统、程序参数的调整、代码的重构优化等。</li>
</ul>


<p>此外，这里给出一个当系统发生问题时（不仅仅是性能问题）的两个应对原则：</p>

<ol>
<li>首先如果团队中有人曾经处理过类似的问题或者确定能够通过某种手段恢复系统的正常运行，那么应该第一时间恢复系统（回滚、重启等），同时务必要保留现场，以备后续对问题的定位和修复；如果没有人有经验，则需要使用比较粗暴的办法保证服务可用，如定时重启、限流等。</li>
<li><p>在定位问题的过程中需要业务负责人、技术负责人、核心研发人员、架构师、运维工程师以及运营人员都要参与，一起分析可能导致问题的原因。分析的过程需要首先考虑近期的各种变动，包括以下几方面：</p>

<ul>
<li>系统最近是否进行了发布上线工作？</li>
<li>服务的使用方是否有运营活动？</li>
<li>网络是否有流量的波动？</li>
<li>最近的业务量是否上升？</li>
<li>运营人员是否在系统做了变动？</li>
<li>依赖的基础平台和资源是否进行了发布上线？</li>
<li>依赖的其他系统是否进行了发布上线？</li>
</ul>
</li>
</ol>


<h2>调优准备</h2>

<p>调优是需要做好准备工作的，毕竟每一个应用的业务目标都不尽相同，性能瓶颈也不会总在同一个点上。在业务应用层面，我们需要：</p>

<ul>
<li>需要了解系统的总体架构，明确压力方向。比如系统的哪一个接口、模块是使用率最高的，面临高并发的挑战。</li>
<li>需要构建测试环境来测试应用的性能，使用ab、loadrunner、jmeter、faban都可以。</li>
<li>对关键业务数据量进行分析，这里主要指的是对一些数据的量化分析，如数据库一天的数据量有多少、缓存的数据量有多大等。</li>
<li>了解系统的响应速度、吞吐量、TPS、QPS等指标需求，比如秒杀系统的相应速度和QPS是要求非常高的。</li>
<li>了解系统相关软件的版本、模式和参数等，有时候限于应用依赖服务的版本、模式等，性能也会受到一定的影响。</li>
</ul>


<p>此外，还需要做以下几个准备工作：</p>

<ol>
<li>了解Java内存相关：<a href="http://www.rowkey.me/blog/2016/05/07/javamm/">http://www.rowkey.me/blog/2016/05/07/javamm/</a>。</li>
<li>对Java代码进行基准性能测试：可以使用JMH来进行，其是一个微基准测试框架，使用其进行性能测试，能够去除JIT热点代码编译对性能的影响。</li>
<li>了解HotSpot虚拟机体系结构。</li>
<li>对系统性能进行调优。</li>
<li>了解JVM关键参数的配置。</li>
</ol>


<h2>调优准备之HotSpot虚拟机</h2>

<p>HotSpot虚拟机是Java开发用的最多的JVM，了解其体系结构，才能够从原理层面更好的理解性能问题，也才能更好地对Java应用进行调优。</p>

<ol>
<li><p>HotSpot VM主要由垃圾回收器、JIT编译器以Runtime组成。</p>

<p> <img src="//post_images/profile/hotspot.jpg" alt="" /></p></li>
<li><p>HotSpot VM的运行时架构包括类加载器、执行引擎以及运行时数据区。</p>

<p> <img src="//post_images/profile/hotspot-runtime.jpg" alt="" /></p>

<p> 如图，Java源码被编译器编译为JVM字节码后进入JVM, 由类加载器进行加载，并交给执行引擎执行，期间的数据都放入运行时数据区。</p>

<p> 这里需要注意的是，JIT编译器是执行引擎中非常影响应用性能的组件，它会把热点代码直接编译为本地机器码，从而提高运行时的性能。此外，垃圾回收器执行GC的时机、效率对应用性能的影响也非常关键。</p></li>
</ol>


<p>HotSpot VM内部有一些线程进行JVM的管理、监控、垃圾回收工作。主要包括：</p>

<ul>
<li>VM thread：这个线程是JVM里面的线程母体，是一个单例的对象（最原始的线程）会产生或触发所有其他的线程，这个单个的VM线程是会被其他线程所使用来做一些VM操作（如，清扫垃圾等）。</li>
<li>Periodic task thread：该线程是JVM周期性任务调度的线程，它由WatcherThread创建，是一个单例对象。</li>
<li>Garbage collection threads： 进行垃圾回收的线程。</li>
<li>JIT compiler threads: 进行JIT编译的线程。</li>
<li>Signal dispatcher thread: 当外部jvm命令接收成功后，会交给signal dispather线程去进行分发到各个不同的模块处理命令，并且返回处理结果。</li>
</ul>


<p>这些线程的运行有时候会影响业务线程的运行，是影响应用性能的关键因素。</p>

<h2>调优准备之系统性能调优</h2>

<p>后端应用都是需要部署在服务器上的，因此在对Java应用调优之前务必先将系统的性能调整到一个相对较好的水平。</p>

<p>一般来说，目前后端系统都是部署在Linux主机上的。所以抛开Win系列不谈，对于Linux系统来说一般有以下配置关系着系统的性能。</p>

<ul>
<li>文件描述符数限制：Linux中所有东西都是文件，一个socket就对应着一个文件描述符，因此系统配置的最大打开文件数以及单个进程能够打开的最大文件数就决定了socket的数目上限。</li>
<li>进程/线程数限制: 对于Apache使用的prefork等多进程模式，其负载能力由进程数目所限制。对Tomcat多线程模式则由线程数所限制。</li>
<li>TCP内核参数：网络应用的底层自然离不开TCP/IP，Linux内核有一些与此相关的配置也决定了系统的负载能力。</li>
</ul>


<h3>文件描述符数限制</h3>

<ul>
<li><p>系统最大打开文件描述符数：/proc/sys/fs/file-max中保存了这个数目,可修改此值。</p>

<pre><code>  临时性
      echo 1000000 &gt; /proc/sys/fs/file-max
  永久性：在/etc/sysctl.conf中设置
      fs.file-max = 1000000
</code></pre></li>
<li><p>进程最大打开文件描述符数：这个是配置单个进程能够打开的最大文件数目。可以通过ulimit -n查看和修改。如果想要永久修改，则需要修改/etc/security/limits.conf中的nofile选项。</p></li>
</ul>


<p>通过读取/proc/sys/fs/file-nr可以看到当前使用的文件描述符总数。另外，对于文件描述符的配置，需要注意以下几点：</p>

<ul>
<li>所有进程打开的文件描述符数不能超过/proc/sys/fs/file-max。</li>
<li>单个进程打开的文件描述符数不能超过user limit中nofile的soft limit。</li>
<li>nofile的soft limit不能超过其hard limit。</li>
<li>nofile的hard limit不能超过/proc/sys/fs/nr_open。</li>
</ul>


<h3>进程/线程数限制</h3>

<ul>
<li>进程数限制：ulimit -u可以查看/修改单个用户能够打开的最大进程数。/etc/security/limits.conf中的noproc则是系统的最大进程数。</li>
<li><p>线程数限制：</p>

<ul>
<li>可以通过/proc/sys/kernel/threads-max查看系统总共可以打开的最大线程数。</li>
<li>单个进程的最大线程数和PTHREAD_THREADS_MAX有关，此限制可以在/usr/include/bits/local_lim.h中查看,但是如果想要修改的话，需要重新编译Linux系统。</li>
<li>Linux内核2.4的线程实现方式为Linux threads，是轻量级进程，会首先创建一个管理线程，线程数目的大小是受PTHREAD_THREADS_MAX影响的。Linux2.6内核的线程实现方式变为NPTL,是一个改进的LWP实现，与Linux thread最大的区别就是其线程公用进程的pid（tgid），线程数目大小只受制于资源。</li>
<li>线程数的大小还受线程栈大小的制约：使用ulimit -s可以查看/修改线程栈的大小，即每开启一个新的线程需要分配给此线程的一部分内存。减小此值可以增加可以打开的线程数目。</li>
</ul>
</li>
</ul>


<h3>TCP内核参数</h3>

<p>在一台服务器CPU和内存资源有限的情况下，最大的压榨服务器的性能，是最终的目的。在节省成本的情况下，可以考虑修改Linux的内核TCP/IP参数，来最大的压榨服务器的性能。如果通过修改内核参数也无法解决的负载问题，也只能考虑升级服务器。</p>

<pre><code>netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'
</code></pre>

<p>使用上面的命令，可以得到当前系统的各个状态的网络连接的数目。如下：</p>

<pre><code>LAST_ACK 13
SYN_RECV 468
ESTABLISHED 90
FIN_WAIT1 259
FIN_WAIT2 40
CLOSING 34
TIME_WAIT 28322
</code></pre>

<p>这里，TIME_WAIT的连接数是需要注意的一点。此值过高会占用大量连接，影响系统的负载能力。需要调整参数，以尽快的释放TIME_WAIT连接。</p>

<p>一般TCP相关的内核参数在/etc/sysctl.conf文件中。为了能够尽快释放TIME_WAIT状态的连接，可以做以下配置：</p>

<ul>
<li>net.ipv4.tcp_syncookies = 1，表示开启SYN Cookies。当出现SYN等待队列溢出时，启用Cookies来处理，可防范少量SYN攻击，默认为0，表示关闭。</li>
<li>net.ipv4.tcp_tw_reuse = 1，表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭。</li>
<li>net.ipv4.tcp_tw_recycle = 1，表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭；</li>
<li>net.ipv4.tcp_fin_timeout = 30，修改系統默认的 TIMEOUT 时间。</li>
</ul>


<p>这里需要注意的一点就是当打开了tcp_tw_recycle，就会检查时间戳，移动环境下的发来的包的时间戳有些时候是乱跳的，会把带了“倒退”的时间戳的包当作是“recycle的tw连接的重传数据，不是新的请求”，于是丢掉不回包，造成大量丢包。另外，当前面有LVS，并且采用的是NAT机制时，开启tcp_tw_recycle也会造成一些异常。如果这种情况下仍然需要开启此选项，那么可以考虑设置net.ipv4.tcp_timestamps=0，忽略掉报文的时间戳即可。</p>

<p>此外，还可以通过优化tcp/ip的可使用端口的范围，进一步提升负载能力。如下：</p>

<ul>
<li>net.ipv4.tcp_keepalive_time = 1200，表示当keepalive启用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为20分钟。</li>
<li>net.ipv4.ip_local_port_range = 10000 65000，表示用于向外连接的端口范围。缺省情况下很小：32768到61000，改为10000到65000。这里需要注意不要将最低值设的太低，否则可能会占用掉正常的端口。</li>
<li>net.ipv4.tcp_max_syn_backlog = 8192，表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。</li>
<li>net.ipv4.tcp_max_tw_buckets = 5000，表示系统同时保持TIME_WAIT的最大数量，如果超过这个数字，TIME_WAIT将立刻被清除并打印警告信息。默认为180000，改为5000，可以很好地减少TIME_WAIT套接字数量。</li>
</ul>


<h2>调优准备之系统常用诊断工具</h2>

<p>当应用运行情况、响应情况异常时，会直接表现为系统的指标异常。而指标需要通过相关的系统命令来获取。Linux系统下常用的诊断工具如下：</p>

<ol>
<li><p>uptime</p>

<p> 使用uptime可以快速查看服务器的负载情况。</p>

<pre><code class="`"> 00:40:16 up 116 days,  5:28,  1 user,  load average: 0.36, 0.32, 0.32
</code></pre>

<p> 此命令返回的是系统的平均负荷，包括1分钟、5分钟、15分钟内可以运行的任务平均数量，包括正在运行的任务以及虽然可以运行但正在等待某个处理器空闲的任务。当然，这个值是和CPU核数有关的，双核的机器，load只要小于2也是正常的状况。CPU的情况可以通过查看/proc/cpuinfo来获得。</p>

<p> 如果1分钟平均负载很高，而15分钟平均负载很低，说明服务器正在命令高负载情况，需要进一步排查CPU资源都消耗在了哪里。反之，如果15分钟平均负载很高，1分钟平均负载较低，则有可能是CPU资源紧张时刻已经过去。</p>

<p> 这里需要注意的是在Linux下平均负载除了包括等待CPU和正在使用CPU的进程的数量以外，还包括阻塞在不可中断休眠状态的进程（进程状态为D，通常是在等待IO）的数量。因此当负载变高的时候，并不一定是可运行的进程数太多，也有可能是IO瓶颈导致不可中断IO的进程数过多造成的。</p></li>
<li><p>dmesg丨tail</p>

<pre><code class="`"> e1000: eth0 NIC Link is Up 1000 Mbps Full Duplex, Flow Control: None
 ADDRCONF(NETDEV_UP): eth0: link is not ready
 ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
 e1000: eth1 NIC Link is Up 1000 Mbps Full Duplex, Flow Control: None
 eth0: no IPv6 routers present
 eth1: no IPv6 routers present
 e1000: eth0 NIC Link is Up 1000 Mbps Full Duplex, Flow Control: None
 e1000: eth1 NIC Link is Up 1000 Mbps Full Duplex, Flow Control: None
 eth0: no IPv6 routers present
 eth1: no IPv6 routers present
</code></pre>

<p> 该命令会输出系统日志的最后10行。常见的OOM kill和TCP丢包在这里都会有记录。</p></li>
<li><p>vmstat 1</p>

<p> vmstat是一个实时性能检测工具,可以展现给定时间间隔的服务器的状态值,包括服务器的CPU使用率、内存使用、虚拟内存交换情况、IO读写情况等系统核心指标。其输出结果如下：</p>

<pre><code class="`"> procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------

    r b swpd free buff cache si so bi bo in cs us sy id wa st

    0 0 0 7887984 1320604 6288252 0 0 0 2 0 1 0 0 100 0 0
</code></pre>

<p>  一般主要关注输出的CPU使用情况，其中id + us + sy = 100，id是空闲CPU使用率，us 是用户CPU使用率，sy是系统CPU使用率。如果用户时间和系统时间相加非常大，说明CPU正忙于执行指令。而如果IO等待时间很长，那么系统的瓶颈可能在磁盘IO。当然，这里输出的IO信息、上下文切换信息也很有用。</p>

<p> 在计算CPU利用率的时候，建议多获取几次，尤其是在脚本里获取时，一般只获取一次是不准确的，建议在脚本里取两次以上并排除掉第一次的数据。而如果是排查最近耗费CPU最多的进程，使用Top的数据比较合理。</p></li>
<li><p>mpstat -P ALL 1</p>

<p> 该命令用来显示每个CPU的使用情况。如果有一个CPU占用率特别高，说明有可能是一个单线程应用程序引起的。</p>

<pre><code class="`"> Linux 2.6.18-194.el5 (xx) 08/01/2017

 12:54:59 AM  CPU   %user   %nice    %sys %iowait    %irq   %soft  %steal   %idle    intr/s
 12:55:00 AM  all    4.74    0.00    1.37    0.00    0.25    1.25    0.00   92.39   3593.07
 12:55:00 AM    0    2.97    0.00    0.99    0.00    0.00    0.00    0.00   96.04    992.08
 12:55:00 AM    1    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00      0.00
 12:55:00 AM    2    1.96    0.00    0.98    0.00    0.00    0.98    0.00   96.08      0.00
 12:55:00 AM    3    2.00    0.00    1.00    0.00    0.00    0.00    0.00   97.00      0.00
 12:55:00 AM    4    2.00    0.00    0.00    0.00    0.00    0.00    0.00   98.00      0.00
 12:55:00 AM    5    2.02    0.00    1.01    0.00    0.00    0.00    0.00   96.97      0.99
 12:55:00 AM    6    4.00    0.00    0.00    0.00    0.00    0.00    0.00   96.00      0.00
 12:55:00 AM    7   23.00    0.00    7.00    0.00    3.00    9.00    0.00   58.00   2599.01
</code></pre></li>
<li><p>free -m</p>

<p> 该命令可以查看系统内存的使用情况，-m参数表示按照兆字节展示。如果可用内存非常少，系统可能会动用交换区（swap），会增加IO开销（可以在iostat命令中体现），降低系统性能。</p>

<pre><code class="`">              total       used       free     shared    buffers     cached
 Mem:         16051      15879        171          0        672       4763
 -/+ buffers/cache:      10444       5607
 Swap:         8001          0       8000
</code></pre>

<p> 这里需要注意的是第一行的信息是针对整个系统来说的，因此Buffer和Cache都被计算在了used里面，其实这两部分内存是可以被很快拿来供应用程序使用的。因此，真正反映内存使用状况的是第二行。</p></li>
<li><p>sar -n DEV 1</p>

<p> sar命令主要用来查看网络设备的吞吐率。可以通过网络设备的吞吐量，判断网络设备是否已经饱和。</p>

<pre><code class="`"> Linux 2.6.18-194.el5 (test-172-16-0-137-ip)     08/01/2017

 01:06:51 AM     IFACE   rxpck/s   txpck/s   rxbyt/s   txbyt/s   rxcmp/s   txcmp/s  rxmcst/s
 01:06:52 AM        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00
 01:06:52 AM      eth0   1162.00   1286.00 223033.00 323209.00      0.00      0.00      0.00
 01:06:52 AM      eth1      1.00      0.00     92.00      0.00      0.00      0.00      0.00
 01:06:52 AM      sit0      0.00      0.00      0.00      0.00      0.00      0.00      0.00

 Average:        IFACE   rxpck/s   txpck/s   rxbyt/s   txbyt/s   rxcmp/s   txcmp/s  rxmcst/s
 Average:           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00
 Average:         eth0   1162.00   1286.00 223033.00 323209.00      0.00      0.00      0.00
 Average:         eth1      1.00      0.00     92.00      0.00      0.00      0.00      0.00
 Average:         sit0      0.00      0.00      0.00      0.00      0.00      0.00      0.00
</code></pre></li>
<li><p>top</p>

<p> top命令包含了系统全局的很多指标信息，包括系统负载情况、系统内存使用情况、系统CPU使用情况等等，基本涵盖了上述几条命令的功能。</p>

<pre><code class="`"> top - 01:02:04 up 116 days,  5:50,  1 user,  load average: 1.20, 0.46, 0.29
 Tasks: 152 total,   2 running, 150 sleeping,   0 stopped,   0 zombie
 Cpu(s):  2.7%us,  0.5%sy,  0.0%ni, 96.3%id,  0.0%wa,  0.1%hi,  0.4%si,  0.0%st
 Mem:  16436664k total, 16302392k used,   134272k free,   688420k buffers
 Swap:  8193140k total,      132k used,  8193008k free,  4878348k cached

   PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
 27852 root      20   0  180m  36m  548 R 56.5  0.2   0:00.29 python
  4259 root      18   0 2396m 815m  13m S 25.3  5.1   7988:19 java
 14128 root      18   0 4603m 1.9g  15m S  3.9 12.4 572:23.01 java
 14785 root      24   0 5554m 2.1g  25m S  1.9 13.2  36:42.81 java
 27851 root      15   0 12744 1048  748 R  1.9  0.0   0:00.02 top
     1 root      15   0 10356  684  576 S  0.0  0.0   0:06.18 init
     2 root      RT  -5     0    0    0 S  0.0  0.0   0:08.14 migration/0
     3 root      34  19     0    0    0 S  0.0  0.0   0:00.17 ksoftirqd/0
     4 root      RT  -5     0    0    0 S  0.0  0.0   0:08.73 migration/1
     5 root      34  19     0    0    0 S  0.0  0.0   0:00.17 ksoftirqd/1
</code></pre>

<p> 通过此命令，可以相对全面的查看系统负载的来源。同时，top命令支持排序，可以按照不同的列排序，方便查找出诸如内存占用最多的进程、CPU占用率最高的进程等。但top命令是一个瞬时输出的值，最好是通过定时存储到文件来进行对比诊断。需要注意的是，对于每一个进程的%CPU这一列，其默认是Irix Mode，即在多处理器环境下，其为占的总的CPU的使用率，例如，4核CPU中%CPU最高值是400%。可以使用I指令切换模式为Solaris Mode，此值则变为单CPU衡量的一个值，最大值为100%。</p>

<p> 需要注意的是，使用ps命令也能够拿到某个进程的CPU使用率，但是其是从进程创建开始就计算，为该进程处于Running状态的时间占进程总时间的百分比，可以看做平均CPU使用率。而Top的%CPU是不断刷新计算的（数据来源于/proc/pid/stats），可以认为是实时的。</p></li>
</ol>


<h2>调优准备之JDK常用诊断工具</h2>

<p>在对Java程序进行问题排查、性能调优时，如果没有合适的工具，很多时候会事倍功半，甚至无法继续进行下去。JDK自身已经提供了很多强大的工具供我们使用。</p>

<p>笔者的开发环境是：OS X EI Captian 10.11.6</p>

<p>JDK版本：</p>

<pre><code>java version "1.8.0_92"
Java(TM) SE Runtime Environment (build 1.8.0_92b14)
Java HotSpot(TM) 64Bit Server VM (build 25.92b14, mixed mode)
</code></pre>

<p>JAVA_HOME/bin下的工具截图如下：</p>

<p><img src="//post_images/profile/jdk-tools.png" alt="" /></p>

<p>和性能分析相关的工具如下表所示：</p>

<table>
<thead>
<tr>
<th>工具 </th>
<th> 描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>javap</td>
<td> Java反编译工具，主要用于根据Java字节码文件反汇编为Java源代码文件。</td>
</tr>
<tr>
<td>jcmd</td>
<td> Java 命令行（Java Command），用于向正在运行的JVM发送诊断命令请求。</td>
</tr>
<tr>
<td>jconsole</td>
<td> 图形化用户界面的监测工具，主要用于监测并显示运行于Java平台上的应用程序的性能和资源占用等信息。</td>
</tr>
<tr>
<td>jdeps</td>
<td> 用于分析Java class的依赖关系。</td>
</tr>
<tr>
<td>jdb</td>
<td> Java调试工具（Java Debugger），主要用于对Java应用进行断点调试。</td>
</tr>
<tr>
<td>jhat</td>
<td> Java堆分析工具（Java Heap Analysis Tool），用于分析Java堆内存中的对象信息。</td>
</tr>
<tr>
<td>jinfo</td>
<td> Java配置信息工具（Java Configuration Information），用于打印指定Java进程、核心文件或远程调试服务器的配置信息。</td>
</tr>
<tr>
<td>jmap</td>
<td> Java内存映射工具（Java Memory Map），主要用于打印指定Java进程、核心文件或远程调试服务器的共享对象内存映射或堆内存细节。</td>
</tr>
<tr>
<td>jmc</td>
<td> Java任务控制工具（Java Mission Control），主要用于HotSpot JVM的生产时间监测、分析、诊断。开发者可以使用jmc命令来创建JMC工具。</td>
</tr>
<tr>
<td>jps</td>
<td> JVM进程状态工具（JVM Process Status Tool），用于显示目标系统上的HotSpot JVM的Java进程信息。</td>
</tr>
<tr>
<td>jrunscript</td>
<td> Java命令行脚本外壳工具（command line script shell），主要用于解释执行javascript、groovy、ruby等脚本语言。</td>
</tr>
<tr>
<td>jstack</td>
<td> Java堆栈跟踪工具，主要用于打印指定Java进程、核心文件或远程调试服务器的Java线程的堆栈跟踪信息。</td>
</tr>
<tr>
<td>jstat</td>
<td> JVM统计监测工具（JVM Statistics Monitoring Tool），主要用于监测并显示JVM的性能统计信息，包括gc统计信息。</td>
</tr>
<tr>
<td>jstatd</td>
<td> jstatd（VM jstatd Daemon）工具是一个RMI服务器应用，用于监测HotSpot JVM的创建和终止，并提供一个接口，允许远程监测工具附加到运行于本地主机的JVM上。</td>
</tr>
<tr>
<td>jvisualvm</td>
<td> JVM监测、故障排除、分析工具，主要以图形化界面的方式提供运行于指定虚拟机的Java应用程序的详细信息。</td>
</tr>
</tbody>
</table>


<blockquote><p>本文节选自《Java工程师修炼之道》一书。</p></blockquote>

<p><img src="http://www.rowkey.me/post_images/book-all.png" width="400"/></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[架构设计与原则（2018修订版）]]></title>
    <link href="http://www.rowkey.me/blog/2018/09/20/arch-new/"/>
    <updated>2018-09-20T22:29:34+08:00</updated>
    <id>http://www.rowkey.me/blog/2018/09/20/arch-new</id>
    <content type="html"><![CDATA[<p>之前的<a href="http://www.rowkey.me/blog/2017/08/24/arch/">《谈谈架构》</a>的最新修订版。</p>

<!--more-->


<p><img src="//post_images/arch-new/arch-2.jpeg" alt="" />
<img src="//post_images/arch-new/arch-3.jpeg" alt="" />
<img src="//post_images/arch-new/arch-4.jpeg" alt="" />
<img src="//post_images/arch-new/arch-5.jpeg" alt="" />
<img src="//post_images/arch-new/arch-6.jpeg" alt="" />
<img src="//post_images/arch-new/arch-7.jpeg" alt="" />
<img src="//post_images/arch-new/arch-8.jpeg" alt="" />
<img src="//post_images/arch-new/arch-9.jpeg" alt="" />
<img src="//post_images/arch-new/arch-10.jpeg" alt="" />
<img src="//post_images/arch-new/arch-11.jpeg" alt="" />
<img src="//post_images/arch-new/arch-12.jpeg" alt="" />
<img src="//post_images/arch-new/arch-13.jpeg" alt="" />
<img src="//post_images/arch-new/arch-14.jpeg" alt="" />
<img src="//post_images/arch-new/arch-15.jpeg" alt="" />
<img src="//post_images/arch-new/arch-16.jpeg" alt="" />
<img src="//post_images/arch-new/arch-17.jpeg" alt="" />
<img src="//post_images/arch-new/arch-18.jpeg" alt="" />
<img src="//post_images/arch-new/arch-19.jpeg" alt="" />
<img src="//post_images/arch-new/arch-20.jpeg" alt="" />
<img src="//post_images/arch-new/arch-21.jpeg" alt="" />
<img src="//post_images/arch-new/arch-22.jpeg" alt="" />
<img src="//post_images/arch-new/arch-23.jpeg" alt="" />
<img src="//post_images/arch-new/arch-24.jpeg" alt="" />
<img src="//post_images/arch-new/arch-25.jpeg" alt="" />
<img src="//post_images/arch-new/arch-26.jpeg" alt="" />
<img src="//post_images/arch-new/arch-27.jpeg" alt="" />
<img src="//post_images/arch-new/arch-28.jpeg" alt="" />
<img src="//post_images/arch-new/arch-29.jpeg" alt="" /></p>

<p><strong>附录链接：</strong></p>

<ul>
<li><a href="https://www.ibm.com/developerworks/cn/rational/06/r-wenyu/index.html">软件架构 &ldquo;4+1&rdquo; 视图模型</a>: 逻辑视图、开发视图、过程视图、物理视图 + 场景视图</li>
<li><a href="https://github.com/superhj1987/awesome-tech-collections/blob/master/document/arch.md">系统设计文档模板</a>:系统架构设计</li>
<li><a href="https://github.com/superhj1987/awesome-tech-collections/blob/master/document/tech-research.md">技术调研文档模板</a>:技术方案调研输出</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[响应式微服务架构-分布式系统设计原则]]></title>
    <link href="http://www.rowkey.me/blog/2018/06/07/reactive-microservice/"/>
    <updated>2018-06-07T19:29:34+08:00</updated>
    <id>http://www.rowkey.me/blog/2018/06/07/reactive-microservice</id>
    <content type="html"><![CDATA[<p>O’Reilly的电子书《Reactive Microservices Architecture》讲述了微服务/分布式系统的一些设计原则，本文是笔者阅读完此书后的理解。书籍地址：<a href="https://info.lightbend.com/COLL-20XX-Reactive-Microservices-Architecture-RES-LP.html">https://info.lightbend.com/COLL-20XX-Reactive-Microservices-Architecture-RES-LP.html</a>。</p>

<!--more-->


<p>微服务相比传统的单体应用能够带来快速的响应，以小的系统产生大的影响。而随着网络加速、磁盘成本降低、RAM成本降低、多核技术的发展、云架构技术的爆发，微服务不再受这些客观条件的限制，已经开始大规模的应用。</p>

<p>与SOA架构，微服务和它都具有相同的初衷：解耦、隔离、组合、集成、分散以及自主，但是SOA经常被误解和误用，尤其是使用ESB来支持对多个单体系统的协议（复杂、低效、不稳定）调用，使得系统变得非常复杂。而随着这些年硬件以及软件架构理念的发展，所有的系统基本都已经变成分布式架构，也带来了很多新的挑战。也就需要新的思路和理念来面对这些问题，其中本书所讲述的响应式原则（Reactive principles）即一种解决分布式系统的思路。响应式原则也并非一个新的东西，Erlang中的Actor模型即一种响应式设计。微服务是响应式原则的一个架构设计，其借鉴了SOA架构中好的理念，并使用了现代的基础服务设施（云服务、自动化工具等）。</p>

<h2>响应式微服务定义</h2>

<p>使用微服务架构最关键的一个原则就是将系统划分成一个个相互隔离、无依赖的子系统，这些子系统通过定义良好的协议进行通信。其中隔离是实现弹性、可伸缩系统的前提，并且需要在服务间建立异步通信边界，因此要在以下两方面进行解耦：</p>

<ul>
<li>时间：允许并发。</li>
<li>空间：允许分布式和移动性，即服务能够随时移动。</li>
</ul>


<p>此外，微服务还需要消除共享状态从而最小化相互协作、联结的成本，要尽量达到“不共享任何东西”。</p>

<h3>隔离任何东西</h3>

<p>隔离是微服务架构中最重要的特性。不仅仅是微服务带来的很多优势的基础，也是对设计和架构影响最大的方面。如康威定律所说，它还对组织架构有非常大的影响，</p>

<pre><code>系统的结构是对团队组织架构的反映。
</code></pre>

<p>失败隔离是一种与“舱壁”（船舱的隔板）相关的设计模式：隔离错误、失败以防止其蔓延至所有服务，导致更大面积的失败。</p>

<p>“舱壁”这种模式已经在轮船上使用了几个世纪：创建一个个密封不漏水的空间以防止船的外壳破损或者其他泄漏。这些空间是完全互相隔离的，这样即使一个隔离区充满了水，也不会蔓延流到其他隔离空间中，从而使得船整体仍然能够运作。</p>

<p>弹性（从失败中恢复的能力）即依赖于这种舱壁和失败隔离的设计，并且需要打破同步通信机制。由此，微服务一般是在边界之间使用异步消息传输，从而使得正常的业务逻辑避免对捕获错误、错误处理的依赖。</p>

<p>进一步的，服务之间的隔离使得“持续交付”变得很容易，能够随时地部署服务而无需担心影响正常的业务。而且隔离的单个服务很容易监控、调试、测试和部署，非常便于扩展。</p>

<h3>自主地行动</h3>

<p>上面所讲的隔离是自主性的前提。只有当服务之间是完全隔离的，那么才可能实现完全的自主，包括独立的决策、独立的行动以及与其他服务协调合作来解决问题。</p>

<p>一个自主的服务仅仅保证其对外公布的协议/API的正确性即可。如此，不仅能够让我们更好地了解协作的这些系统以及对他们的建模，也能够在面对冲突、失败状况时，只在一个服务内进行排查、修复即可。</p>

<p>使用自主服务能够给服务编排、工作流管理以及服务合作上带来很大灵活性，同时也带来可扩展性、可用性、运行时管理等优势。但其付出的代价就是需要花心思在定义良好的可组合的API设计上，这个是有一定挑战性的。</p>

<h3>只做一件事，并且做好</h3>

<p>如Unix编程哲学所说：程序应该只做一件事，并且做好它。然后让他们一起工作完成任务。这也类似于面向对象编程语言中软件开发单一职责原则（SRP）的描述。</p>

<p>而在微服务中一个很大的问题就是如何正确地确定服务的大小。比如怎样的粒度才能被认为是“微”（micro）？多少行代码还能被认为是微服务。但这里“micro”其实是和职责范围有关的，就如Unix的SRP原则：只做一件事并且做好。</p>

<p>每一个服务都应该只有一个存在的原因，提供了一组相关的功能，业务和职责不会糅杂在一起。所有服务组织在一起整体上能够便于扩展、具有弹性、易理解和维护。</p>

<h3>拥有自己的私有状态</h3>

<p>微服务中有一个很关键的部分就是状态（state），很多微服务也都是有状态的实体，包括对状态和行为的封装。而在“无状态”的设计理念下，很多服务都把自己的状态下沉到一个大的共享数据库中，这也是很多传统的Web框架的做法。如此就造成了在扩展性、可用性以及数据集成上很难做好把控。而本质上，一个有着共享数据库的微服务架构本质还是一个单体应用。</p>

<p>合理的方式是一个服务既然具有单一职责，那么就应该拥有自己的状态和持久化机制，建模成一个边界上下文，有自己的域名和语言。这些也都是DDD（Domain-Drivern Design）里面的技术。微服务受DDD影响很大，其中很多微服务的上下文的概念都来自于DDD。</p>

<p>当访问一个服务时，也只能是客气的请求其状态而并不能强制其一定具有状态。如此，每个服务都能够通过事件溯源(Event Sourcing)和CQRS（Command Query Responsibility Segreation）自定义自己的状态表示和实现（RDBMS、NoSQL、Time-Series、EventLog）。</p>

<p>去中心化的数据管理和持久化（多语言持久化）能够带来很多优势。数据的存储媒介可以根据服务自己的需要选择，服务包括其数据都可以看做一个单独的单元。同时并不允许一个服务直接去访问另一个服务的数据库，如果要访问只能通过API（通过指定规范、策略和Code Review来保证）。</p>

<p>Event Log是一种消息的存储方式。我们可以以消息进入服务的形式存储（发送到服务的Commnds），即命令溯源(Command Sourcing)。我们也可以忽略命令，让命令先执行对服务产生一些作用，如果触发了状态变更，那么我们捕获此次变动并用事件溯源（Event Sourcing）将此次Event存储到EventLog中。</p>

<p>消息有序存储，能够提供服务所有的交互历史。同时消息也保存了服务的事务，也就能够对这些事务日志进行查询、审计、重放从而用于弹性伸缩、调试以及冗余等。</p>

<p>命令溯源和事件溯源是不同的语义。重放命令意味着会重放其带来的副作用。而重放事件则是执行状态的改变。需要根据具体场景的不同选择使用哪种溯源技术。</p>

<p>使用EventLog可以避免&#8221;对象关系不匹配&#8221;的问题（ORM中经常出现）。而由于其自身天然适合异步消息传输，因此绝大多数情况下，Event Log是微服务中最佳的持久化模型。</p>

<h3>拥抱异步消息传输</h3>

<p>微服务之间的通信的最佳机制就是消息传输。如上文所说，服务之间的异步边界能够在时间和空间两方面进行解耦，能够提升整体系统的性能。</p>

<p>异步非阻塞执行以及IO都是对资源的高效操作，能够最小化访问共享资源时的阻塞消耗（扩展性、低延迟以及高吞吐的最大障碍）。简单的例子：如果要发起对10个服务的访问，其中每一个请求需要耗时100ms，那么如果使用同步模式，则完成所有请求则需要10*100=1000ms。而如果使用异步模式，同时发起10个线程，则一共就需要100ms。</p>

<p>异步消息传输还能够让我们注重网络编程的限制，而不是假装这些限制不存在，尤其是在失败场景下。还能够让我们更关注工作流以及服务间的数据流、协议、交互是怎样进行的。</p>

<p>然而目前微服务的默认通信协议以REST为主，其本质是同步通信机制，比较适用于可控的服务调用或者紧耦合的服务调用上。</p>

<p>此外，使用异步消息传输的另一个需求在于对消息的持续流处理（可能是无界的）。也是我们从“data at rest”到&#8221;data in motion&#8221;的理念的改变。之前的数据是离线被使用的，而现在的数据是被在线处理的。应用对数据变更的响应需要达到实时级别：当变动发生，需要实时进行持续的查询、聚合并反馈给最终的应用。这个理念的形成经历了三个主要阶段：</p>

<ol>
<li><p>&ldquo;data at rest&rdquo;: 将大量数据存储在HDFS类似的数据存储媒介中，然后使用离线批处理技术去处理这些数据，一般会有数个小时的延迟。</p></li>
<li><p>意识到了“data in motion”正变得越来越重要：在数秒内捕获数据、处理数据并反馈给运行中的系统。Lambda即此时出现的一种架构: 加速层用来做实时在线计算；批处理层用来做复杂的离线处理。加速层实时处理的结果后续被批处理层的结果合并。这个模型解决了某些场景需要数据即时响应的问题，但其架构的复杂使得不容易维护。</p></li>
<li><p>“data in motion”: 全面拥抱移动数据的概念。传统的面向批处理的架构都在逐渐向纯流处理的架构转变。这种模型作为通信协议和持久化方案（通过Event Logging）也能够给微服务带来“data in motion”和流处理的能力。</p></li>
</ol>


<h3>保持移动，但可寻址</h3>

<p>如上述所讲，异步消息传输带来了对时间和空间的解耦。其中，对于空间的解耦也被称为“位置透明”：在多核或者多结点上的微服务在运行时无须改变结点即可以动态扩展的能力。这也决定了系统的弹性和移动性。要实现这些需要依赖云计算带来的一些特性和其“按需使用”模型。</p>

<p>而可寻址则是说服务的地址需要是稳定的，从而可以无限地引用此服务，而无论服务目前是否可以被定位到。当服务在运行中、已经停止、被挂起、升级中、已经崩溃等等情形下，地址都应该是可用的，任意客户端能够随时发送消息给一个地址。实际中，这些消息有可能进入队列排队、重提交、代理、日志记录或者进入死信队列。此外，地址需要是虚拟的，可以代表一组实例提供的服务。</p>

<ul>
<li>在无状态的服务间做负载均衡：如果服务是无状态的，那么请求被哪一个服务实例处理都是没任何问题的。也有很多种的路由算法供使用，如：轮训、广播或者基于度量信息。</li>
<li>在有状态的服务之间构建Active-Passive的冗余设计：如果一个服务是有状态的，那么可以使用sticky路由算法（同一个客户端的请求都会发送给同一个服务实例）。冗余一个passive实例是为了在主实例挂的时候接管上面的请求。因此，服务的每一个状态变动都需要同步到passive实例上。</li>
<li>有状态的服务的重定位：将一个服务实例从一个位置移动到另一个位置可以提高引用的本地性（让数据和计算靠近）和资源利用率。</li>
</ul>


<p>使用虚拟地址能够让服务消费方无须关心服务目前是如何配置操作的，只要知道地址即可。</p>

<h2>微服务系统实现</h2>

<p>一个微服务并非真正的“微服务”，一系列微服务通过通信、合作才能够解决问题，才能组成一个完整的业务系统。实现一个服务是相对简单的，困难的是其他基础设施的实现：服务发现、协作、安全、冗余、数据一致性、容错、部署以及与其他系统的集成。</p>

<h3>系统需要利用现实</h3>

<p>微服务架构带来的一个很大优势就在于它提供了一套工具，能够利用现实，模仿真实的世界来创建系统，包括真实世界的限制和机会。</p>

<p>首先根据“康威定律”，微服务的部署是和现实中工程组织/部门如何工作是相适应的。此外，还需要注意的是现实不是一致的，任何事情都是相对的，即使是时间和“现在”这个概念。</p>

<p>信息的传播速度不可能比光快，甚至大部分是很慢的，这也意味着信息通信是有延迟的。信息都是来自过去的，我们稍微思考一下可以知道信息承载的都是我们观察到的东西。而我们观察/学习到的事实至少都是很短时间之前发生的，也就是说我们总是在看过去，“现在”只是旁观者的视角。</p>

<p>每一个微服务都可以看做一个安全的小岛，提供了确定性和强一致性，上面的时间和“目前”都是绝对的。但是当离开一个微服务的边界时，就进入了一片充满非确定性的大海-分布式系统的世界。如很多人所说，构建分布式系统是困难的。但现实世界同时也提供了如何解决诸如弹性、可伸缩、隔离性等分布式问题的解决思路。因此，即使构建分布式系统是困难的，但是我们也不应该退化为单体应用，而是学习如何使用一系列的设计原则、抽象概念和工具来管理它。</p>

<p>正如Pat Helland在《Data on the Outside versus Data on the Inside.”》对&#8221;data on the inside&#8221;和“data on the outside”的对比所说：内部的数据就是我们本地的“目前”，而外部数据-事件即是来自过去的信息，服务之间的命令则是“对未来的希望”。</p>

<h3>服务发现</h3>

<p>服务发现要解决的问题就是如何定位一系列的服务从而可以使用地址去调用。其中最简单的手段就是将地址和端口信息硬编码在所有服务中或者外置在服务的配置文件中。这种方式的问题在于其是一种静态部署模型，与微服务的初衷是相矛盾的。</p>

<p>服务需要保持解耦和移动，而系统需要是弹性和动态的。因此可以通过使用控制反转（Inversion of Control）模式引入一个间接层来解决此问题。也就是说每一个服务都上报自己的信息（位置、如何访问）给一个统一的平台。这个平台被称作“服务发现”，是微服务平台的一个基础部分。这样，一旦服务的信息被存储了，服务就可以使用“服务注册中心”来查找调用服务的信息，这种模式被称作“Client-Side服务发现”。另一种策略是将信息存储、维护在一个负载均衡器（AWS的ELB)或者直接维护在服务提供方的地址中-“Server-Side服务发现”。</p>

<p>可以选择CP特性的数据库作为服务信息的存储，能够保证信息的一致性。但是这种数据库是牺牲了一定程度的可用性来达到强一致性的，并且依赖一些额外的基础设施，而很多时候强一致性并非那么需要。因此，更好的选择是使用AP特性的点对点的技术来存储，比如使用CRDTs（Conflict-Free Replicated Data Types ）与Epidemic Gossip可以实现信息的最终一致性传播，能够有更好的弹性，也不需要额外的基础设施。</p>

<h3>API管理</h3>

<p>API管理解决的问题在于如何将服务的协议和API统一管理起来，以方便服务的调用。包括协议和数据版本的升级和后退等。解决此问题可以通过引入一个负责序列化编码、协议维护以及数据传输的层，甚至直接将服务版本化。这在DDD中被称作&#8221;Anti-Corruption&#8221;层，可以加入到服务本身或者在API网关中实现。</p>

<p>假如一个客户端需要调用10个服务（每一个都有不同的API）来完成一个任务，那么对于这个客户端来说是非常繁琐的。相比起让客户端直接去调用服务，更好的方式是让客户端通过API网关服务来调用。API网关负责接受客户端的请求，然后路由请求到相应的服务（如果有必要需要转换协议），组装响应并将其返回给客户端。这样，做为客户端和服务之间的一层其就能够简化client-to-service协议。但这里如果是中心化的则很难达到高可用和可扩展性，所以使用去中心化技术（比如服务发现）实现API网关则是更好的选择。</p>

<p><img src="//post_images/rx-ms/gw.png" alt="" /></p>

<p>但需要注意的是API网关，包括所有的核心出服务并不是一定要自建的，理想地它应该是底层平台的一部分。</p>

<h3>管理通信模式</h3>

<p>在一个由数个微服务组成的系统中，使用点对点的通信就能完成服务间的通信工作。但是当服务数目越来越多，如果还是让他们之间直接调用，那么很快整个系统会变得混乱不堪。解决此问题需要一个机制能够解耦发送者和接受者，并且能够按照某种定义好的原则路由数据。</p>

<p>发布订阅机制是一种解决方案：发布者发布信息到某个topic中，订阅者监听此topic以监听消息。可以使用可扩展消息系统（Apache Kafka、Amazon Kinesis）或者NoSQL数据库（AP特性数据库，如Cassendra和Riak）来实现。</p>

<p>在SOA架构中，ESB承担的即这种角色。微服务中我们肯定不会使用它来桥接单体应用，但是可以将它做为一个发布系统用来广播任务和数据或者做为系统间的通信总线（通过Spark Streaming收集数据到Spark中）。</p>

<p>发布订阅协议有时候也是有不足的。比如无法提供允许程序员自定义路由规则的高级路由特性或者数据的转化、丰富、分隔以及合并等功能（可以使用Akka Streams或者Apache Camel）。</p>

<h3>集成</h3>

<p>系统与外界或者系统之间的通信都是必需的。当与一个外部系统通信时，尤其当外部系统无法把控时，那么就会有很大的失败风险（系统超载、业务失败）。因此即使协议协商得再好，也不能信赖外部服务，需要做好各种预防措施以保证自身服务的安全。</p>

<p>首先要达成一个良好的协议从而可以最小化一个系统突发超载造成服务不可用的风险，比如要避免发起的请求超过服务提供方的承载能力。也要尽量避免使用同步通信机制，否则就把自身服务的可用性放在了依赖的第三方服务的控制中。</p>

<p>避免级联失败需要服务足够解耦和隔离。使用异步通信机制是一个最佳的方案。此外，还需要通过背压（back-pressure，接收方根据自己的接受状况调节接受速率，通过反向的响应来控制发送方的发送速率）来达成数据流速度的一致性，以防止响应快速的系统压垮其中较慢的部分。而越来越多的工具和库都在开始拥抱“响应式流”（Reactive Streams）规范（Akka Stream、RxJava、Spark Streaming、Cassandra drivers），这些技术使用异步背压实时流来桥接系统，从而在总体上提高系统的可靠性、性能以及互操作性。</p>

<p>如何管理调用服务时候的失败也是微服务中一个关键的问题。捕获到错误后，先重试，而如果错误一直发生，那么就隔离服务一段时间直到服务恢复-“断路器”模式（Netflix和Akka中都有实现）。</p>

<p>面对可扩展性、高吞吐以及可用性的要求，系统集成的实现从传统的依赖于中心化服务如RDBMS/ESB逐渐变为现在采用去中心化策略（HTTP REST、ZeroMQ）或者订阅发布系统（Kakka、Amazon Kinesis）。而最近事件流平台（Event Streaming Platforms）正成为系统集成选型的趋势，其理念来自于Fast Data和实时数据管理。</p>

<p>如上文所述，服务之间使用异步通信机制能够得到很多的好处。但是如果是客户端（浏览器、APP）与服务之间的通信，使用REST经常是更好的选择。但是并非所有的地方都非得使用同步通信机制，需要根据不同的场景做不同的评估。很多情况下，开发者出于习惯都会倾向于使用同步方案，而不是根据真正的需要作出能够简化操作、提升操作性的选择。这里给出几个通常会使用同步方案建模但其本质是异步行为的事例：</p>

<ul>
<li>查询一个商品是否有货，如果此商品比较热门被卖光了，用户要得到通知。</li>
<li>如果一个餐馆的特价菜单改动了，用户要立刻知道。</li>
<li>用户对于一个网站的评论需要实时对话。</li>
<li>广告系统根据用户在页面上的行为输出不同的响应。</li>
</ul>


<p>对于上述实例，我们需要分别进行分析去理解怎样才是符合客户端和服务通信的最自然的方式。同时也经常需要根据数据的完整性约束来寻找可以弱化一致性保证（有序）的可能，目的就是找到最少的协调约束条件给用户以直观的语义：找到利用现实的最佳策略。</p>

<h3>安全管理</h3>

<p>安全管理主要是对服务的认证授权管理，限制某些service只允许某些服务访问。</p>

<ul>
<li>TLS Client Certificates也被称为相互验证、双路验证。它给每一个service都分配一个单独的私钥和证书，从而能够很好地保证服务间的认证访问。不仅仅服务要验证客户端的身份，客户端也要验证服务的身份。因此，其不仅能防止数据被窃听，而且即使在不安全的网络中也能防止对数据的拦截和转发。基于SSL之上的通信不仅安全，其也是一个公开、易于理解的标准。但是其非常复杂，无法得到底层平台的足够支持。同样的，HTTPS Basic Authentication也是双路验证，但其对SSL证书的管理也很复杂，请求也不能被反向代理缓存。</li>
<li>Asymmetric Request Signing：每一个服务都需要使用自己的私钥给自己发送的请求进行签名，同时每一个服务的公钥都要上报给“服务发现”服务。此方案的缺点在于一旦网络不可靠，那么则很难防止数据窃听或者请求重放攻击。</li>
<li>Hash Message Authentication Code (HMAC) ： 基于共享密钥来对请求进行一定规则的签名。这个方案比较简单，但是由于每一对需要通信的服务都需要唯一的一个共享密钥，整个系统则需要所有服务排列数目的共享密钥数量，实现起来比较麻烦。</li>
</ul>


<h3>最小化数据耦合</h3>

<p>微服务架构中，完成一个任务需要多个服务的协同，因此最小化服务之间的状态协作成本，有助于提升微服务系统的整体性能。</p>

<p>需要做的是要从业务的视角去分析数据以理解数据间的关系、担保和完整性约束。然后对数据进行反范式设计并在系统内定义一致性边界。如此，可以在边界内部实现强一致性。接着，需要使用这些边界来驱动微服务的设计和范围。如果设计的服务之间有很多数据依赖和关系，那么就需要去减少甚至是消除这些数据的耦合，从而避免对服务状态的协同。</p>

<h3>最小化协作成本</h3>

<p>如上节所述，已经最小化数据耦合了，但仍然还是会有业务场景需要多个服务协作完成。这个的确是无法避免的，但到了目前这一步，需要做的是可以根据需要逐渐的添加协作，而不是一开始各种耦合再逐渐去消除（比较麻烦和困难）。</p>

<p>这里提供几种可扩展、弹性伸缩的方式来协同数据改变，以达到Composability（对数据的变动无须停止数据所在的服务，也无须等待某些条件）。</p>

<ul>
<li>Apology-Oriented Programming: 基于请求原谅比请求权限容易的想法。如果你不能响应协作，那么就做出一个合理的猜测，赌一个条件已经满足，后续如果错了，那么就道歉并做补偿。这种做法和现实是非常相似的。比如航班的超售，如果起飞的时候没有座位那么就去做一些补偿措施。</li>
<li>事件驱动架构（Event-Driven Architecture ）：基于异步消息传输和事件溯源。需要区分命令和事件，命令表示一个将要产生副作用的操作意图（对未来的希望），而事件表示已经发生的事实。使用CQRS模式进行查询，将写入方（持久化事件日志）与读取方（将数据存入RDBMS或者NoSQL数据库中）分离。这里使用事件日志做状态管理和持久化具有很多好处：简化审计、调试、冗余、容错，并且允许重放过去任意时间点的事件流。</li>
<li>ACID2.0：由Pat Helland创造，定义了一组原则，目的是实现可扩展、弹性的协议以及API设计。A是Associative，表示分组消息不会产生影响，可以批量处理。C是Commutative，表示消息的顺序不重要。I是Idempotent，表示消息重复不会产生影响。D是Distributed，没有实质的意义，猜测是为了凑ACID这个首字母缩写。</li>
</ul>


<p>CRDTs是一个囊括了上面这些东西的工具，可以实现最终一致性、据有丰富的数据结构（counters、sets、maps、graphs），并且不需要协作就可以收敛聚合。其更新操作的顺序前后也并不影响最终的合并结果，能够自动安全的进行合并。虽然CRDTs最近才出现在业界视野中，但其实它已经在生产环境使用了很多年。已经有一些生产级别的库可以直接使用（Akka、Riak）。</p>

<p>然而，很多业务场景并不允许最终一致性。这时可以使用因果一致性（causal consistency）。因果关系很容易被大家理解。而且因果一致性模型能够实现可扩展性和可用性。其一般使用逻辑时间，在很多NoSQL数据库、事件日志以及分布式事件流产品中都可用。</p>

<p>分布式事务是一个经常使用的方式用来协调分布式系统的变动，但其本质需要约束并发执行，保证同一时间只有一个用户在操作。因此其成本非常昂贵，会使得系统变慢、无法扩展。<a href="http://bit.ly/22hhl6F">Saga模式</a>是分布式事务之外的一个能够实现可扩展、弹性伸缩的选择。它的理论基础在于一个长时间运行的业务事务大多时候都是由多个事务步骤组成的，而事务步骤的总体一致性能够通过将这些步骤分组成一个总体的分布式事务来实现。该技术将每一个阶段的事务与一个可补偿回滚的事务配对，如果一个阶段的事务失败了，那么总体的分布式事务就可以回滚（反向顺序）。</p>

<p><img src="//post_images/rx-ms/saga.png" alt="" /></p>

<h3>总结</h3>

<p>当设计一个响应式微服务时，需要坚持隔离、单一职责、自主、独占状态、异步消息传输和移动等特质。微服务需要协作才能形成一个系统去发挥作用。一个能够提供基础服务和响应式原则模式的复杂微服务平台是有必要的。</p>

<p><img src="http://www.rowkey.me/post_images/book-all.png" width="400"/></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[微服务的十个反模式和陷阱]]></title>
    <link href="http://www.rowkey.me/blog/2018/06/02/microservice-pitfall/"/>
    <updated>2018-06-02T19:29:34+08:00</updated>
    <id>http://www.rowkey.me/blog/2018/06/02/microservice-pitfall</id>
    <content type="html"><![CDATA[<p>O’Reilly的电子书《Microservices AntiPatterns and Pitfalls》讲述了在微服务设计实现时十种最常见的反模式和陷阱。本文基于此书，将这十个点列出。书籍地址：<a href="https://www.oreilly.com/programming/free/microservices-antipatterns-and-pitfalls.csp">https://www.oreilly.com/programming/free/microservices-antipatterns-and-pitfalls.csp</a>，更全的反模式和陷阱可见作者的视频：<a href="http://oreil.ly/29GVuDG">http://oreil.ly/29GVuDG</a></p>

<!--more-->


<h2>数据驱动迁移反模式-Data-Driven Migration</h2>

<p><img src="//post_images/ms-anti/data-driven-1.png" alt="" /></p>

<p>如上图所示，此种反模式的问题在于微服务的粒度没有最终确定之前就做了数据迁移，如此当不断的调整服务粒度时，那么数据库就免不了频繁迁移，带来极大的成本。更好的方式如下图所示：</p>

<p><img src="//post_images/ms-anti/data-driven-2.png" alt="" /></p>

<p>即先分离功能，数据库先保持之前的单体，等到服务粒度最终确定之后，再分离数据库。</p>

<h2>超时反模式-The Timeout</h2>

<p>微服务架构是由一系列分离的服务组成的，这些服务之间通过一些远程协议进行互相之间的通信。其中牵扯到了服务的可用性和响应性问题。如下图所示：</p>

<p><img src="//post_images/ms-anti/availability-res.png" alt="" /></p>

<ul>
<li>可用性：服务消费方能够连接服务方，并可以向其发送请求。</li>
<li>响应性：服务方能够在消费方期望时间内给予请求响应。</li>
</ul>


<p>为了防止服务的不可用和无法响应，通常的做法就是设置一个调用超时。此种做法表面上看是没问题的，但是试想一下如下情景：发起一个购买100个商品的请求，请求成功返回一个确认号。如果当请求超时但是请求在服务端已经成功执行了，此时这个交易实际是完成的，但是消费方没有拿到确认号，如果重试请求，那么服务方需要一个复杂的机制判断这是否一次重复提交。</p>

<p>一种解决此问题的方案是设置一个较长的超时时间，如一个服务的通常响应耗时需要2s，最大耗时需要5s，那么超时时间可以设置为10s。但这样的问题就是如果服务不可用，所有消费方都得等待10s，这个是非常损耗性能的。</p>

<p>解决超时反模式的方案就是使用“断路器模式”。就类似于房屋中的电源断路器，当断路器关闭，电流可以通过，当断路器打开，那么电流中断一直到断路器关闭。断路器模式就是说当检测到服务方无法响应时就打开，后续的请求都会被拒绝掉。一旦服务方可响应了，那么断路器关闭，恢复请求。其工作模式如下图所示：</p>

<p><img src="//post_images/ms-anti/circuit-breaker.png" alt="" /></p>

<p>断路器会持续地监测远程服务，确保其是可响应的。只要服务可响应，那么断路器会一直关闭，允许请求通过。如果服务突然不可响应，那么断路器打开，拒绝后续的请求。而后续如果断路器又检测到服务恢复了，那么断路器会自动关闭，请求也就恢复了。此种方案与超时时间相比，最大的优势就是一旦服务不可响应，那么断路器模式可以让请求立刻返回而不是需要等待一定的时间。</p>

<p>Hystrix的Netflix是此种断路器模式的一种开源实现。此外，Akka中也包含了一个断路器实现：Akka CircuitBreaker类。</p>

<p>关于“断路器模式”的详细信息可见：<a href="https://martinfowler.com/bliki/CircuitBreaker.html">https://martinfowler.com/bliki/CircuitBreaker.html</a>。</p>

<h2>共享反模式-I Was Taught to Share</h2>

<p>微服务被普遍认为是一种不共享任何东西的架构。但实际上只能是尽可能地少共享，毕竟在某些层面代码被多个服务共享也能带来一定好处。例如，与单独部署一套安全服务（认证和授权）其他所有服务都通过远程访问此服务相比，把安全相关的功能封装成jar包（security.jar），然后其他服务都集成此jar包，就能够避免每次都要发起对安全服务的访问，从而提高性能和可靠性。但后面的方案带来的问题就是依赖噩梦：每一个服务都依赖多个自定义的jar包。如此不仅打破了服务之间的边界上下文，同时也引入了诸如总体可靠性、变更控制、易测试性、部署等问题。</p>

<p>在一个使用面向对象编程语言的单体应用中，使用abstract类和接口实现代码复用和共享是一个良好的实践。但当从单体切换到微服务架构时，对于很多自定义的共享类和工具类（日期、字符串、计算）的处理要考虑到微服务间共享的东西越少越有利于保持服务间的边界上下文，从而更利于快速测试和部署。以下是几种推荐的方式，也是解决“共享反模式”的方案：</p>

<ol>
<li><p>共享项目</p>

<p> <img src="//post_images/ms-anti/share-project.png" alt="" /></p>

<p> 将共享的代码作为一个项目在编译期与各个服务集成。此种方式便于变更和开发软件，但是最大的问题在于很难发觉哪一个共享模块被修改以及修改的原因，也无法确定自己的服务是否需要这些变更。尤其是在服务发布前期发现某一个共享模块发生了变动的话需要再一次的测试才能走后续流程。</p></li>
<li><p>共享库</p>

<p> <img src="//post_images/ms-anti/share-library.png" alt="" /></p>

<p> 此种方式即将共享的代码作为类库集成到服务中。如此每次共享的库有改动，服务都需要重新打包、测试、重启。但相比起第一种，其有版本标记，能够更好地控制服务的部署和开发，服务开发者可以自己控制何时将共享库的改动集成进来。</p>

<p> 更进一步的，如果采用此种方案，一定要避免把所有共享的代码都打包进一个jar包中如common.jar。否则会很难确定何时要把库的变动集成到服务中。更好的做法是将共享代码分成几个单独上下文的库，如：security.jar、dateutils.jar、persistence.jar等，如此会比较容易的确定何时去集成共享库的变动。</p></li>
<li><p>冗余</p>

<p> <img src="//post_images/ms-anti/replica.png" alt="" /></p>

<p> 此种方案违反DRY原则，在每一服务中都冗余一份共享代码，能够避免依赖共享也能够保持边界上下文。但是一旦共享的代码有变动，那么所有服务都需要改动。因此，此种方案适用于共享模块非常稳定，极小可能变动的情况。</p></li>
<li><p>服务合并</p>

<p> <img src="//post_images/ms-anti/consolidation.png" alt="" /></p>

<p> 当多个服务共享的代码变动比较频繁时可以采用此种方案合并成一个服务，如此就避免了多了服务频繁的测试和部署，也避免了依赖共享库。</p></li>
</ol>


<h2>可达性报告反模式-Reach-in Reporting</h2>

<p>微服务中各个服务以及其相应的数据都是包含在一个单独的边界上下文中的，也就是说数据是隔离到多个数据库中的。因此，这也会使得收集微服务的各种数据生成报告变得相对困难。一般来说有四种方案解决这个问题。其中，前三种都是从各个微服务中拉取数据，是这里所说的反模式，被称作“Reach-in Reporting”。</p>

<ol>
<li><p>数据库拉取模式</p>

<p> <img src="//post_images/ms-anti/database-pull.png" alt="" /></p>

<p> 报告服务直接从各个服务的数据库中拉取数据从而生成各种报告。此种方式简单迅速，但是会让报告服务和业务服务相互依赖，是一种数据库共享集成风格（通过共享的数据库将多个应用耦合在一起）。如此一旦数据库有改动，所有相关服务都要改动，也就打破了微服务中极为重要的边界上下文。</p></li>
<li><p>HTTP拉取模式</p>

<p> <img src="//post_images/ms-anti/http-pull.png" alt="" /></p>

<p> 与数据库拉取模式相比，此种方式不再是直接去访问服务的数据库，而是通过HTTP接口去请求服务的数据。此种方式能够保持服务的边界上下文，但是性能比较慢，而且HTTP请求无法很好的承载大数据。</p></li>
<li><p>批量拉取模式</p>

<p> <img src="//post_images/ms-anti/batch-pull.png" alt="" /></p>

<p> 此种方式会有一个单独的报告数据库/数据仓库来存储各个服务的聚合数据。会通过一个批量任务（离线或者基于增量实时）将服务更新的数据导入到报告数据库/数据仓库中。与数据库拉取模式一样，此种方式这也是一种数据库共享集成风格，会打破服务的边界上下文。</p></li>
<li><p>异步事件推送模式</p>

<p> <img src="//post_images/ms-anti/event.png" alt="" /></p>

<p> 此种方式即解决“Reach-in Reporting&#8221;反模式的方案。每个服务都把自己的发生的事件异步推送到一个数据捕获服务，后续数据捕获服务会将数据解析存储到报告数据库中。此种方式实现起来较复杂，需要在服务和数据捕获服务之间制定一种协议用于异步传输事件数据。但其能够保持服务的边界上下文，同时也能保证数据的时效性。</p></li>
</ol>


<h2>沙粒陷阱-Grains of Sand</h2>

<p>微服务实现中最有挑战的问题在于如何拆分service，如何控制服务的粒度，而正确的服务粒度则决定了微服务是否能够成功实现。服务粒度也能够影响到性能、健壮性、可靠性、易测试性、部署等。</p>

<p>“沙粒陷阱”即把服务拆分的太细。其中的一个原因就是很多时候开发者会把一个class与一个服务等同。合理的，应该是一个服务组件(Service component)对应一个服务。一个服务组件具有清晰、简洁的角色、职责，具有一组定义好的操作。其一般通过多个模块(Java Class)实现。如果组件和模块是一对一的关系，那么不仅仅会造成服务粒度过细同时也是一种不好的编程实践：服务的实现都是通过一个Class，那么此Class会非常大并且承担太多的责任，不利于测试和维护。</p>

<p>更进一步的，服务的粒度并不应该受其中实现类的数目影响：有些服务可能只需要一个类就可以实现，而有些服务会需要多个类来实现。</p>

<p>为了避免“沙粒陷阱”，可以通过以下三种测试来判断服务粒度是否合理：</p>

<ol>
<li><p>分析服务范围和功能</p>

<p> 要明确服务用来干什么？有哪些操作？一般通过使用文档或者语言来描述服务的范围和功能就能够看出来服务是否做的工作太多。如果在描述中使用了“和”（“and”）或者“此外”（“in addition”）之类的词，很有可能就是此服务职责太多。</p>

<p> 服务的高内聚是一种良好的实践，其明确一个服务提供的操作之间必须要是有关联的。如对于一个顾客服务，有以下操作：</p>

<ul>
<li>添加顾客</li>
<li>更新顾客信息</li>
<li>获取顾客信息</li>
<li>通知顾客</li>
<li>记录顾客评论</li>
<li>获取顾客评论</li>
</ul>


<p> 其中的前三个操作都是对顾客的CRUD操作，是相关联的。而后三者则无关。为了实现服务的高内聚，合理的应该是把此服务拆分成三个服务：顾客维护、顾客通知、顾客评论。</p>

<p> 如此，以粗粒度的服务开始，然后逐渐拆分成细粒度的服务有利于对微服务的拆分。</p></li>
<li><p>分析数据库事务</p>

<p> 传统的关系型数据库都提供了ACID事务特性用于把多个更新操作打包成一个整体提交，要么都成功，要么都失败。而在微服务中，由于服务都是一个个分离的应用，很难实现ACID，一般实现BASE事务（basic availability、soft state、eventual consistence）即可。但是无法避免的，仍然会有一些场景是需要ACID的。因此，当你不断的需要在BASE和ACID事务做判断和取舍的时候，很有可能就是服务粒度过细。</p>

<p> 如果业务场景无法接受最终一致性，那么最好就是将服务粒度粗化一些，把多个更新操作放到一个服务中。</p></li>
<li><p>分析服务编排</p>

<p> 这里主要说的是服务之间的互相通信。由于对服务的调用都是一次远程调用，因此服务编排会非常大的影响微应用总体的性能。此外，它也会影响系统整体的健壮性和可靠性，越多的远程调用，那么越高的几率会有失败或者超时的请求出现。</p>

<p> 如果发现完成一次业务逻辑需要调用太多的远程服务，就说明服务的粒度可能太细了。这时候就需要将服务粗化。而合并细粒度服务还能够提高性能，提升总体的健壮性和可靠性。同时也减少了多个服务间的依赖，更利于测试和部署。</p>

<p> 此外，使用响应式编程技术异步并行调用远程服务也是一种提升性能和可靠性的方案。</p></li>
</ol>


<h2>无因的开发者陷阱-Developer Without a Cause</h2>

<p>此陷阱主要讲的是开发者或者架构师在做设计时很多时候是拍脑袋在做，没有任何合理的原因或者原因是错误的，也不会做取舍。而想要解决此问题，不仅仅是架构师，开发者也需要同时了解技术带来的好处以及缺陷，从中做权衡。</p>

<p>了解业务驱动是避免此陷阱的关键一步。每一个开发者和架构师都应该清楚的了解下面这些问题的答案：</p>

<ul>
<li>为什么要使用微服务？</li>
<li>最重要的业务驱动是什么？</li>
<li>架构中的哪一点是最为重要的？</li>
</ul>


<p>假如易部署性、性能、健壮性、可扩展性是系统最看重的特性，那么对于不同的业务侧重点，微服务的粒度需求也是不同的。细粒度的服务能够达到更好的易测试性和易部署性，而粗粒度的服务则有更好的性能、健壮性以及可靠性。</p>

<h2>追随流行陷阱-Jump on the Bandwagon</h2>

<p>微服务是目前非常流行的架构理念，越来越多的公司也都在紧跟这个潮流纷纷转型微服务架构，而不管到底自己是否真的需要。为了避免此陷阱，需要首先了解微服务的优点和缺点。</p>

<p>优点：</p>

<ul>
<li>易部署：容易部署是微服务的一个很大的优点。毕竟相比起一个庞大的单体应用，一个小并且职责单一的微服务的部署非常简单并且带来的风险也会小很多。而持续部署技术则进一步放大了这个优点。</li>
<li>易测试：职责单一、共享依赖少使得测试一个微服务是很容易的。而基于微服务做回归测试与单体大应用相比也是很容易的。</li>
<li>控制变更：每个服务的范围和边界上下文使得很容易控制服务的功能变动。</li>
<li>模块化：微服务就是一个高度模块化的架构风格。这种风格也是一种敏捷方式的表达，能够很快的响应变化。一个系统模块化程度越高，就越容易测试、部署和发布变更。一个服务粒度划分合理的微服务系统是所有架构中模块化程度最高的架构形式。</li>
<li>可扩展性：由于每一个服务都是一个职责单一的细粒度服务，因此此种架构风格是所有架构分隔中可扩展性最高的。其非常容易扩展某一个或者某几个功能从而满足整体系统的需求。而得益于服务的容器化特性以及各种运维监控工具，服务也能够自动化进行启动和关闭。</li>
</ul>


<p>缺点：</p>

<ul>
<li>组织变动：微服务需要组织在很多层面进行变动。研发团队需要包含UI、后端开发、规则处理、数据库处理建模等多种职位，从而使得一个小的团队能够具有实现微服务的所有技术栈。同时，传统的单体、分层应用架构的软件发布流程也需要更新为自动化、高效的部署流水线。</li>
<li>性能：由于服务都是隔离的，因此发起对服务的远程调用肯定是会影响性能的。服务编排、运行环境都是影响性能的很大因素。了解远程调用的延迟、需要与多少服务通信都是与性能相关的需要掌握的信息。</li>
<li>可靠性：和性能一样。服务的远程调用越多，那么失败的几率就越高，总体的可靠性就会越低。</li>
<li>DevOps：随着微服务架构而来的是成千上百的服务。手动管理这么多的服务是很不现实的。这就对于自动化运维部署、协作提出了很高的挑战。需要依赖非常多的操作工具和实践，是一个非常复杂的工作。目前差不多有12种类型的操作工具（监控工具、服务注册、发现工具、部署工具等）和框架在微服务架构中被使用，其中每一种又包含了很多具体的工具和产品供选择。对于这些工具和框架的选择一般都会需要将近数月的研究、测试、权衡分析才能做出最适合的技术选型。</li>
</ul>


<p>了解了微服务的优缺点后，下一步则需要根据实际的业务来分析微服务是不是解决这些问题的最佳方案。可以采取以下问题：</p>

<ul>
<li>业务和技术的目标是什么？</li>
<li>使用微服务是为了完成什么？</li>
<li>目前和可预知的痛点是什么？</li>
<li>应用的最关键的技术特性是什么？（性能、易部署性、易测试性、可扩展性）</li>
</ul>


<p>回答这些问题再结合微服务的优缺点能够让你明确现在是否是使用微服务的适当时机。</p>

<p>除了微服务以外，还有其他7种比较普遍使用的架构供选择：</p>

<ul>
<li>基于服务的架构（Service-Based）</li>
<li>面向服务的架构（Service-Oriented）</li>
<li>分层架构（Layered）</li>
<li>微内核架构（Microkernel）</li>
<li>基于空间的架构（Space-Based）</li>
<li>事件驱动架构（Event-Driven）</li>
<li>流水线架构（Pipeline）</li>
</ul>


<h2>静态合约陷阱-The Static Contract</h2>

<p>微服务的消费方和服务提供方之间会有一个合约/协议用来规定输入输出数据的格式、操作名称等等。一般情况下这个合约是不变的。但是如果没有使用版本号来管理服务接口，那么就会进入“静态合约”陷阱。</p>

<p>给合约打上版本标记不仅仅能够避免巨大的变动（服务提供方修改合约使得所有消费方也都得修改），还能够提供向后兼容性。这里有两种技术可以实现合约的版本号：</p>

<ul>
<li><p>在头部信息附加版本号</p>

<p>  <img src="//post_images/ms-anti/header-version.png" alt="" /></p>

<p>  如图，此种方式即在远程访问协议的头部添加版本信息。而如果远程协议使用的是REST，那么还可以使用vendor mime type（vnd）来指定合约的版本号。如下：</p>

<pre><code class="``">  POST /trade/buy
  Accept: application/vnd.svc.trade.v2+json
</code></pre>

<p>  服务接受到请求，能够通过正则等手段简单解析出其中的合约版本号再根据版本号做相应的处理。</p>

<p>  如果使用消息队列，那么可以将版本号放置在属性部分(Property section)。JMS的一个例子如下：</p>

<pre><code class="``">  String msg = createJSON("acct","12345","sedol","2046251","shares","1000");
  jsmContext.createProducer()
      .setProperty("version",2)
      .send(queue,msg);
</code></pre></li>
<li><p>在合约本身中附加版本号</p>

<p>  <img src="//post_images/ms-anti/schema-version.png" alt="" /></p>

<p>  此种方式版本号独立于远程访问协议，与头部信息版本号相比，这也是其最大的优点。但与此同时，其缺点比较多。首先要从请求信息主体中解析版本号，会出现很多解析的问题。其次，合约的模式可能会非常复杂，使得很难做数据转换。最后，服务还要引入对模式的验证逻辑。</p></li>
</ul>


<h2>我们到了吗陷阱-Are We There Yet</h2>

<p>微服务架构中，各个服务都是独立的个体，也就意味着所有客户端或者API层和服务之间的通信都是一次远程调用。如果对这些远程调用的耗时没有什么概念，那么就陷入了“Are We There Yet”陷阱。合理的做法需要去测试远程访问的平均延迟、长尾延迟（95%、99%、99.%之外的请求延迟）等指标。而很多时候即使有很好的平均延迟，但是较差的长尾延迟会造成非常大的破坏。</p>

<p>在生产环境或者准生产环境测试有助于去了解应用的真实性能。例如，一个业务请求需要调用四个服务，假设一个服务调用的延迟是100毫秒，那么加上业务请求本身的延迟，完成此次业务请求共需要500毫秒的延迟。这和单单从代码上去看得出的结论是不一样的。</p>

<p>了解目前所用协议的平均延迟是一方面，另一方面则需要对比其他远程协议的延迟，从而在合适的地方使用合适的协议。如：JMS、AMQP、MSMQ。</p>

<p><img src="//post_images/ms-anti/comparing-protocol.png" alt="" /></p>

<p>如图，AMQP协议的性能是最好的。那么结合业务场景，就可以选择REST作为客户端与服务间的通信协议，AMQP做为服务之间的通信协议以提高应用的性能。</p>

<p>当然，性能并非在选择远程协议时唯一考虑的因素。下一节中就会考虑利用消息队列的一些额外功能。</p>

<h2>REST使用陷阱-Give It a Rest</h2>

<p>REST现在是微服务中用的最多的通信协议。流行的开发框架如DropWizard、Spring Boot都提供了REST支持。但是如果只选择REST这一种协议，不去考虑其他诸如消息队列的优势，那么就陷入了“REST使用”陷阱。毕竟异步通信、广播、合并请求事务这些需求，REST是很难实现的。</p>

<p>消息队列标准目前包括平台特定和平台无关两种。前者包括Java平台中的JMS和C#平台的MSMQ，后者则是AMQP。对于平台特定的消息标准JMS，其规范了API，因此切换broker实现（ActiveMQ、HornetQ）时无需修改API，但由于底层通信协议是不同的，集成的客户端或者服务端jar包需要随着修改。对于平台无关的消息标准，其规范了协议实现标准，并没有规范API。使得不同平台之间都可以互相通信，而不管实际产品是什么。如一个使用了RabbitMQ的客户端可以很容易地与一个StormMQ通信（假设使用的协议相同）。也就是其独立于平台的特性使得RabbitMQ成为微服务架构中最流行的消息队列。</p>

<ol>
<li><p>异步请求</p>

<p> 异步通信是消息队列适用的场景之一。服务消费者发起请求后无需等待服务方响应能够提高总体的性能，同时调用方无需担心调用超时，也就无需使用断路器，从而提高了系统的可靠性。</p></li>
<li><p>广播</p>

<p> 将消息广播给多个service是消息队列的又一个适用场景。一个消息生产者向多个消息接受者发送消息，无需知道谁在接受消息以及如何处理它。</p></li>
<li><p>事务请求</p>

<p> 消息系统提供了对事务消息的支持：如果多个消息被发送到了在一个交易上下文的多个队列或者主题中时，那么直到消息发送者commit，服务才会真正的接受到相应的所有消息（在commit之前会一直保存在队列中）。</p>

<p> 因此对于服务消费者需要合并多个远程请求到一个事务中的场景可以选择事务消息。</p></li>
</ol>


<p><img src="http://www.rowkey.me/post_images/book-all.png" width="400"/></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[架构简明指南]]></title>
    <link href="http://www.rowkey.me/blog/2018/04/25/arch-usage/"/>
    <updated>2018-04-25T19:29:34+08:00</updated>
    <id>http://www.rowkey.me/blog/2018/04/25/arch-usage</id>
    <content type="html"><![CDATA[<p>之前的<a href="http://www.rowkey.me/blog/2017/08/24/arch/">《谈谈架构》</a>讲述了架构的概念、原则等等，这里择出其中的设计原则部分供大家随手参考。</p>

<p>《Clean Architecture》一书中对于软件架构目的的解释：</p>

<blockquote><p>The goal of software architecture is to miminize the human resources required to build and maintain the required system.</p></blockquote>

<p>即：软件架构的目的就是将构建和维护系统需要的人力成本降到最低。</p>

<p>因此，可以得出架构设计的关键思维就是判断和取舍（程序设计的关键思维是逻辑和实现），即如何选择技术、组合技术使得需要的人力资源最少。</p>

<p>需要注意的一点是，脱离业务谈架构是不合理的，技术架构及其演进都是业务目标驱动的。</p>

<!--more-->


<h2>架构原则</h2>

<p><img src="//images/blog_images/arch-spec.png" alt="" /></p>

<ul>
<li><strong>避免过度设计</strong>：简单的架构就是最好的架构。最简单的方案最容易实现和维护，也可以避免浪费资源。但方案中需要包括扩展。</li>
<li><strong>冗余设计</strong>：对服务、数据库的做结点冗余，保证服务的高可用。通过数据库主从模式、应用集群来实现。</li>
<li><strong>多活数据中心</strong>：为了容灾，从根本上保障应用的高可用性。需要构建多活的数据中心，以防止一个数据中心由于不可控因素出现故障后，引起整个系统的不可用。</li>
<li><strong>无状态设计</strong>：API、接口等的设计不能有前后依赖关系，一个资源不受其他资源改动的影响。无状态的系统才能更好地进行扩展。如果非得有状态，则要么客户端管理状态，要么服务端用分布式缓存管理状态。</li>
<li><strong>可回滚</strong>：对于任何业务尤其是关键业务，都具有恢复机制。可以使用基于日志的WAL、基于事件的Event sourcing等来实现可回滚。</li>
<li><strong>可禁用/自我保护</strong>：具有限流机制，当上游的流量超过自身的负载能力时，能够拒绝溢出的请求。可以通过手动开关或者自动开关（监测异常流量行为），在应用前端挡住流量。限流算法包括：令牌桶（支持突发流量）、漏桶（匀速流量）、计数器以及信号量（限制并发访问的数量）。此外永远不要信赖第三方服务的可靠性，依赖于第三方的功能务必有服务降级措施以及熔断管理，如：对于每一个网络操作，都需要设置超时时间，超过这个时间就放弃或者返回兜底响应。</li>
<li><strong>问题可追踪</strong>：当系统出现问题时，能够定位请求的轨迹、每一步的请求信息等。分布式链路追踪系统即解决的此方面的问题。</li>
<li><strong>可监控</strong>：可监控是保障系统能够稳定运行的关键。包括对业务逻辑的监控、应用进程的监控以及应用依赖的CPU、硬盘等系统资源的监控。每一个系统都需要做好这几个层面的监控。</li>
<li><strong>故障隔离</strong>：将系统依赖的资源(线程、CPU)和服务隔离开来能够使得某个服务的故障不会影响其他服务的调用。通过线程池或者分散部署结点可以对故障进行隔离。此外，为不同的用户提供单独的访问通道，不仅仅能够做故障隔离，也有利于做用户权限控制。</li>
<li><strong>成熟可控的技术选型</strong>：使用市面上主流、成熟、文档、支持资源多的技术，选择合适的而非最火的技术实现系统。如果面对自研和开源技术的选择，需要考虑契合度：如果功能需求契合度很高，那么选择开源即可；如果开源技术是需求的子集或者超集，那么要衡量吃透这个开源技术的成本和自研的成本那个高。</li>
<li><strong>梯级存储</strong>：内存->SSD硬盘->传统硬盘->磁带，可以根据数据的重要性和生命周期对数据进行分级存储。</li>
<li><strong>缓存设计</strong>：隔离请求与后端逻辑、存储，是就近原则的一种机制。包括客户端缓存（预先下发资源）、Nginx缓存、本地缓存以及分布式缓存。</li>
<li><strong>异步设计</strong>：对于调用方不关注结果或者允许结果延时返回的接口，采用队列进行异步响应能够很大程度提高系统性能；调用其他服务的时候不去等待服务方返回结果直接返回，同样能够提升系统响应性能。异步队列也是解决分布式事务的常用手段。</li>
<li><strong>前瞻性设计</strong>：根据行业经验和对业务量的预判，提前把可扩展性、后向兼容性、容量预警设计好。以防止超过系统容量后造成各种问题影响服务。</li>
<li><strong>水平扩展</strong>：相比起垂直扩展，能够通过堆机器解决问题是最优先考虑的问题，系统的负载能力也才能接近无限扩展。此外，基于<strong>云计算</strong>技术根据系统的负载自动调整容量能够在节省成本的同时保证服务的可用性。</li>
<li><strong>小步构建和发布</strong>：快速迭代项目，快速试错。不能有跨度时间过长的项目规划。</li>
<li><strong>自动化</strong>：打包、测试的自动化称为持续集成，部署的自动化称为持续部署。自动化机制是快速迭代和试错的基础保证。</li>
</ul>


<h2>架构六步思考法</h2>

<blockquote><p>笔者对美团总架构师夏华夏一次分享提出的架构六步思考法的理解。</p></blockquote>

<p><img src="http://www.rowkey.me/post_images/arch-six-think.png" width="450"/></p>

<p>这里尤其需要注意的一点是在面对问题时，首先要试图将未知问题转化为已知问题，而不是创造新问题。</p>

<h2>数据设计原则</h2>

<ul>
<li>注意存储效率

<ul>
<li>减少事务</li>
<li>减少联表查询</li>
<li>适当使用索引</li>
<li>考虑使用缓存</li>
</ul>
</li>
<li>避免依赖于数据库的运算功能(函数、存储器、触发器等)，将负载放在更容易扩展的业务应用端</li>
<li>数据统计场景中，实时性要求较高的数据统计可以用Redis；非实时数据则可以使用单独表，通过队列异步运算或者定时计算更新数据。此外，对于一致性要求较高的统计数据，需要依靠事务或者定时校对机制保证准确性。</li>
<li>索引区分度法则：辨识度超过20%的属性，如果有查询需求，就应该建立索引。</li>
<li>对于数值型数据，可以使用保序压缩方式在保证顺序不变的前提下减少字符串长度。如：进行36进制转化即一种保序压缩方式。</li>
<li>大量数据的去重计数如果允许误差可以选择基数估计算法（Hyperhyperlog、Loglogcount）或者布隆过滤器。</li>
</ul>


<h2>系统响应性能提升五板斧</h2>

<ul>
<li><strong>异步</strong>：队列缓冲、异步请求。</li>
<li><strong>并发</strong>：利用多CPU多线程执行业务逻辑。</li>
<li><strong>就近原则</strong>：缓存、梯度存储。</li>
<li><strong>减少IO</strong>：合并细粒度接口为粗粒度接口、频繁的覆盖操作可以只做最后一次操作。这里一个需要特别注意的地方: <strong>代码中尽量避免在循环中调用外部服务，更好的做法是使用粗粒度批量接口在循环外面只进行一次请求。</strong></li>
<li><strong>分区</strong>：频繁访问的数据集规模保持在合理的范围。</li>
</ul>


<h2>系统容量规划</h2>

<p>需要对系统/关键模块做好评估、量化，以防止超出容量时不至于压垮服务器，仍然能够服务于大部分用户。</p>

<p><img src="//post_images/capacity-plan.png" alt="" /></p>

<ol>
<li>根据流量模型、历史数据、预测算法预估未来某一个时间点的业务量：QPS、每日数据量等。</li>
<li>评估单点最大承载量（数据库的单点承载数据量、应用服务器的单点承载并发量）【通过性能测试】，根据业务量计算需要部署的结点数目，做1.5倍部署（DID原则）。</li>
<li>性能压测验证整个系统的负载能力。</li>
<li>设计达到容量预估值时的预警、限流、快速恢复措施以及后续扩展方案。</li>
</ol>


<p>PS: 在容量预估中，机器数目的计算遵循DID原则：20倍设计、3倍实施/实现、1.5倍部署。即需要部署1.5倍的可承载预估业务流量的机器数目。</p>

<h2>架构重构的原则</h2>

<p>一个系统的架构是随着业务而不断演化的，因此不可避免地会留下很多技术债。如果一味地不去管，那么总有一天技术债会爆发出来造成意想不到的破坏。因此很多时候对架构的重构是必须的。其需要遵循的原则如下：</p>

<ul>
<li>确定重构的目的和必要性：为了业务需要；有无其他备选方案</li>
<li>定义“重构完成”的界限</li>
<li>渐进式重构</li>
<li>确定当前的架构状态</li>
<li>不要忽略数据</li>
<li>管理好技术债务</li>
<li>远离那些虚荣的东西</li>
<li>做好面对压力的准备</li>
<li>了解业务</li>
<li>做好面对非技术因素的准备</li>
<li>能够掌握代码质量</li>
</ul>


<h2>其他</h2>

<ul>
<li>系统扩展思路

<ul>
<li>通过克隆扩展->高可用</li>
<li>通过拆分不同的东西来扩展->垂直扩展</li>
<li>拆分类似的东西来扩展->水平扩展</li>
</ul>
</li>
<li>讨论技术方案时，以是否合理为依据，而不要以工作量少为依据。</li>
</ul>


<p><img src="http://www.rowkey.me/post_images/book-all.png" width="400"/></p>
]]></content>
  </entry>
  
</feed>
