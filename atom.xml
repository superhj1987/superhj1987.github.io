<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[后端技术杂谈 | 飒然Hang]]></title>
  <link href="http://www.rowkey.me/atom.xml" rel="self"/>
  <link href="http://www.rowkey.me/"/>
  <updated>2016-11-17T19:25:27+08:00</updated>
  <id>http://www.rowkey.me/</id>
  <author>
    <name><![CDATA[飒然Hang]]></name>
    <email><![CDATA[superhj1987@126.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[JDK自带工具之排查问题示例]]></title>
    <link href="http://www.rowkey.me/blog/2016/11/16/java-trouble-shooting/"/>
    <updated>2016-11-16T22:21:34+08:00</updated>
    <id>http://www.rowkey.me/blog/2016/11/16/java-trouble-shooting</id>
    <content type="html"><![CDATA[<h2>目录</h2>

<ul>
<li><a href="#%E5%BC%95%E8%A8%80">引言</a></li>
<li><a href="#%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E5%9C%BA%E6%99%AF">问题排查场景</a>

<ul>
<li><a href="#%E8%8E%B7%E5%8F%96%E6%AD%A3%E5%9C%A8%E8%BF%90%E8%A1%8C%E7%9A%84JVM%E5%88%97%E8%A1%A8">获取正在运行的JVM列表</a></li>
<li><a href="#Java%E5%A0%86%E7%9A%84DUMP">Java堆的DUMP</a></li>
<li><a href="#%E5%88%86%E6%9E%90%E7%B1%BB%E6%9F%B1%E7%8A%B6%E5%9B%BE">分析类柱状图</a></li>
<li><a href="#%E7%BA%BF%E7%A8%8BDump">线程Dump</a></li>
<li><a href="#%E8%BF%90%E8%A1%8CJava%E9%A3%9E%E8%A1%8C%E8%AE%B0%E5%BD%95%E5%99%A8(Java%20Flight%20Recorder">运行Java飞行记录器(Java Flight Recorder)</a>)</li>
<li><a href="#%E5%90%8E%E8%AE%B0">后记</a></li>
</ul>
</li>
</ul>


<p>最近看到了大量关于java性能调优、故障排查的文章，自己也写了一篇<a href="http://www.rowkey.me/blog/2016/11/02/java-profile/">Java调优经验谈</a>。接着此篇文章，其实一直打算写写一些常用调优的工具以及惯常用法的。后来在<a href="http://java-performance.info">http://java-performance.info</a>这个站点上看到了类似的一篇博文，自我感觉很有指导意义。于是决定翻译+重组织一下此篇文章：<a href="http://java-performance.info/java-server-application-troubleshooting-using-jdk-tools/">Java server application troubleshooting using JDK tools</a>。</p>

<h2><a name='引言'></a>引言</h2>

<p>在Java世界中，我们的很多开发工作从编码、调试到调优都是在使用GUI工具进行。我们经常尝试在本地构建一套和生产环境一样的环境从而使得问题能够重现，进而使用我们常用的工具来排查定位故障。但不幸的是，很多情况下我们是无法在本地重现线上问题的。例如，我们是没有权限获取线上真实客户端提交到服务端的数据的。</p>

<p>由于上文提到的问题，很多时候都是需要远程来排查线上服务器上发生的问题。但是如果单单只有一个JRE的话，你也是无法有合适的方案来进行排查的。你需要JDK或者第三方的工具。有时候使用JDK提供的工具就是最可取的方案，毕竟，在线上环境使用第三方工具有时候会牵扯到权限的问题。</p>

<p>一般情况下，在线上环境安装JDK发布版本可以让排查进行的更高效。建议安装使用最新的Java7/8 JDK或者构建与线上JRE匹配的一些工具。(原文作者貌似不建议安装jdk的发布版本，而是建议逐渐地根据需求安装这些)</p>

<!--more-->


<h2><a name='问题排查场景'></a>问题排查场景</h2>

<h3><a name='获取正在运行的JVM列表'></a>获取正在运行的JVM列表</h3>

<p>为了开始排查工作，我们首先需要获取正在运行的jvm进程列表，包括进程id、命令行参数等。有时候仅仅这一步就可以定位到问题，例如，同样的app被多启动一次在并发做同样的事情(破坏输出文件、重新打开sockets后者其他愚蠢的事情)。</p>

<p>使用<strong>jcmd</strong>不加任何参数即可获取jvm进程列表</p>

<pre><code>25691 org.apache.catalina.startup.Bootstrap start
20730 org.apache.catalina.startup.Bootstrap start
26828 sun.tools.jcmd.JCmd
3883 org.apache.catalina.startup.Bootstrap start
</code></pre>

<p>使用<strong>jcmd <PID> help</strong>能够获取某个jvm进程其他可用的诊断命令。例如：</p>

<pre><code>[root@test-172-16-0-34-ip ~]# jcmd 3883 help
3883:
The following commands are available:
VM.commercial_features
ManagementAgent.stop
ManagementAgent.start_local
ManagementAgent.start
Thread.print
GC.class_histogram
GC.heap_dump
GC.run_finalization
GC.run
VM.uptime
VM.flags
VM.system_properties
VM.command_line
VM.version
help
</code></pre>

<p>输入<strong>jcmd <PID> <COMMAND_NAME></strong>可以运行一个诊断命令或者获取到参数错误信息。</p>

<pre><code>[root@test-172-16-0-34-ip ~]# jcmd 3883 GC.heap_dump
3883:
java.lang.IllegalArgumentException: Missing argument for diagnostic command 
</code></pre>

<p>通过<strong>jcmd <PID> help <COMMAND_NAME></strong>你能够获取此诊断命令更多的信息。如下是<strong>GC.heap_dump</strong>命令的help。</p>

<pre><code>[root@test-172-16-0-34-ip ~]# jcmd 3883 help GC.heap_dump
3883:
GC.heap_dump
Generate a HPROF format dump of the Java heap.

Impact: High: Depends on Java heap size and content. Request a full GC unless the '-all' option is specified.

Syntax : GC.heap_dump [options] &lt;filename&gt;

Arguments:
    filename :  Name of the dump file (STRING, no default value)

Options: (options must be specified using the &lt;key&gt; or &lt;key&gt;=&lt;value&gt; syntax)
    -all : [optional] Dump all objects, including unreachable objects (BOOLEAN, false)  
</code></pre>

<h3><a name='Java堆的DUMP'></a>Java堆的DUMP</h3>

<p>jcmd提供了输出HPROF格式的堆dump接口。运行<strong>jmcd <PID> GC.heap_dump <FILENAME></strong>即可。注意这里的FILENaME是相对于运行中的jvm目录在说的，因此推荐使用绝对路径。此外，也建议使用.hprof作为输出文件的扩展名。</p>

<p>在堆dump完成之后，你可以复制此文件到本地用VisualVM或者用jmc的JOverflow插件打开，进而通过分析堆的状况定位内存问题。</p>

<p>需要注意的两点：</p>

<ul>
<li>还有很多可以打开分析hprof文件的工具：NetBeans, Elipse的MAT，jhat等等。用你最熟悉的即可。</li>
<li>同样可以使用<strong>jmap -dump:live,file=<FILE_NAME> <PID></strong>来产生堆dump文件，但是官方文档标注了此工具为unsupported的。虽然我们绝大多数人都会认为JDK中unsupported的特性会永远存在，但是事实并非这样：<a href="http://openjdk.java.net/jeps/240">JEP 240</a>, <a href="http://openjdk.java.net/jeps/241">JEP 241</a>。</li>
</ul>


<h3><a name='分析类柱状图'></a>分析类柱状图</h3>

<p>如果正在排查内存泄漏问题，你可能想要知道堆中某种类型的存活对象数目。例如，某一时刻某些类应该只有一个实例(单例模式)，但是此类的另外一个或者多个实例却已经到了老年代，但是它们不应该能被GC roots访问到。</p>

<p>运行以下命令可以打印出类柱状图(同时也打印出存活对象的数目)：</p>

<pre><code>jcmd &lt;PID&gt; GC.class_histogram
jmap -histo:live &lt;PID&gt;
</code></pre>

<p>输入如下：</p>

<pre><code>    num     #instances         #bytes  class name
----------------------------------------------
   1:         37083       48318152  [B
   2:        235781       22496784  [C
   3:        103958       16069448  &lt;constMethodKlass&gt;
   4:        482361       15435552  java.util.HashMap$Entry
   5:        103958       14152480  &lt;methodKlass&gt;
   6:          9576       11192168  &lt;constantPoolKlass&gt;
   7:        186264       10430784  com.mysql.jdbc.ConnectionPropertiesImpl$BooleanConnectionProperty
   8:        274109        8771488  java.util.Hashtable$Entry
   9:          9576        7210152  &lt;instanceKlassKlass&gt;
  10:          7972        6404256  &lt;constantPoolCacheKlass&gt;
  11:        229637        5511288  java.lang.String
  12:         48471        5428752  java.net.SocksSocketImpl
  13:         21599        3859672  [Ljava.util.HashMap$Entry;
</code></pre>

<p>这里的以byte为单位的占用大小是浅的(shallow size)，并没有包括子对象的大小。其实很容易注意到char[]和String的统计数据：这俩的实例数目是差不多的，但是char[]的占用大小要大很多，这在于String并未包含下面的char[]的大小。</p>

<p>有了类柱状图信息，你就可以grep/search类的名字从而获取存活实例的数目。如果你发现比期望的数目要大很多，你就可以做heap dump，然后用任意的heap分析工具来分析问题。</p>

<h3><a name='线程Dump'></a>线程Dump</h3>

<p>很多时候，应用会呈现出“卡在那里”的情形。这里有很多种卡住的状况：死锁、cpu密集运算。为了定位到问题所在需要知道线程在做什么、持有了什么锁等等。</p>

<p>Java中有两种锁：sychronized和Object.wait/notifyAll方法的原始锁以及java5引入的java.util.concurrent锁。这俩种锁的不同之处主要在于前者是限制在你进入synchronies块的地方的栈帧(stack frame)中，并且会一直在线程dump中存在。后者却并不限制在栈帧中-你可以在一个方法中进入锁，在另一方法中解锁。因此，thread dump有时候并没有包含这些信息。尽管如此，还是应该使用thread dump来查看线程信息排查问题。</p>

<p>这里有三种方法可以打印应用的thread dump。</p>

<pre><code>kill -3 &lt;PID&gt; #仅限Linux平台
jstack &lt;PID&gt;
jcmd &lt;PID&gt; Thread.print
</code></pre>

<h3><a name='运行Java飞行记录器(Java Flight Recorder)'></a>运行Java飞行记录器(Java Flight Recorder)</h3>

<p>上面讲到的工具都是作为快速的查看诊断工具的。如果要深入分析问题，可以选择使用内置的Java飞行记录器:<a href="http://java-performance.info/oracle-java-mission-control-overview/">Java Mission Control</a>。</p>

<p>运行JFR需要三步：</p>

<ol>
<li><p>创建一个包含了你自己配置的JFR模板文件。运行<strong>jmc</strong>, 然后<strong>Window->Flight Recording Template Manage</strong>菜单。准备好档案后，就可以导出文件，并移动到要排查问题的环境中。</p></li>
<li><p>由于JFR需要JDK的商业证书，这一步需要解锁jdk的商业特性。</p>

<pre><code> jcmd &lt;PID&gt; VM.unlock_commercial_features
</code></pre></li>
<li><p>最后你就可以启动JFR。</p>

<pre><code> jcmd &lt;PID&gt; JFR.start name=test duration=60s settings=template.jfc filename=output.jfr
</code></pre>

<p> 上述命令立即启动JFR并开始使用<strong>templayte.jfc</strong>的配置收集60s的JVM信息，输出到<strong>output.jfr</strong>中。</p></li>
</ol>


<p>一旦记录完成之后，就可以复制.jfr文件到你的工作环境使用jmc GUI来分析。它几乎包含了排查jvm问题需要的所有信息，包括堆dump时的异常信息。</p>

<h3><a name='后记'></a>后记</h3>

<p>本文基本上是对英文原文的翻译，主要描述了几个常见问题的排查场景。</p>

<p>不得不说的是，JDK自带的工具是非常强大的。用好了这些工具其实已经足以应付绝大多数的Java问题排查场景。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[译]使用Groovy的AST Transformation实现DSL]]></title>
    <link href="http://www.rowkey.me/blog/2016/11/12/groovy-ast/"/>
    <updated>2016-11-12T21:21:34+08:00</updated>
    <id>http://www.rowkey.me/blog/2016/11/12/groovy-ast</id>
    <content type="html"><![CDATA[<p>最近在看一些java开源项目时，发现不少是用gradle做为项目构建工具的。之前虽然也用过gradle，但是却没怎么仔细留意build.gradle的语法是groovy的。但这次再怎么看也觉得里面的好多语法都和以前用过的groovy都联系不到一起。各种懵逼状态。。。后来阅读了这篇文章，算是解答了一些疑问：<a href="http://www.cnblogs.com/CloudTeng/p/3418072.html">http://www.cnblogs.com/CloudTeng/p/3418072.html</a>。但是对于下面这种写法，还是不知道是怎么回事：</p>

<pre><code>task copyFile(type: Copy){
    from 'xml'
    into 'destination'
}
</code></pre>

<p>copyFile做为task名称竟然不是一个字符串，阅读了groovy的文档也没发现字符串可以省略引号的说明(php中引号倒是可以省略），此外一个方法后面跟一个参数然后这个参数又跟着一个括号，这又是什么语法。。。凭直觉觉得这里的copyFile应该是一个方法，但是这时候copyFile还没有定义啊。。。</p>

<p>带着以上疑问，去翻了一下groovy的官方文档，凭感觉觉得gradle是利用了groovy的ast trasnfomation，也就是抽象语法树转换(故名思议，也就是能够转换groovy的语法树从而创造自己的一套语法)。那么到底是不是这样呢？<a href="http://blog.csdn.net/zxhoo/article/details/29830529">http://blog.csdn.net/zxhoo/article/details/29830529</a>给出了解释并证明了这个结论。但是groovy的ast transformation到底是什么东西呢？国外有一篇博客给出了比较清晰明了的讲述：<a href="http://www.jroller.com/DhavalDalal/entry/a_case_for_using_groovy">http://www.jroller.com/DhavalDalal/entry/a_case_for_using_groovy</a>。以下即对此篇博文的翻译。</p>

<!--more-->


<hr />

<p>为了给出此问题的一个上下文，在我目前的项目上创建了一个使用Groovy作为主要语言的内嵌DSL。这个DSL和MongoDB lingo类似，下面是一个例子：</p>

<pre><code>// Sample Delta file 
use test
db.customers.add("{'city' : 'Please Set City', 'pin':  'Please Pin code' }")

//Increment age field by 1
db.customers.transform('age', "{ $add: ["$age", 1] }")

// Set context to transactions db
use transactions

// add to orders collection a nested document
db.orders.add('{"dispatch" : { "status" : "Default", "address": { "line1" : "Road", "city": "City" }}}')
</code></pre>

<p>和Mongo Shell类似的，我想要支持在命令参数中使用单引号和双引号包裹住的字符串。和javascript一样，你可以在字符串内部使用引号，只要不要和外部包裹字符串的引号匹配就可以。为了实现这些，我现在遇到两个问题：</p>

<ol>
<li><strong><em>use</em></strong> 是Groovy的一个供Groovy Categories使用的默认方法，和Scala中的implicit以及c#中的扩展方法类似。</li>
<li>在add、tranform函数中的双引号参数是Groovy中的GString，可以使用$来做字符串替换-在Groovy的世界中，你可能听过&#8221;You need a $ in GString ;)&ldquo;这种说法。它会解析出现在$后面的表达式然后替换为表达式的字符串输出。此外，GString是延迟解析的，只有当toString被调用或者做为参数传递给函数的时候，GString才会对其中的$做解析。因此，上面的例子中age并没有定义，会在GString被解析的时候产生问题。</li>
</ol>


<p>当然，我们可以做一些hack的事情来解决上面的问题。我们不用use而是换成using来解决第一个问题。但是第二个问题，我怎样才能阻止人们不在函数参数中使用双引号字符串呢？在文档中注明规范意味着被动并且依赖于遵守规范的开发者。因此，这样做并不很hack。上面两个问题看起来都像是编译级别的问题。下面就讲述我是如何一石二鸟解决这些问题的。</p>

<p>Groovy提供了访问抽象语法树并转换它的方法。一个AST是编译器在编译阶段生成的中间表示。这里讲的AST指的是能够产生另外的翻译或者字节码。Groovy以<a href="http://groovy.codehaus.org/gapi/org/codehaus/groovy/transform/ASTTransformation.html">ASTTransformation</a>的形式提供了一个钩子让我们可以在编译阶段添加、修改语法树。一个实现了此接口的类必须以<a href="http://groovy.codehaus.org/gapi/org/codehaus/groovy/transform/GroovyASTTransformation.html">@GroovyASTTransformation</a>注解，这样Groovy才能知道应该在哪一个阶段运行。这样我可以处理全局AST转换，其中visit方法会为sourceUnit(原始的源代码)调用一次，并且我会忽略ASTNode[]中的第一个和第二个元素。下面是我的ASTTransformation代码：</p>

<pre><code>@Slf4j
@GroovyASTTransformation
public class StatementTransformation implements ASTTransformation {
  private def transformations = ['use' : 'using']

  @Override
  void visit(ASTNode[] nodes, SourceUnit source) {
    log.info("Source name = ${source.name}")
    ModuleNode ast = source.ast
    def blockStatement = ast.statementBlock

    blockStatement.visit(new CodeVisitorSupport() {
      void visitConstantExpression(ConstantExpression ce) {
        def name = ce.value
        if (transformations.containsKey(name)) {
          def newName = transformations[name]
          log.debug("Transform Name =&gt; $name -&gt; $newName")
          ce.value = newName
        } else {
          log.debug("Skip Name =&gt; $name")
        }
      }

      public void visitArgumentlistExpression(ArgumentListExpression ale) {
        log.debug("Arg List $ale.expressions")
        def expressions = ale.expressions
        expressions.eachWithIndex { expr, idx -&gt;
          if(expr.getClass() == GStringExpression) {
            log.debug("Transform GString =&gt; String ($expr.text)")
            expressions[idx] = new ConstantExpression(expr.text)
          }
        }
        log.debug("Transformed Arg List $ale.expressions")
        super.visitArgumentlistExpression(ale)
      }
    })
  }
}
</code></pre>

<ol>
<li>当遇到like, use, db, customers, add, transform, fn params等常量时，visitConstantExpression(&hellip;)会被调用。根据已经定义的transformations map(第四行)，相应的值会被简单重新赋值。(18行)</li>
<li>当调用函数时，visitArgumentlistExpression会被调用。在我的例子中db.customers.transform(&hellip;)和db.customers.add(&hellip;)是函数调用并且整个所有的参数都被传给了visitArgumentlistExpression方法。在GStringExpression出现的时候将它转换为了ConstantExpression(30行)。</li>
</ol>


<p>接下来看看如何使用上面的代码。</p>

<p>Reader读取所有的DSL文件，在的例子中，我们把它们叫做delta文件。对于每一个deleta文件，我创建了一个新的GroovyShell并让它去解析代码(delta文件中的)。这里的shell用我自定义的AST transformer做了相应的配置。shell解析出一个对象并传递给Parser。这样Pardser得到的结点其中的GString已经全被转换为了普通String，&#8217;use&#8217;也已经被转换为了&#8217;using&#8217;方法。</p>

<pre><code>@Slf4j
public class Reader {
  private def createNewShell() {
    def secureCustomizer = new SecureASTCustomizer()
    secureCustomizer.with {
      methodDefinitionAllowed = false // user will not be able to define methods
      importsWhitelist = [] // empty whitelist means imports are disallowed
      staticImportsWhitelist = [] // same for static imports
      staticStarImportsWhitelist = []
      ....
    }

    def astCustomizer = 
      new ASTTransformationCustomizer(new StatementTransformation())
    def config = new CompilerConfiguration()
    config.addCompilationCustomizers(secureCustomizer, 
                          astCustomizer)
    new GroovyShell(config)
  }

  public Tree read(final List&lt;File&gt; deltas) {
    def parser = new Parser()
    deltas.each { delta -&gt;
      def deltaName = delta.name
      def dslCode = """{-&gt; $delta.text}"""
      //shell evaluates once, hence create new each time
      def shell = createNewShell()
      def deltaObject = shell.evaluate(dslCode, deltaName)
      try {
        parser.parse(deltaObject)
      } catch (Throwable t) {
        throw new InvalidGrammar("$deltaName --&gt; ${t.message}")
      }
      shell = null
    }
    parser.ast()
  }
}
</code></pre>

<p>下面是Parser的代码。在自定义ast转换应用之后调用using(db)。这里聪明的读者会发现我是如何使用getProperty(Groovy元对象协议编程的一部分，和invokeMethod、methodmissing类似)来拦截住对象属性的访问来改变数据库上下文的。</p>

<pre><code>@Slf4j
class Parser {
  private Tree tree = new Tree()
  private def dbContext

  @CompileStatic
  def getProperty(String name) {
    log.debug("property name is: $name")
    if(name == 'db') {
      return dbContext
    }
    tree.using(name)
  }

  def using(db) {
     log.info "Setting db context to ${db.toString()}"
     dbContext = db
  }

  public Tree parse(Closure closure) {
    def cloned = closure.clone()
    cloned.delegate = this
    cloned.resolveStrategy = Closure.DELEGATE_FIRST
    cloned()
    tree
  }

  def ast() {
    tree
  }
}
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[JDK自带工具之概览]]></title>
    <link href="http://www.rowkey.me/blog/2016/11/03/jdk-tools/"/>
    <updated>2016-11-03T22:29:34+08:00</updated>
    <id>http://www.rowkey.me/blog/2016/11/03/jdk-tools</id>
    <content type="html"><![CDATA[<p>在我们平常对java程序进行问题排查、性能调优时，如果没有合适的工具，很多时候会事倍功半，甚至无法继续进行下去。其实，jdk自身已经提供了很多强大的工具供我们使用。本文就对这些工具做一个概览性的描述。</p>

<p>笔者的开发环境是：OS X EI Captian 10.11.6</p>

<p>JDK版本：</p>

<pre><code>java version "1.8.0_92"
Java(TM) SE Runtime Environment (build 1.8.0_92-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.92-b14, mixed mode)
</code></pre>

<p>JAVA_HOME/bin下的工具截图如下：</p>

<p><img src="http://www.rowkey.me/images/blog_images/jdk-tools.png" alt="" /></p>

<!--more-->


<ul>
<li><p>appletviewer: 用于运行并浏览applet小程序。</p></li>
<li><p>extcheck: 扩展检测工具，主要用于检测指定jar文件与当前已安装的Java SDK扩展之间是否存在版本冲突。</p></li>
<li><p>idlj: IDL转Java编译器(IDL-to-Java Compiler)，用于为指定的IDL文件生成Java绑定。IDL意即接口定义语言(Interface Definition Language)。</p></li>
<li><p>jar: jar文件管理工具，主要用于打包压缩、解压jar文件。</p></li>
<li><p>jarsigner: jar密匙签名工具。</p></li>
<li><p>java: Java运行工具，用于运行.class字节码文件或.jar文件。</p></li>
<li><p>javac: Java编译工具(Java Compiler)，用于编译Java源代码文件。</p></li>
<li><p>javadoc: Java文档工具，主要用于根据Java源代码中的注释信息生成HTML格式的API帮助文档。</p></li>
<li><p>javafxpackager: JavaFX包装器，用于执行与封装或签名JavaFX应用有关的任务。JDK 8u20已经迁移此工具到javapackager。</p></li>
<li><p>javah: Java头文件工具，用于根据Java类生成C/C++头文件和源文件(主要用于JNI开发领域)。</p></li>
<li><p>javap: Java反编译工具，主要用于根据Java字节码文件反汇编为Java源代码文件。</p></li>
<li><p>javapackager: 执行针对Java应用程序和JavaFX应用程序的打包和签名的任务。包含了javafxpackager的功能。</p></li>
<li><p>jcmd: Java 命令行(Java Command)，用于向正在运行的JVM发送诊断命令请求。</p></li>
<li><p>jconsole: 图形化用户界面的监测工具，主要用于监测并显示运行于Java平台上的应用程序的性能和资源占用等信息。</p></li>
<li><p>jdeps: 用于分析Java class的依赖关系.</p></li>
<li><p>jdb: Java调试工具(Java Debugger)，主要用于对Java应用进行断点调试。</p></li>
<li><p>jhat: Java堆分析工具(Java Heap Analysis Tool)，用于分析Java堆内存中的对象信息。</p></li>
<li><p>jinfo: Java配置信息工具(Java Configuration Information)，用于打印指定Java进程、核心文件或远程调试服务器的配置信息。</p></li>
<li><p>jjs: 对Nashorn引擎的调用。<a href="http://www.infoq.com/cn/articles/nashorn">Nashorn</a>是基于Java实现一个轻量级高性能的JavaScript运行环境。</p></li>
<li><p>jmap: Java内存映射工具(Java Memory Map)，主要用于打印指定Java进程、核心文件或远程调试服务器的共享对象内存映射或堆内存细节。</p></li>
<li><p>jmc: Java任务控制工具(Java Mission Control)，主要用于HotSpot JVM的生产时间监测、分析、诊断。开发者可以使用jmc命令来创建JMC工具。 <a href="https://docs.oracle.com/javacomponents/jmc-5-5/jmc-user-guide/toc.htm">https://docs.oracle.com/javacomponents/jmc-5-5/jmc-user-guide/toc.htm</a></p></li>
<li><p>jps: JVM进程状态工具(JVM Process Status Tool)，用于显示目标系统上的HotSpot JVM的Java进程信息。</p></li>
<li><p>jrunscript: Java命令行脚本外壳工具(command line script shell)，主要用于解释执行javascript、groovy、ruby等脚本语言。</p></li>
<li><p>jsadebugd: Java可用性代理调试守护进程(Java Serviceability Agent Debug Daemon)，主要用于附加到指定的Java进程、核心文件，或充当一个调试服务器。</p></li>
<li><p>jstack: Java堆栈跟踪工具，主要用于打印指定Java进程、核心文件或远程调试服务器的Java线程的堆栈跟踪信息。</p></li>
<li><p>jstat: JVM统计监测工具(JVM Statistics Monitoring Tool)，主要用于监测并显示JVM的性能统计信息，包括gc统计信息。</p></li>
<li><p>jstatd: jstatd(VM jstatd Daemon)工具是一个RMI服务器应用，用于监测HotSpot JVM的创建和终止，并提供一个接口，允许远程监测工具附加到运行于本地主机的JVM上。</p></li>
<li><p>jvisualvm: JVM监测、故障排除、分析工具，主要以图形化界面的方式提供运行于指定虚拟机的Java应用程序的详细信息。</p></li>
<li><p>keytool: 密钥和证书管理工具，主要用于密钥和证书的创建、修改、删除等。主要用于获取或缓存Kerberos协议的票据授权票据。允许用户查看本地凭据缓存和密钥表中的条目(用于Kerberos协议)。Kerberos密钥表管理工具，允许用户管理存储于本地密钥表中的主要名称和服务密钥。</p></li>
<li><p>native2ascii: 本地编码到ASCII编码的转换器(Native-to-ASCII Converter)，用于&#8221;任意受支持的字符编码&#8221;和与之对应的&#8221;ASCII编码和(或)Unicode转义&#8221;之间的相互转换。</p></li>
<li><p>orbd: 对象请求代理守护进程(Object Request Broker Daemon)，它使客户端能够透明地定位和调用位于CORBA环境的服务器上的持久对象。</p></li>
<li><p>pack200: JAR文件打包压缩工具，它可以利用Java类特有的结构，对普通JAR文件进行高效压缩，以便于能够更快地进行网络传输。这是微软提供的对象包装程序，用于对象安装包。</p></li>
<li><p>policytool: 策略工具，用于管理用户策略文件(.java.policy)。</p></li>
<li><p>rmic: Java RMI 编译器，为使用JRMP或IIOP协议的远程对象生成stub、skeleton、和tie类，也用于生成OMG IDL。</p></li>
<li><p>rmid: Java RMI 激活系统守护进程，rmid启动激活系统守护进程，允许在虚拟机中注册或激活对象。</p></li>
<li><p>rmiregistry: Java 远程对象注册表，用于在当前主机的指定端口上创建并启动一个远程对象注册表。</p></li>
<li><p>schemagen: XML schema生成器，用于生成XML schema文件。</p></li>
<li><p>serialver: 序列版本命令，用于生成并返回serialVersionUID。</p></li>
<li><p>servertool: Java IDL 服务器工具，用于注册、取消注册、启动和终止持久化的服务器。</p></li>
<li><p>tnameserv: Java IDL瞬时命名服务。</p></li>
<li><p>unpack200: JAR文件解压工具，将一个由pack200打包的文件解压提取为JAR文件。</p></li>
<li><p>wsgen: XML Web Service 2.0的Java API，生成用于JAX-WS Web Service的JAX-WS便携式产物。</p></li>
<li><p>wsimport: XML Web Service 2.0的Java API，主要用于根据服务端发布的wsdl文件生成客户端存根及框架</p></li>
<li><p>xjc: 主要用于根据XML schema文件生成对应的Java类。</p></li>
</ul>


<h2>参考资料</h2>

<p><a href="https://docs.oracle.com/javase/8/docs/technotes/tools/unix/toc.html">https://docs.oracle.com/javase/8/docs/technotes/tools/unix/toc.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java调优经验谈]]></title>
    <link href="http://www.rowkey.me/blog/2016/11/02/java-profile/"/>
    <updated>2016-11-02T19:29:34+08:00</updated>
    <id>http://www.rowkey.me/blog/2016/11/02/java-profile</id>
    <content type="html"><![CDATA[<h2>目录</h2>

<ul>
<li><a href="#%E8%B0%83%E4%BC%98%E5%87%86%E5%A4%87">调优准备</a></li>
<li><a href="#%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90">性能分析</a></li>
<li><a href="#%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98">性能调优</a></li>
<li><a href="#%E5%85%B6%E4%BB%96%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%AE">其他优化建议</a></li>
<li><a href="#JVM%E5%8F%82%E6%95%B0%E8%BF%9B%E9%98%B6">JVM参数进阶</a></li>
</ul>


<p>对于调优这个事情来说，一般就是三个过程：</p>

<ul>
<li>性能监控：问题没有发生，你并不知道你需要调优什么？此时需要一些系统、应用的监控工具来发现问题。</li>
<li>性能分析：问题已经发生，但是你并不知道问题到底出在哪里。此时就需要使用工具、经验对系统、应用进行瓶颈分析，以求定位到问题原因。</li>
<li>性能调优：经过上一步的分析定位到了问题所在，需要对问题进行解决，使用代码、配置等手段进行优化。</li>
</ul>


<p>Java调优也不外乎这三步。</p>

<p>此外，本文所讲的性能分析、调优等是抛开以下因素的：</p>

<ul>
<li>系统底层环境：硬件、操作系统等</li>
<li>数据结构和算法的使用</li>
<li>外部系统如数据库、缓存的使用</li>
</ul>


<!--more-->


<h2><a name='调优准备'></a>调优准备</h2>

<p>调优是需要做好准备工作的，毕竟每一个应用的业务目标都不尽相同，性能瓶颈也不会总在同一个点上。在业务应用层面，我们需要：</p>

<ul>
<li>需要了解系统的总体架构，明确压力方向。比如系统的哪一个接口、模块是使用率最高的，面临高并发的挑战。</li>
<li>需要构建测试环境来测试应用的性能，使用ab、loadrunner、jmeter都可以。</li>
<li>对关键业务数据量进行分析，这里主要指的是对一些数据的量化分析，如数据库一天的数据量有多少；缓存的数据量有多大等</li>
<li>了解系统的响应速度、吞吐量、TPS、QPS等指标需求，比如秒杀系统对响应速度和QPS的要求是非常高的。</li>
<li>了解系统相关软件的版本、模式和参数等，有时候限于应用依赖服务的版本、模式等，性能也会受到一定的影响。</li>
</ul>


<p>此外，我们还需要了解Java相关的一些知识：</p>

<ol>
<li>Java内存相关：这一部分可以参见<a href="http://www.rowkey.me/blog/2016/05/07/javamm/">谈谈Java内存管理</a>一文</li>
<li>对Java代码进行基准性能测试：可以使用JMH来进行，<a href="http://www.hollischuang.com/archives/1072">[译]使用JMH进行微基准测试：不要猜，要测试！</a>。</li>
<li>HotSpot VM相关知识：<a href="http://www.oracle.com/technetwork/cn/java/javase/tech/index-jsp-136373-zhs.html">http://www.oracle.com/technetwork/cn/java/javase/tech/index-jsp-136373-zhs.html</a></li>
<li>jdk自带各种java工具：<a href="http://www.rowkey.me/blog/2016/11/03/jdk-tools/">http://www.rowkey.me/blog/2016/11/03/jdk-tools/</a></li>
</ol>


<h2><a name='性能分析'></a>性能分析</h2>

<p>在系统层面能够影响应用性能的一般包括三个因素：CPU、内存和IO，可以从这三方面进行程序的性能瓶颈分析。</p>

<h3>CPU分析</h3>

<p>当程序响应变慢的时候，首先使用top、vmstat、ps等命令查看系统的cpu使用率是否有异常，从而可以判断出是否是cpu繁忙造成的性能问题。其中，主要通过us（用户进程所占的%）这个数据来看异常的进程信息。当us接近100%甚至更高时，可以确定是cpu繁忙造成的响应缓慢。一般说来，cpu繁忙的原因有以下几个：</p>

<ul>
<li>线程中有无限空循环、无阻塞、正则匹配或者单纯的计算</li>
<li>发生了频繁的gc</li>
<li>多线程的上下文切换</li>
</ul>


<p>确定好cpu使用率最高的进程之后就可以使用jstack来打印出异常进程的堆栈信息：</p>

<p><strong>jstack [pid]</strong></p>

<p><img src="http://www.rowkey.me/images/blog_images/profile/jstack.jpg" alt="jstack" /></p>

<p>接下来需要注意的一点是，Linux下所有线程最终还是以轻量级进程的形式存在系统中的，而使用jstack只能打印出进程的信息，这些信息里面包含了此进程下面所有线程(轻量级进程-LWP)的堆栈信息。因此，进一步的需要确定是哪一个线程耗费了大量cpu，此时可以使用top -p [processId]来查看，也可以直接通过ps -Le来显示所有进程,包括LWP的资源耗费信息。最后，通过在jstack的输出文件中查找对应的lwp的id即可以定位到相应的堆栈信息。其中需要注意的是线程的状态：RUNNABLE、WAITING等。对于Runnable的进程需要注意是否有耗费cpu的计算。对于Waiting的线程一般是锁的等待操作。</p>

<p>也可以使用jstat来查看对应进程的gc信息，以判断是否是gc造成了cpu繁忙。</p>

<p><strong>jstat -gcutil [pid]</strong></p>

<p><img src="http://www.rowkey.me/images/blog_images/profile/jstat.jpg" alt="jstat" /></p>

<p>还可以通过vmstat，通过观察内核状态的上下文切换(cs)次数，来判断是否是上下文切换造成的cpu繁忙。</p>

<p><strong>vmstat 1 5</strong></p>

<p><img src="http://www.rowkey.me/images/blog_images/profile/vmstat.jpg" alt="jstat" /></p>

<p>此外，有时候可能会由jit引起一些cpu飚高的情形，如大量方法编译等。这里可以使用-XX:+PrintCompilation这个参数输出jit编译情况，以排查jit编译引起的cpu问题。</p>

<h3>内存分析</h3>

<p>对Java应用来说，内存主要是由堆外内存和堆内内存组成。</p>

<ol>
<li><p>堆外内存</p>

<p> 堆外内存主要是JNI、Deflater/Inflater、DirectByteBuffer（nio中会用到）使用的。对于这种堆外内存的分析，还是需要先通过vmstat、sar、top、pidstat(这里的sar,pidstat以及iostat都是<a href="http://sebastien.godard.pagesperso-orange.fr/documentation.html">sysstat</a>软件套件的一部分，需要单独安装)等查看swap和物理内存的消耗状况再做判断的。此外，对于JNI、Deflater这种调用可以通过<a href="http://www.oschina.net/p/perftools">Google-preftools</a>来追踪资源使用状况。</p></li>
<li><p>堆内内存</p>

<p> 此部分内存为Java应用主要的内存区域。通常与这部分内存性能相关的有：</p>

<ul>
<li>创建的对象：这个是存储在堆中的，需要控制好对象的数量和大小，尤其是大的对象很容易进入老年代</li>
<li>全局集合：全局集合通常是生命周期比较长的，因此需要特别注意全局集合的使用</li>
<li>缓存：缓存选用的数据结构不同，会很大程序影响内存的大小和gc</li>
<li>ClassLoader：主要是动态加载类容易造成永久代内存不足</li>
<li>多线程：线程分配会占用本地内存，过多的线程也会造成内存不足</li>
</ul>


<p> 以上使用不当很容易造成：</p>

<ul>
<li>频繁GC -> Stop the world，使你的应用响应变慢</li>
<li>OOM，直接造成内存溢出错误使得程序退出。OOM又可以分为以下几种：

<ul>
<li>Heap space：堆内存不足</li>
<li>PermGen space：永久代内存不足</li>
<li>Native thread：本地线程没有足够内存可分配</li>
</ul>
</li>
</ul>


<p> 排查堆内存问题的常用工具是jmap，是jdk自带的。一些常用用法如下：</p>

<ul>
<li>查看jvm内存使用状况：jmap -heap <pid></li>
<li>查看jvm内存存活的对象：jmap -histo:live <pid></li>
<li>把heap里所有对象都dump下来，无论对象是死是活：jmap -dump:format=b,file=xxx.hprof <pid></li>
<li>先做一次full GC，再dump，只包含仍然存活的对象信息：jmap -dump:format=b,live,file=xxx.hprof <pid></li>
</ul>


<p> 此外，不管是使用jmap还是在OOM时产生的dump文件，可以使用Eclipse的MAT(MEMORY ANALYZER TOOL)来分析，可以看到具体的堆栈和内存中对象的信息。当然jdk自带的jhat也能够查看dump文件(启动web端口供开发者使用浏览器浏览堆内对象的信息)。此外，VisualVM也能够打开hprof文件，使用它的heap walker查看堆内存信息。</p>

<p> <img src="http://www.rowkey.me/images/blog_images/profile/jhat.png" alt="" /></p></li>
</ol>


<h3>IO分析</h3>

<p>通常与应用性能相关的包括：文件IO和网络IO。</p>

<ol>
<li><p>文件IO</p>

<p> 可以使用系统工具pidstat、iostat、vmstat来查看io的状况。这里可以看一张使用vmstat的结果图。</p>

<p> <img src="http://www.rowkey.me/images/blog_images/profile/io.png" alt="" /></p>

<p> 这里主要注意bi和bo这两个值，分别表示块设备每秒接收的块数量和块设备每秒发送的块数量，由此可以判定io繁忙状况。进一步的可以通过使用strace工具定位对文件io的系统调用。通常，造成文件io性能差的原因不外乎：</p>

<ul>
<li>大量的随机读写</li>
<li>设备慢</li>
<li>文件太大</li>
</ul>
</li>
<li><p>网络IO</p>

<p> 查看网络io状况，一般使用的是netstat工具。可以查看所有连接的状况、数目、端口信息等。例如：当time_wait或者close_wait连接过多时，会影响应用的相应速度。</p>

<pre><code> netstat -anp
</code></pre>

<p> <img src="http://www.rowkey.me/images/blog_images/profile/netstat.png" alt="" /></p>

<p> 此外，还可以使用tcpdump来具体分析网络io的数据。当然，tcpdump出的文件直接打开是一堆二进制的数据，可以使用wireshark阅读具体的连接以及其中数据的内容。</p>

<pre><code> tcpdump -i eth0 -w tmp.cap -tnn dst port 8080 #监听8080端口的网络请求并打印日志到tmp.cap中
</code></pre>

<p> 还可以通过查看/proc/interrupts来获取当前系统使用的中断的情况。</p>

<p> <img src="http://www.rowkey.me/images/blog_images/profile/interrupts.png" alt="" /></p>

<p> 各个列依次是：</p>

<pre><code> irq的序号， 在各自cpu上发生中断的次数，可编程中断控制器，设备名称（request_irq的dev_name字段）
</code></pre>

<p> 通过查看网卡设备的终端情况可以判断网络io的状况。</p></li>
</ol>


<h3>其他分析工具</h3>

<p>上面分别针对CPU、内存以及IO讲了一些系统/JDK自带的分析工具。除此之外，还有一些综合分析工具或者框架可以更加方便我们对Java应用性能的排查、分析、定位等。</p>

<ul>
<li><p>VisualVM</p>

<p>  这个工具应该是Java开发者们非常熟悉的一款java应用监测工具，原理是通过jmx接口来连接jvm进程，从而能够看到jvm上的线程、内存、类等信息。
  <img src="http://www.rowkey.me/images/blog_images/profile/visualvm.png" alt="" />
  如果想进一步查看gc情况，可以安装visual gc插件。此外，visualvm也有btrace的插件，可以可视化直观的编写btrace代码并查看输出日志。
  与VisualVm类似的，jconsole也是通过jmx查看远程jvm信息的一款工具，更进一步的，通过它还可以显示具体的线程堆栈信息以及内存中各个年代的占用情况，也支持直接远程执行MBEAN。当然，visualvm通过安装jconsole插件也可以拥有这些功能。
  <img src="http://www.rowkey.me/images/blog_images/profile/jconsole.png" alt="" />
  但由于这俩工具都是需要ui界面的，因此一般都是通过本地远程连接服务器jvm进程。服务器环境下，一般并不用此种方式。</p></li>
<li><p>Java Mission Control(jmc)</p>

<p>  此工具是jdk7 u40开始自带的，原来是JRockit上的工具，是一款采样型的集诊断、分析和监控与一体的非常强大的工具: <a href="https://docs.oracle.com/javacomponents/jmc-5-5/jmc-user-guide/toc.htm">https://docs.oracle.com/javacomponents/jmc-5-5/jmc-user-guide/toc.htm</a>。但是此工具是基于JFR(jcmd <PID> JFR.start name=test duration=60s settings=template.jfc filename=output.jfr)的，而开启JFR需要商业证书：jcmd <PID> VM.unlock_commercial_features。</p>

<p>  <img src="http://www.rowkey.me/images/blog_images/profile/jmc.png" alt="" /></p></li>
<li><p>Btrace</p>

<p>  这里不得不提的是btrace这个神器，它使用java attach api+ java agent + instrument api能够实现jvm的动态追踪。在不重启应用的情况下可以加入拦截类的方法以打印日志等。具体的用法可以参考<a href="http://calvin1978.blogcn.com/articles/btrace1.html">Btrace入门到熟练小工完全指南</a>。</p></li>
<li><p>Jwebap</p>

<p>  <a href="http://www.oschina.net/p/jwebap">Jwebap</a>是一款JavaEE性能检测框架，基于asm增强字节码实现。支持：http请求、jdbc连接、method的调用轨迹跟踪以及次数、耗时的统计。由此可以获取最耗时的请求、方法，并可以查看jdbc连接的次数、是否关闭等。但此项目是2006年的一个项目，已经将近10年没有更新。根据笔者使用，已经不支持jdk7编译的应用。如果要使用，建议基于原项目二次开发，同时也可以加入对redis连接的轨迹跟踪。当然，基于字节码增强的原理，也可以实现自己的JavaEE性能监测框架。</p>

<p>  <img src="http://www.rowkey.me/images/blog_images/profile/jwebap.png" alt="" /></p>

<p>  上图来自笔者公司二次开发过的jwebap，已经支持jdk8和redis连接追踪。</p></li>
<li><p>useful-scripts</p>

<p>  这里有一个本人参与的开源的项目：<a href="https://github.com/superhj1987/useful-scripts">https://github.com/superhj1987/useful-scripts</a>，封装了很多常用的性能分析命令，比如上文讲的打印繁忙java线程堆栈信息，只需要执行一个脚本即可。</p></li>
</ul>


<h2><a name='性能调优'></a>性能调优</h2>

<p>与性能分析相对应，性能调优同样分为三部分。</p>

<h3>CPU调优</h3>

<ul>
<li>不要存在一直运行的线程(无限while循环)，可以使用sleep休眠一段时间。这种情况普遍存在于一些pull方式消费数据的场景下，当一次pull没有拿到数据的时候建议sleep一下，再做下一次pull。</li>
<li>轮询的时候可以使用wait/notify机制</li>
<li>避免循环、正则表达式匹配、计算过多，包括使用String的format、split、replace方法(可以使用apache的commons-lang里的StringUtils对应的方法)，使用正则去判断邮箱格式(有时候会造成死循环)、序列/反序列化等。</li>
<li>结合jvm和代码，避免产生频繁的gc，尤其是full GC。</li>
</ul>


<p>此外，使用多线程的时候，还需要注意以下几点：</p>

<ul>
<li>使用线程池，减少线程数以及线程的切换</li>
<li>多线程对于锁的竞争可以考虑减小锁的粒度(使用ReetrantLock)、拆分锁(类似ConcurrentHashMap分bucket上锁), 或者使用CAS、ThreadLocal、不可变对象等无锁技术。此外，多线程代码的编写最好使用jdk提供的并发包、Executors框架以及ForkJoin等，此外<a href="http://ifeve.com/disruptor-getting-started/">Discuptor</a>和<a href="http://ifeve.com/introducing-actors-akka-notes-part-1/">Actor</a>在合适的场景也可以使用。</li>
</ul>


<h3>内存调优</h3>

<p>内存的调优主要就是对jvm的调优。</p>

<ul>
<li>合理设置各个代的大小。避免新生代设置过小(不够用，经常minor gc并进入老年代)以及过大(会产生碎片)，同样也要避免Survivor设置过大和过小。</li>
<li>选择合适的GC策略。需要根据不同的场景选择合适的gc策略。这里需要说的是，cms并非全能的。除非特别需要再设置，毕竟cms的新生代回收策略parnew并非最快的，且cms会产生碎片。此外，G1直到jdk8的出现也并没有得到广泛应用，并不建议使用。</li>
<li>jvm启动参数配置-XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:[log_path]，以记录gc日志，便于排查问题。</li>
</ul>


<p>其中，对于第一点，具体的还有一点建议：</p>

<ul>
<li><strong>年轻代大小选择</strong>：响应时间优先的应用，尽可能设大，直到接近系统的最低响应时间限制（根据实际情况选择）。在此种情况下，年轻代收集发生gc的频率是最小的。同时，也能够减少到达年老代的对象。吞吐量优先的应用，也尽可能的设置大，因为对响应时间没有要求，垃圾收集可以并行进行，建议适合8CPU以上的应用使用。</li>
<li><strong>年老代大小选择</strong>：响应时间优先的应用，年老代一般都是使用并发收集器，所以其大小需要小心设置，一般要考虑并发会话率和会话持续时间等一些参数。如果堆设置小了，会造成内存碎片、高回收频率以及应用暂停而使用传统的标记清除方式；如果堆大了，则需要较长的收集时间。最优化的方案，一般需要参考以下数据获得：

<ul>
<li>并发垃圾收集信息</li>
<li>持久代并发收集次数</li>
<li>传统GC信息</li>
<li>花在年轻代和年老代回收上的时间比例</li>
</ul>


<p>  一般吞吐量优先的应用都应该有一个很大的年轻代和一个较小的年老代。这样可以尽可能回收掉大部分短期对象，减少中期的对象，而年老代存放长期存活对象。</p></li>
</ul>


<p>此外，<strong>较小堆引起的碎片问题</strong>：因为年老代的并发收集器使用标记、清除算法，所以不会对堆进行压缩。当收集器回收时，会把相邻的空间进行合并，这样可以分配给较大的对象。但是，当堆空间较小时，运行一段时间以后，就会出现“碎片”，如果并发收集器找不到足够的空间，那么并发收集器将会停止，然后使用传统的标记、清除方式进行回收。如果出现“碎片”，可能需要进行如下配置：-XX:+UseCMSCompactAtFullCollection，使用并发收集器时，开启对年老代的压缩。同时使用-XX:CMSFullGCsBeforeCompaction=xx设置多少次Full GC后，对年老代进行压缩。</p>

<p>其余对于jvm的优化问题可见后面<strong>JVM参数进阶</strong>一节。</p>

<p>代码上，也需要注意：</p>

<ul>
<li>避免保存重复的String对象，同时也需要小心String.subString()与String.intern()的使用，尤其是后者其底层数据结构为StringTable，当字符串大量不重复时，会使得StringTable非常大(一个固定大小的hashmap，可以由参数-XX:StringTableSize=N设置大小)，从而影响young gc的速度。在jackson和fastjson中使用了此方法，某些场景下会引起gc问题: <a href="http://hellojava.info/?p=514">YGC越来越慢，为什么</a>。</li>
<li>尽量不要使用finalizer</li>
<li>释放不必要的引用：ThreadLocal使用完记得释放以防止内存泄漏，各种stream使用完也记得close。</li>
<li>使用对象池避免无节制创建对象，造成频繁gc。但不要随便使用对象池，除非像连接池、线程池这种初始化/创建资源消耗较大的场景，</li>
<li>缓存失效算法，可以考虑使用SoftReference、WeakReference保存缓存对象</li>
<li>谨慎热部署/加载的使用，尤其是动态加载类等</li>
<li><p>不要用Log4j输出文件名、行号，因为Log4j通过打印线程堆栈实现，生成大量String。此外，使用log4j时，建议此种经典用法，先判断对应级别的日志是否打开，再做操作，否则也会生成大量String。</p>

<pre><code>  if (logger.isInfoEnabled()) {
      logger.info(msg);
  }
</code></pre></li>
</ul>


<h3>IO调优</h3>

<p>文件IO上需要注意：</p>

<ul>
<li>考虑使用异步写入代替同步写入，可以借鉴redis的aof机制。</li>
<li>利用缓存，减少随机读</li>
<li>尽量批量写入，减少io次数和寻址</li>
<li>使用数据库代替文件存储</li>
</ul>


<p>网络IO上需要注意：</p>

<ul>
<li>和文件IO类似，使用异步IO、多路复用IO/事件驱动IO代替同步阻塞IO</li>
<li>批量进行网络IO,减少IO次数</li>
<li>使用缓存，减少对网络数据的读取</li>
<li>使用协程: <a href="http://colobu.com/2016/07/14/Java-Fiber-Quasar/">Quasar</a></li>
</ul>


<h2><a name='其他优化建议'></a>其他优化建议</h2>

<ul>
<li>算法、逻辑上是程序性能的首要，遇到性能问题，应该首先优化程序的逻辑处理</li>
<li>优先考虑使用返回值而不是异常表示错误</li>
<li>查看自己的代码是否对内联是友好的: <a href="http://www.infoq.com/cn/articles/Java-Application-Hostile-to-JIT-Compilation">你的Java代码对JIT编译友好么？</a></li>
</ul>


<p>此外，jdk7、8在jvm的性能上做了一些增强：</p>

<ul>
<li>通过-XX:+TieredCompilation开启JDK7的<a href="http://rednaxelafx.iteye.com/blog/1022095">多层编译（tiered compilation）支持</a>。多层编译结合了客户端C1编译器和服务端C2编译器的优点(客户端编译能够快速启动和及时优化，服务器端编译可以提供更多的高级优化)，是一个非常高效利用资源的切面方案。在开始时先进行低层次的编译，同时收集信息，在后期再进一步进行高层次的编译进行高级优化。<strong>需要注意的一点：</strong>这个参数会消耗比较多的内存资源，因为同一个方法被编译了多次，存在多份native内存拷贝，建议把code cache调大一点儿（-XX:+ReservedCodeCacheSize，InitialCodeCacheSize）。否则有可能由于code cache不足，jit编译的时候不停的尝试清理code cache，丢弃无用方法，消耗大量资源在jit线程上。</li>
<li>Compressed Oops：压缩指针在jdk7中的server模式下已经默认开启。</li>
<li>Zero-Based Compressed Ordinary Object Pointers：当使用了上述的压缩指针时，在64位jvm上，会要求操作系统保留从一个虚拟地址0开始的内存。如果操作系统支持这种请求，那么就开启了Zero-Based Compressed Oops。这样可以使得无须在java堆的基地址添加任何地址补充即可把一个32位对象的偏移解码成64位指针。</li>
<li>逃逸分析(Escape Analysis): Server模式的编译器会根据代码的情况，来判断相关对象的逃逸类型，从而决定是否在堆中分配空间，是否进行标量替换(在栈上分配原子类型局部变量)。此外，也可以根据调用情况来决定是否自动消除同步控制，如StringBuffer。这个特性从Java SE 6u23开始就默认开启。</li>
<li>NUMA Collector Enhancements：这个重要针对的是The Parallel Scavenger垃圾回收器。使其能够利用NUMA (Non Uniform Memory Access，即每一个处理器核心都有本地内存，能够低延迟、高带宽访问) 架构的机器的优势来更快的进行gc。可以通过-XX:+UseNUMA开启支持。</li>
</ul>


<p><strong>此外，网上还有很多过时的建议，不要再盲目跟随</strong>:</p>

<ul>
<li>变量用完设置为null，加快内存回收，这种用法大部分情况下并没有意义。一种情况除外：如果有个Java方法没有被JIT编译但里面仍然有代码会执行比较长时间，那么在那段会执行长时间的代码前显式将不需要的引用类型局部变量置null是可取的。具体的可以见R大的解释：<a href="https://www.zhihu.com/question/48059457/answer/113538171">https://www.zhihu.com/question/48059457/answer/113538171</a></li>
<li>方法参数设置为final，这种用法也没有太大的意义，尤其在jdk8中引入了effective final，会自动识别final变量。</li>
</ul>


<h2><a name='JVM参数进阶'></a>JVM参数进阶</h2>

<p>jvm的参数设置一直是比较理不清的地方，很多时候都搞不清都有哪些参数可以配置，参数是什么意思，为什么要这么配置等。这里主要针对这些做一些常识性的说明以及对一些容易让人进入陷阱的参数做一些解释。</p>

<p><strong><em>以下所有都是针对Oracle/Sun JDK 6来讲</em></strong></p>

<ol>
<li><p>启动参数默认值</p>

<p> Java有很多的启动参数，而且很多版本都并不一样。但是现在网上充斥着各种资料，如果不加辨别的全部使用，很多是没有效果或者本来就是默认值的。一般的，我们可以通过使用java -XX:+PrintFlagsInitial来查看所有可以设置的参数以及其默认值。也可以在程序启动的时候加入-XX:+PrintCommandLineFlags来查看与默认值不相同的启动参数。如果想查看所有启动参数(包括和默认值相同的)，可以使用-XX:+PrintFlagsFinal。
 <img src="http://www.rowkey.me/images/blog_images/profile/flags-1.png" alt="" />
 <img src="http://www.rowkey.me/images/blog_images/profile/flags-2.png" alt="" /></p>

<p> 输出里“=”表示使用的是初始默认值，而“:=”表示使用的不是初始默认值，可能是命令行传进来的参数、配置文件里的参数或者是<a href="https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/ergonomics.html">ergonomics</a>自动选择了别的值。</p>

<p> 此外，还可以使用jinfo命令显示启动的参数。</p>

<ul>
<li>jinfo -flags [pid] #查看目前启动使用的有效参数</li>
<li>jinfo -flag [flagName] [pid] #查看对应参数的值</li>
</ul>


<p> <strong>这里需要指出的是，当你配置jvm参数时，最好是先通过以上命令查看对应参数的默认值再确定是否需要设置。也最好不要配置你搞不清用途的参数，毕竟默认值的设置是有它的合理之处的。</strong></p></li>
<li><p>动态设置参数</p>

<p> 当Java应用启动后，定位到了是GC造成的性能问题，但是你启动的时候并没有加入打印gc的参数，很多时候的做法就是重新加参数然后重启应用。但这样会造成一定时间的服务不可用。最佳的做法是能够在不重启应用的情况下，动态设置参数。使用jinfo可以做到这一点(本质上还是基于jmx的)。</p>

<pre><code> jinfo -flag [+/-][flagName] [pid] #启用/禁止某个参数
 jinfo -flag [flagName=value] [pid] #设置某个参数
</code></pre>

<p> 对于上述的gc的情况，就可以使用以下命令打开heap dump并设置dump路径。</p>

<pre><code> jinfo -flag +HeapDumpBeforeFullGC [pid] 
 jinfo -flag +HeapDumpAfterFullGC [pid]
 jinfo -flag HeapDumpPath=/home/dump/dir [pid]
</code></pre>

<p> 同样的也可以动态关闭。</p>

<pre><code> jinfo -flag -HeapDumpBeforeFullGC [pid] 
 jinfo -flag -HeapDumpAfterFullGC [pid]
</code></pre>

<p> 其他的参数设置类似。</p></li>
<li><p>-verbose:gc 与 -XX:+PrintGCDetails</p>

<p> 很多gc推荐设置都同时设置了这两个参数，其实，只要打开了-XX:+PrintGCDetails，前面的选项也会同时打开，无须重复设置。</p></li>
<li><p>-XX:+DisableExplicitGC</p>

<p> 这个参数的作用就是使得system.gc变为空调用，很多推荐设置里面都是建议开启的。但是，如果你用到了NIO或者其他使用到堆外内存的情况，使用此选项会造成oom。可以用XX:+ExplicitGCInvokesConcurrent或XX:+ExplicitGCInvokesConcurrentAndUnloadsClasses(配合CMS使用，使得system.gc触发一次并发gc)代替。</p>

<p> 此外，还有一个比较有意思的地方。如果你不设置此选项的话，当你使用了RMI的时候，会周期性地来一次full gc。这个现象是由于分布式gc造成的，为RMI服务。具体的可见此链接内容中与dgc相关的：<a href="http://docs.oracle.com/javase/6/docs/technotes/guides/rmi/sunrmiproperties.html">http://docs.oracle.com/javase/6/docs/technotes/guides/rmi/sunrmiproperties.html</a></p></li>
<li><p>MaxDirectMemorySize</p>

<p> 此参数是设置的堆外内存的上限值。当不设置的时候为-1，此值为-Xmx减去一个survivor space的预留大小。</p></li>
<li><p>由于遗留原因，作用相同的参数</p>

<ul>
<li>-Xss 与 -XX:ThreadStackSize</li>
<li>-Xmn 与 -XX:NewSize，此外这里需要注意的是设置了-Xmn的话，NewRatio就没作用了。</li>
</ul>
</li>
<li><p>-XX:MaxTenuringThreshold</p>

<p> 使用工具查看此值默认值为15，但是选择了CMS的时候，此值会变成4。当此值设置为0时，所有eden里的活对象在经历第一次minor GC的时候就会直接晋升到old gen，survivor space直接就没用。</p></li>
<li><p>-XX:HeapDumpPath</p>

<p> 使用此参数可以指定-XX:+HeapDumpBeforeFullGC、-XX:+HeapDumpAfterFullGC、-XX:+HeapDumpOnOutOfMemoryError触发heap dump文件的存储位置。</p></li>
</ol>


<h2><a name='参考资料'></a>参考资料</h2>

<ul>
<li><a href="http://docs.oracle.com/javase/8/docs/technotes/guides/vm/performance-enhancements-7.html">Java HotSpot™ Virtual Machine Performance Enhancements</a></li>
<li><a href="http://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/index.html">Java HotSpot Virtual Machine Garbage Collection Tuning Guide </a></li>
<li><a href="http://hllvm.group.iteye.com/group/topic/27945">[HotSpot VM] JVM调优的&#8221;标准参数&#8221;的各种陷阱</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[BTrace原理浅析]]></title>
    <link href="http://www.rowkey.me/blog/2016/09/20/btrace/"/>
    <updated>2016-09-20T21:39:34+08:00</updated>
    <id>http://www.rowkey.me/blog/2016/09/20/btrace</id>
    <content type="html"><![CDATA[<p>之前在看agentzh的此篇博文<a href="http://openresty.org/posts/dynamic-tracing/">动态追踪技术漫谈</a>时，领会到了动态追踪技术的强大之处，也一直由于无法在不重启线上服务器的情况下排查线上问题在寻找Java中的动态追踪工具。在公司内部的JavaEE性能检测框架中，我们使用了asm做字节码注入来做线上性能的监测，沿着这个思路，如果要做到动态追踪应该是需要做字节码注入的，但是额外的一点是需要动态加载字节码替换掉原有的类的。此外，性能监测框架是需要耦合到业务应用中的，无法做到一个监测工具的灵活性。</p>

<p>后来听同事提到了BTrace这个工具，于是去尝试了一下。BTrace是SUN Kenai云计算开发平台下的一个开源项目，旨在为java提供安全可靠的动态跟踪分析工具。江南白衣的这篇文章<a href="http://calvin1978.blogcn.com/articles/btrace1.html">http://calvin1978.blogcn.com/articles/btrace1.html</a>做了比较详细的描述。</p>

<p>那么，BTrace这么神奇的功能是如何实现的呢？既然这是个开源的代码，那么直接从代码找原理。BTrace代码开源在<a href="https://github.com/btraceio/btrace">https://github.com/btraceio/btrace</a>。</p>

<!--more-->


<p>总体来说，BTrace是基于动态字节码修改技术(Hotswap)来实现运行时java程序的跟踪和替换。大体的原理可以用下面的公式描述：</p>

<pre><code>Client(Java compile api + attach api) + Agent（脚本解析引擎 + ASM + JDK6 Instumentation） + Socket
</code></pre>

<p>BTrace的入口类在<a href="https://github.com/btraceio/btrace/blob/master/src/share/classes/com/sun/btrace/client/Main.java">https://github.com/btraceio/btrace/blob/master/src/share/classes/com/sun/btrace/client/Main.java</a>中。在其main方法中，可以看到起最终的核心逻辑是在<a href="https://github.com/btraceio/btrace/blob/master/src/share/classes/com/sun/btrace/client/Client.java">https://github.com/btraceio/btrace/blob/master/src/share/classes/com/sun/btrace/client/Client.java</a>中。方法调用如下：</p>

<ul>
<li>client.compile</li>
<li>client.attach</li>
<li>client.submit</li>
</ul>


<h2>Client</h2>

<p>首先是client.compile方法，使用的是Java compile api，将我们传递的java源文件编译为.class文件，当然你如果使用btracec提前编译了源代码，那么这里就不会有这一步。</p>

<p>针对官方脚本的一个例子：</p>

<pre><code>import com.sun.btrace.annotations.*;
import static com.sun.btrace.BTraceUtils.*;
@BTrace
public class HelloWorld {
    @OnMethod(
        clazz="java.lang.Thread",
        method="start"
    )
    public static void func() {
        println("about to start a thread!");
    }
}
</code></pre>

<p>@OnMethod告诉Btrace解析引擎需要代理的类和方法。
这个例子的作用是当java.lang.Thread类的任意一个对象调用 start 方法后，会调用func方法。</p>

<p>client端在编译完脚本之后，进行了一次字节码修改，但是仅仅是做了一些兼容性，例如域访问控制器、简写等。</p>

<p>接着client.attach中使用java的attach api将agent动态attach到目标jvm进程中(ava agent，通常有两种方式添加到jvm进程中：动态attach；在目标jvm启动之前添加agent参数)。</p>

<pre><code>VirtualMachine vm  = VirtualMachine.attach(pid);
...
vm.loadAgent(agentPath, agentArgs);
</code></pre>

<p>最后client的submit方法，会向agent发送监控命令以及传递对应code的字节码。</p>

<h2>Agent</h2>

<p>BTrace的agent实现类就在<a href="https://github.com/btraceio/btrace/blob/master/src/share/classes/com/sun/btrace/agent/Main.java">https://github.com/btraceio/btrace/blob/master/src/share/classes/com/sun/btrace/agent/Main.java</a>中，具体的实现可以看其main方法，此agent的premain和agentmain方法都是调用了这个方法。这里需要注意的一点：必须要上jdk6，因为jdk5虽然已经有了instrument api，但是其仅仅支持premain方法，也就是仅仅支持在main方法运行之前执行一些动作，而jdk6后加入了agentmain方法和VirtualMachine，是可以在main方法运行后执行的(如果是通过命令行启动的，那么agentmain方法不会被调用)。此外，在jdk6之前，程序启动之后是无法再设置boot class加载路径和system class加载路径的。而jdk6之后，instrument新增的appendToBootstrapClassLoaderSearch和appendToSystemClassLoaderSearch是可以动态添加classpath的。</p>

<p>agent被提交到目标jvm进程后，首先会添加boot classpath.</p>

<pre><code>...
inst.appendToBootstrapClassLoaderSearch(jf);
...
inst.appendToSystemClassLoaderSearch(jf);
</code></pre>

<p>接着开启一个serversocket等待client的连接。之后client和agent之间的数据通讯，比如生成.class发送到agent，agent将线上程序打印的数据回传给
client都是通过socket来进行的。当agent接收到监控命令后，主要有以下两部分工作：</p>

<ul>
<li>重写类：遍历当前所有的class,根据正则找到匹配的类，用asm重写</li>
<li>替换类：替换掉原来的class</li>
</ul>


<p>agent接受到client发来的监控指令以及对应的参数后，会load所有的class,根据正则去匹配指定的类和方法，并使用脚本解析引擎去处理发送过来的字节码然后使用ASM将脚本里标注的类java.lang.Thread的字节码重写，植入跟踪代码或新的逻辑。在上面那个例子中，Java.lang.Thread这个类的字节码被重写并在start方法体尾部植入了func方法的调用。</p>

<p>BTrace的agent利用instrumentation的retransformClasses方法将原始字节码替换掉，使用的transfomer见<a href="https://github.com/btraceio/btrace/blob/master/src/share/classes/com/sun/btrace/runtime/BTraceTransformer.java">https://github.com/btraceio/btrace/blob/master/src/share/classes/com/sun/btrace/runtime/BTraceTransformer.java</a>。如下：</p>

<pre><code>new ClassFileTransformer() {
    public byte[] transform(ClassLoader l, String className, Class c， ProtectionDomain pd, byte[] b) throws IllegalClassFormatException {
        // BTrace解析脚本，利用asm重写bytecode，然后classLoader加载
    }
}, true);
</code></pre>

<p>其中，在agent的agentmain中通过handleNewClient方法启动一个异步线程进行class transformer，而在这个异步线程中最终是通过调用<a href="https://github.com/btraceio/btrace/blob/master/src/share/classes/com/sun/btrace/agent/Client.java">https://github.com/btraceio/btrace/blob/master/src/share/classes/com/sun/btrace/agent/Client.java</a>中的retransformLoaded()来进行的。</p>

<h2>总结</h2>

<p>其实BTrace就是使用了java attach api附加agent.jar，然后使用脚本解析引擎+asm来重写指定类的字节码，再使用instrument实现对原有类的替换。借鉴这些，我们也完全可以实现自己的动态追踪工具。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[推荐系统杂谈]]></title>
    <link href="http://www.rowkey.me/blog/2016/08/30/recommend-sys/"/>
    <updated>2016-08-30T20:39:34+08:00</updated>
    <id>http://www.rowkey.me/blog/2016/08/30/recommend-sys</id>
    <content type="html"><![CDATA[<p>推荐系统是近些年非常火的技术，不管是电商类软件还是新闻类app，都号称有精准的推荐系统能给你推送你最感兴趣的内容。现象级的资讯类app“今日头条”就得益于此成为了势头非常猛的一款产品。本文就针对推荐系统讲述一些相关概念和实践经验。</p>

<p>首先需要明确的就是推荐系统的目标，一般来说不外乎以下几个：</p>

<ul>
<li>用户满意性：首当其冲的，推荐系统主要就是为了满足用户的需求，因此准确率是评判一个推荐系统好坏的最关键指标。</li>
<li>多样性：虽然推荐系统最主要还是满足用户的兴趣，但是也要兼顾内容的多样性，对于权重不同的兴趣都要做到兼顾。</li>
<li>新颖性：用户看到的内容是那些他们之前没有听说过的物品。简单的做法就是在推荐列表去掉用户之前有过行为的那些内容。</li>
<li>惊喜度：和新颖性类似，但新颖性只是用户没看到过的但是确实是和他行为是相关的，而惊喜度是用户既没有看过和他之前的行为也不相关，但用户看到后的确是喜欢的。</li>
<li>实时性：推荐系统要根据用户的上下文来实时更新推荐内容，用户的兴趣也是随着时间而改变的，需要实时更新。</li>
<li>推荐透明度：对于用户看到的最终结果，要让用户知道推荐此内容的原因。比如，“买过这本书的人同时也买过”、&#8221;你购买过的xx和此商品类似&#8221;。</li>
<li>覆盖率：挖掘长尾内容也是推荐系统很重要的目标。因此，推荐的内容覆盖到的内容越多越好。</li>
</ul>


<!--more-->


<p>基于这些目标，推荐系统包括四种推荐方式：</p>

<ul>
<li>热门推荐：就是热门排行榜的概念。这种推荐方式不仅仅在IT系统，在平常的生活中也是处处存在的。这应该是效果最好的一种推荐方式，毕竟热门推荐的物品都是位于曝光量比较高的位置的。</li>
<li>人工推荐：人工干预的推荐内容。相比于依赖热门和算法来进行推荐。一些热点时事如世界杯、nba总决赛等就需要人工加入推荐列表。另一方面，热点新闻带来的推荐效果也是很高的。</li>
<li>相关推荐：相关推荐有点类似于关联规则的个性化推荐，就是在你阅读一个内容的时候，会提示你阅读与此相关的内容。</li>
<li>个性化推荐：基于用户的历史行为做出的内容推荐。也是本文主要讲述的内容。</li>
</ul>


<p>其中，前三者是和机器学习没有任何关系的，但却是推荐效果最好的三种方式。一般说来，这部分内容应该占到总的推荐内容的80%左右，另外20%则是对长尾内容的个性化推荐。</p>

<h2>个性化推荐系统</h2>

<p>个性化推荐是机器学习应用的一个典型场景。在本质上和搜索引擎是一样的，同样是为了解决信息过载的问题。搜索引擎某种意义上也是一个个性化推荐系统，但是其输入特征是可以从搜索关键字直接可以得到的。而一般的推荐系统，输入特征则是需要机器学习才能得到。</p>

<p>个性化推荐系统一般由日志系统、推荐算法、内容展示UI三部分组成。</p>

<ul>
<li>日志系统：这是推荐系统的输入源，是一个推荐系统所有信息的源头。</li>
<li>推荐算法：这是推荐系统的核心，根据输入数据得出最终的推荐结果的具体过程就在这里。</li>
<li>内容展示UI：对于推荐结果如何展示，也是一个值得权衡的地方。以更好地满足推荐系统的目标，并能更好的收集用户的行为信息等。</li>
</ul>


<p>其中，个性化推荐中最为核心的推荐算法，目前比较流行的有以下几种：</p>

<ul>
<li>基于内容的推荐：根据内容本身的属性(特征向量)所作的推荐。</li>
<li>基于关联规则的推荐：“啤酒与尿布”的方式，是一种动态的推荐，能够实时对用户的行为作出推荐。是基于物品之间的特征关联性所做的推荐，在某种情况下会退化为物品协同过滤推荐。</li>
<li>协同过滤推荐：与基于关联规则的推荐相比是一种静态方式的推荐，是根据用户已有的历史行为作分析的基础上做的推荐。可分为物品协同过滤、用户协同过滤、基于模型的协同过滤。其中，基于模型的协同又可以分为以下几种类型：基于距离的协同过滤；基于矩阵分解的协同过滤，即Latent Factor Model(SVD)；基于图模型协同，即Graph，也叫社会网络图模型。</li>
</ul>


<p>个性化推荐系统的典型架构如下图所示：</p>

<p><img src="http://www.rowkey.me/images/blog_images/recommend-sys/recommend-sys-arch.png" alt="recommend-sys" /></p>

<p>在线业务系统的日志接入数据高速公路，再由数据高速公路迅速运转到离线数据处理平台和在线流计算平台；离线数据处理平台周期性地以批处理方式加工过去一段时间的数据，得到人群标签和其他模型参数，存放在高速缓存中，供在线业务系统使用，与此同时，在线流计算平台实时对线上的日志数据做处理，对离线计算出的数据进行补充、修正等；在线业务系统综合离线特征和在线特征使用一定的逻辑得到输出供业务使用，产生的日志流入数据高速公路。</p>

<p>基于此框架，个性化推荐系统的典型流程如下所示：</p>

<p><img src="http://www.rowkey.me/images/blog_images/recommend-sys/recommend-process.png" alt="recommend" /></p>

<p>可知，一个推荐系统主要有以下模块组成：</p>

<ul>
<li>用户行为日志：此部分主要是用户行为日志的存储，属于数据统计的一部分, 存储在hive中。在此不做赘述。</li>
<li>数据ETL-1：将用户日志转换为推荐算法所需要的数据格式。</li>
<li>推荐算法：是个性化推荐最主要的部分，包括通过用户行为计算相关内容以及推荐结果等。</li>
<li>数据ETL-2: 将推荐算法得到的结果进一步加工为存储模块的输入数据。</li>
<li>用户画像存储：存储用户的偏好以及行为数据，如对内容关键字的偏好、点击过哪些内容等。</li>
<li>推荐结果存储：存储各种推荐算法产生的推荐结果，可以分为两部分：{用户 : itemList}推荐结果，为用户推荐的内容列表；{item : itemList}推荐结果，与item相关的内容列表。</li>
<li>服务调用模块：整合推荐结构，对外提供提供推荐的调用接口。</li>
</ul>


<h3>数据ETL-1</h3>

<p>对原始的用户行为等数据进行清洗、加工，如字段、属性、格式化等，作为下一步推荐算法的输入。</p>

<h3>推荐算法</h3>

<p>对于个性化推荐系统来说，推荐算法应该是其最核心的部分。目前有很多流行的算法，比如：</p>

<ul>
<li>基于内容和用户画像的推荐：此种算法，可见之前的一篇文章：<a href="http://www.rowkey.me/blog/2016/04/07/up-recommend/">http://www.rowkey.me/blog/2016/04/07/up-recommend/</a>。</li>
<li>基于矩阵分解的推荐: 基于SVD/ALS算法对用户进行内容推荐。相比起SVD，ALS更加适合解决稀疏矩阵的问题。Spark mlib中已经集成了对als算法的实现，需要做的就是在etl-1中把数据转换为als需要的数据格式以及调整als算法的各种参数。这里有一篇文章比较具体地描述了如何使用spark来做基于ALS的推荐：<a href="http://colobu.com/2015/11/30/movie-recommendation-for-douban-users-by-spark-mllib/">http://colobu.com/2015/11/30/movie-recommendation-for-douban-users-by-spark-mllib/</a>。</li>
<li>用户&amp;物品协同过滤推荐：包括UserBased CF和ItemBased CF。对于这两者，需要根据业务的不同来选择不同的算法。当用户非常多的时候，考虑到维护用户矩阵的成本，一般是不推荐选择用户协同过滤的，而对于候选item很多的时候，则不推荐使用物品协同过滤。</li>
</ul>


<p>推荐算法的输出结果一般是一个用户对应一个item列表或者是一个item对应一个item列表。此部分主要考虑的是算法的时间复杂度，不管是哪一种算法，一旦用户或者内容数据上了百万级别，都需要通过分布式计算如MapReduce、Spark等来进行解决。</p>

<p>推荐算法的基本流程如下图所示：</p>

<p><img src="http://www.rowkey.me/images/blog_images/recommend-sys/recommend-sys.png" style="width:400px"/></p>

<h3>数据ETL-2</h3>

<p>对推荐算法产生的结果进行清洗、格式化等，作为下一步存储模块的输入。</p>

<h3>用户画像存储</h3>

<p>存储用户的偏好以及行为数据等信息。对于偏好，采用标签量化来表示，是一种随着时间衰减的值。对于用户画像，是批量写入、实时读取，所以存储要着重考虑读的性能。可以选择使用Redis集群作为技术方案，能够最大满足读的性能，缺点是Redis的成本昂贵且不支持auto index。也可使用Hbase作为存储，使用<strong>ElasricSearch</strong>构建二级索引，以应对根据多种维度聚集用户的需求(比如过滤某一个标签下的所有用户)。</p>

<h3>推荐结果存储</h3>

<p>对各种推荐算法计算出的推荐结果的存储。存储空间要求大，格式复杂。对于存储的容量和读写性能要求都比较高。可以选择使用Redis集群作为此部分的存储方案。</p>

<h3>服务调用</h3>

<p>整合用户画像和推荐结果两部分数据，向外提供推荐调用的接口。主要是数据库IO调用开销。</p>

<ol>
<li>根据用户id，获取推荐的item列表。</li>
<li>根据item，获取相关联的item列表。</li>
<li>根据用户id, 获取用户画像。</li>
</ol>


<p>该模块需要采取一定的策略聚合多种推荐算法的推荐结果，直接面向业务。策略由于会随着面向的业务不同而不同，需要可配置化。同时也提供对外暴露用户画像的接口，使得业务方可以使用用户画像做针对性的处理。可以采用RPC机制对外暴露服务接口。</p>

<h2>需要考虑的问题</h2>

<p>对于一个推荐系统，结合其实现目标，还有一些需要注重考虑的问题。</p>

<h3>实时性问题</h3>

<p>由于计算用户、item矩阵或者进行矩阵分解是需要离线进行且比较耗时，因此协同的推荐算法是很难达到实时性的。实时部分的推荐主要依靠基于用户画像的推荐来进行。最终的推荐列表是根据一定的策略对这两部分进行聚合的结果。</p>

<h3>时效性内容问题</h3>

<p>时效性内容指的是那些与时间强相关的内容，比如新闻、时事等。如果一条10天前xx球员获得冠军的新闻现在被推荐了出来，可想用户肯定是莫名其妙或者是很失望的。因此，对于时效性内容，需要与普通的待推荐的内容区分开，做单独的推荐或者不走个性化推荐。</p>

<h3>冷启动问题</h3>

<p>不管使用何种推荐算法，都会面临冷启动问题：当用户是新用户，如何给用户推荐item呢？当内容是新内容，如何推荐给用户？</p>

<ul>
<li>对于新用户，可以采取的一种策略就是采用热门推荐或者人工推荐，把绝大多人关心的内容推荐出来。</li>
<li>对于内容，可以将内容分为新内容池和待推荐内容池。新内容产生时，首先进入新内容池。每次推荐的时候，先从新内容池做候选推荐，并给此内容的传播度+1，直到其传播度大于一个阈值的时候，将其移至待推荐内容池。这样既可以解决新内容的冷启动问题也在一定程度上可以保证新内容的曝光量。</li>
</ul>


<h3>多样性问题</h3>

<p>在基于用户画像的推荐算法中，取出用户的多个标签，然后根据相关度从不同的标签中取不同数量的内容，这样既兼顾了用户的多种兴趣也能够在一定程度上解决多样性的问题。</p>

<p>如：用户具有tag:A B C D,相关度为wA wB wC wD，Total推荐为总共需要推荐的条数，那么</p>

<pre><code>RecommendList(u) = A[Total推荐 * wA] + B[Total推荐 * wB] + C[Total推荐 * wC] + D[Total推荐 * wD]
</code></pre>

<h3>内容质量</h3>

<p>不管是热门推荐、人工推荐还是取某一标签下的内容列表都牵扯到的一个问题就是：如何给内容排序？</p>

<p>当用户对内容的喜好不一样，可以按照兴趣度来排序；但当无法区分兴趣度的时候(比如：用户是新用户；内容都是新内容；用户对于某一标签下的内容兴趣度一样)，可以使用内容质量来做排序。<strong>click/pv</strong>是一种评判内容质量的方式。此外，使用卷积神经网络相关算法也可以构建内容质量模型。</p>

<h4>惊喜问题</h4>

<p>推荐系统的惊喜目标一直是一个难题，被称作EE(Exploit &amp; Explore)问题，bandit算法是解决这个问题的一个派系，就是估计置信区间的做法，然后按照置信区间的上界来进行推荐，以UCB、LinUCB为代表的。简单点说就是先不考虑你喜不喜欢就把质量高的内容推荐给你，后面根据用户的行为反馈对推荐内容作调整。具体的可以参见此篇文章：<a href="https://mp.weixin.qq.com/s?__biz=MzA4OTk5OTQzMg==&amp;mid=2449231300&amp;idx=1&amp;sn=fe975d6af79596b5eaf576e5f65e8e06">推荐系统的苟且和远方</a>。</p>

<h2>总结</h2>

<p>借用<a href="http://itindex.net/detail/50820-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F">推荐系统的那点事</a>一文的几句话做为结语：</p>

<ul>
<li>实力派的【算法工程师】往往都是ABC[always be coding]，这样的算法工程师才能根据实际问题建立模型或者建立规则库，是真正能解决问题的人。往往是一些有研究背景，经验丰富的研究员，更加重视工程，因为工程架构上一些恰当合理的设计，效果往往就能远远高过于模型算法优化。</li>
<li>学院派的【算法工程师】往往是为了算法而算法，而不是为了解决推荐系统的问题去找最适合算法。这也是为什么大公司经常招了一些博士毕业的算法工程师后，不是研究算法而是让他们整天在那看数据报表？【因为发现算法没啥好研究，只能让他们在那看看报表找找规律了。】</li>
<li>【几乎所有所谓的智能推荐算法都是花拳绣腿】</li>
<li>当一个做推荐系统的部门开始重视【数据清理，数据标柱，效果评测，数据统计，数据分析】这些所谓的脏活累活，这样的推荐系统才会有救。</li>
</ul>


<p><strong><em>以上是推荐系统实践的一些经验</em></strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[谈谈互联网后端基础设施]]></title>
    <link href="http://www.rowkey.me/blog/2016/08/27/server-basic-tech-stack/"/>
    <updated>2016-08-27T23:10:34+08:00</updated>
    <id>http://www.rowkey.me/blog/2016/08/27/server-basic-tech-stack</id>
    <content type="html"><![CDATA[<p>对于一个互联网企业，后端服务是必不可少的一个组成部分。抛开业务应用来说，往下的基础服务设施做到哪些才能够保证业务的稳定可靠、易维护、高可用呢？纵观整个互联网技术体系再结合公司的目前状况，个人认为必不可少或者非常关键的后端基础技术/设施如下图所示：</p>

<p><a href="http://www.rowkey.me/images/blog_images/server_basic_stack/server-basic-tech-stack.png" target="_blank"><img src="http://www.rowkey.me/images/blog_images/server_basic_stack/server-basic-tech-stack.png"/></a></p>

<ul>
<li><a href="#Api%E7%BD%91%E5%85%B3">Api网关</a></li>
<li><a href="#%E4%B8%9A%E5%8A%A1%E5%BA%94%E7%94%A8%E5%92%8C%E5%90%8E%E7%AB%AF%E5%9F%BA%E7%A1%80%E6%A1%86%E6%9E%B6">业务应用和后端基础框架</a></li>
<li><a href="#%E7%BC%93%E5%AD%98%E3%80%81%E6%95%B0%E6%8D%AE%E5%BA%93%E3%80%81%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E3%80%81%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97">缓存、数据库、搜索引擎、消息队列</a></li>
<li><a href="#%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8">文件存储</a></li>
<li><a href="#%E7%BB%9F%E4%B8%80%E8%AE%A4%E8%AF%81%E4%B8%AD%E5%BF%83">统一认证中心</a></li>
<li><a href="#%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95%E7%B3%BB%E7%BB%9F">单点登录系统</a></li>
<li><a href="#%E7%BB%9F%E4%B8%80%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83">统一配置中心</a></li>
<li><a href="#%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86%E6%A1%86%E6%9E%B6">服务治理框架</a></li>
<li><a href="#%E7%BB%9F%E4%B8%80%E8%B0%83%E5%BA%A6%E4%B8%AD%E5%BF%83">统一调度中心</a></li>
<li><a href="#%E7%BB%9F%E4%B8%80%E6%97%A5%E5%BF%97%E6%9C%8D%E5%8A%A1">统一日志服务</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD">数据基础设施</a></li>
<li><a href="#%E6%95%85%E9%9A%9C%E7%9B%91%E6%8E%A7">故障监控</a></li>
</ul>


<p>这里的后端基础设施主要指的是应用在线上稳定运行需要依赖的关键组件/服务等。开发或者搭建好以上的后端基础设施，一般情况下是能够支撑很长一段时间内的业务的。此外，对于一个完整的架构来说，还有很多应用感知不到的系统基础服务，如负载均衡、自动化部署、系统安全等，并没有包含在本文的描述范围内。</p>

<!--more-->


<h2><a name='Api网关'></a>Api网关</h2>

<p>在移动app的开发过程中，通常后端提供的接口需要以下功能的支持：</p>

<ul>
<li>负载均衡</li>
<li>api访问权限控制</li>
<li>用户鉴权</li>
</ul>


<p>一般的做法，使用nginx做负载均衡，然后在每个业务应用里做api接口的访问权限控制和用户鉴权，更优化一点的方式则是把后两者做成公共类库供所有业务调用。但从总体上来看，这三种特性都属于业务的公共需求，更可取的方式则是集成到一起作为一个服务，既可以动态地修改权限控制和鉴权机制，也可以减少每个业务集成这些机制的成本。这种服务就是Api网关(<a href="http://blog.csdn.net/pzxwhc/article/details/49873623">http://blog.csdn.net/pzxwhc/article/details/49873623</a>)，可以选择自己实现，也可以使用开源软件实现，如<a href="https://getkong.org/">Kong</a>。如下图所示：</p>

<p><img src="http://www.rowkey.me/images/blog_images/server_basic_stack/api_gw.png" alt="api_gw.png" /></p>

<p>但是以上方案的一个问题是由于所有api请求都要经过网关，它很容易成为系统的性能瓶颈。因此，可以采取的方案是：去掉api网关，让业务应用直接对接统一认证中心，在基础框架层面保证每个api调用都需要先通过统一认证中心的认证，这里可以采取缓存认证结果的方式避免对统一认证中心产生过大的请求压力。</p>

<h2><a name='业务应用和后端基础框架'></a>业务应用和后端基础框架</h2>

<p>业务应用分为：在线业务应用和内部业务应用。</p>

<ul>
<li>在线业务应用：直接面向互联网用户的应用、接口等，典型的特点就是：请求量大、高并发、高可用、对故障的容忍度低。</li>
<li>内部业务应用：这个是面向公司内部的应用。比如，内部数据管理平台、广告投放平台等。相比起在线业务应用，其特点: 数据保密性高、压力小、并发量小、允许故障的发生。</li>
</ul>


<p>业务应用基于后端的基础框架开发，针对Java后端来说，应该有的几个框架如下：</p>

<ul>
<li>MVC框架：从十年前流行的Struts1、2到现在最为推崇的SpringMVC、Jersey以及国人开发的JFinal、阿里的WebX等等，这些框架尤其是后面流行的这些都是各有千秋的。选型的主要因素是看你的团队是否有一个对某框架能够做二次开发、定制的人在。很多时候，针对这些通用的框架，你是需要做一些特定的开发才能满足特定的需求的。比如，很多团队传递参数使用的都是UnderScore的命名法(下划线连接单词)，但是Java中确是使用LowCamel命名的。对于SpringMVC，可以通过注解的alias来指定，但这样需要对每一个参数都要指定alias有点效率太低，此外ModelAttribute也不支持别名，更好的方式是在框架层面统一对参数做Camel命名的转换达到目的。</li>
<li>IOC框架：ioc带来的好处无须多言。目前Java中最为流行的Spring自诞生就天然支持IOC。</li>
<li>ORM框架：MyBatis是目前最为流行的orm框架。此外，Spring ORM中提供的JdbcTemplate也很不错。当然，对于分库分表、主从分离这些需求，一般就需要实现自己的ORM框架来支持了，像阿里的tddl。此外，为了在服务层面统一解决分库分表、主从分离、主备切换、缓存、故障恢复等问题，很多公司都是有自己的数据库中间件的，比如阿里的Cobar、360的Atlas、网易的DDB，还有官方提供的<a href="http://downloads.mysql.com/archives/proxy/">MySQL Proxy</a>以及开源的<a href="https://github.com/MyCATApache/Mycat-Server">MyCat</a>、<a href="https://github.com/flike/kingshard">kingshard</a>和收费的<a href="http://www.onexsoft.com/?page_id=3391">oneproxy</a>。目前，线上有一定规模使用的应该是kingshard，当然如果不缺钱也可以上oneproxy。</li>
<li>缓存框架：缓存框架主要指的是对redis、memcached这些缓存服务器的操作统一封装，一般使用Spring的RedisTemplate即可，也可以使用jedis做自己的封装，支持客户端分布式方案、主从等。</li>
<li>JavaEE应用性能检测框架：对于线上的JavaEE应用，需要有一个统一的框架集成到每一个业务中检测每一个请求、方法调用、jdbc连接、redis连接等的耗时、状态等。<a href="http://www.oschina.net/p/jwebap">jwebap</a>是一个可以使用的性能检测工具，但由于其已经很多年没有更新，有可能的话建议基于此项目做二次开发。</li>
</ul>


<p>一般来说，以上几个框架即可以完成一个后端应用的雏形。</p>

<p>对于这些框架来说，最为关键的是根据团队技术构成选择最合适的，有能力开发自己的框架则更好。此外，这里需要提供一个后端应用的模板或生成工具(如maven的archetype)给团队成员使用，可以让大家在开发新的应用的时候，迅速的生成雏形应用，而无需再做一些框架搭建的重复性劳动。</p>

<h2><a name='缓存、数据库、搜索引擎、消息队列'></a>缓存、数据库、搜索引擎、消息队列</h2>

<p>缓存、数据库、搜索引擎、消息队列这四者都是应用依赖的后端基础服务，他们的性能直接影响到了应用的整体性能，有时候你代码写的再好也许就是因为这些服务导致应用性能无法提升上去。</p>

<h3>缓存</h3>

<p>如缓存五分钟法则所讲：如果一个数据频繁被访问，那么就应该放内存中。这里的缓存就是一种读写效率都非常高的存储方案，能够应对高并发的访问请求，通常情况下也不需要持久化的保证。但相对其他存储来说，缓存一般是基于内存的，成本比较昂贵，因此不能滥用。</p>

<p>缓存可以分为：本地缓存和分布式缓存。</p>

<ul>
<li>本地缓存：主要指的是内存中的缓存机制。在Java中，Google Guava中就提供了本地缓存的实现机制。当然使用java的ConncurrentHashMap你也可以实现自己的本地缓存方案。</li>
<li>分布式缓存：指的单独的缓存服务。几年前比较流行的是memcached，但其只是一个KV的存储，支持的数据结构太少。现在最为流行的就是Redis，能够支持丰富的数据结构，基于事件驱动的单线程非阻塞IO也能够应对高并发的场景。集群方案除了官方的redis cluster, 目前比较流行的还有豌豆荚的<a href="https://github.com/wandoulabs/codis">codis</a>、twitter的<a href="https://github.com/twitter/twemproxy">twemproxy</a>。</li>
</ul>


<p>对于缓存的使用，需要注意以下几点：</p>

<ul>
<li>缓存的失效机制：当给某一个key设置了有效期，那么缓存何时对此key进行删除呢？一般来说会有以下几种方式：

<ul>
<li>守护进程定时去扫描key，找到已经失效的key，然后删除</li>
<li>读取key的时候先去判断key是否失效，如果失效则删除并返回空。</li>
</ul>
</li>
<li>缓存的淘汰机制：是当缓存内存达到上限时如何删除缓存中的key。Redis提供了以下数据淘汰策略：

<ul>
<li>volatile-lru：从已设置过期时间的数据集中挑选最近最少使用的数据淘汰</li>
<li>volatile-ttl：从已设置过期时间的数据集中挑选将要过期的数据淘汰</li>
<li>volatile-random：从已设置过期时间的数据集中任意选择数据淘汰</li>
<li>allkeys-lru：从数据集中挑选最近最少使用的数据淘汰</li>
<li>allkeys-random：从数据集中任意选择数据淘汰</li>
<li>no-enviction（驱逐）：禁止驱逐数据</li>
</ul>


<p>  对于其具体的实现机制，可以参考<a href="http://redisbook.com/">《Redis设计与实现》</a>一书</p></li>
<li>缓存的更新机制: 通常来说有四种方式：Cache aside, Read through, Write through, Write behind caching，具体的可见陈皓大神的这篇总结：<a href="http://coolshell.cn/articles/17416.html">缓存更新的套路</a>。</li>
<li>缓存的服务过载保护：缓存的服务过载指的是由于缓存失效，而引起后端服务的压力骤增，进一步产生雪崩效应。这个现象和缓存更新是相关的，采取何种策略在缓存失效的时候去更新缓存直接决定了服务过载的保护机制。通常的分为客户端和服务端的应对方案。前者的方案有：基于超时的简单模式、基于超时的常规模式、基于刷新的简单模式、基于刷新的常规模式、基于刷新的续费模式。后者的方案则是很常见的流量控制和服务降级。具体的可以看美团技术团队总结的这篇文章：<a href="http://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&amp;mid=2651745239&amp;idx=1&amp;sn=60490558770ade79fd9f1e88f9c7c0ac&amp;scene=1&amp;srcid=0617o5PapWXlKUP4OxSzA7KE#rd">Cache应用中的服务过载案例研究</a>。</li>
</ul>


<h3>数据库</h3>

<p>数据库是后端开发中非常常见的一个服务组件。对于数据库的选型，要根据业务的特点和数据结构的特点来决定。</p>

<p>从存储介质上，数据库可以分为：</p>

<ul>
<li>内存数据库： 数据主要存储在内存中，同时也可以采取措施对数据进行持久化到硬盘中。如Redis、H2DB的内存模式。对于这种数据库，由于内存成本昂贵，因此一定要做好存储的量化分析、容量预估，防止内存不足造成服务不可用。</li>
<li>硬盘数据库：数据存储在硬盘上的这种数据库是最为常见的。MySQL、Oracle、Postgresql、HBASE、H2DB、SqlLite等等都是硬盘数据库。此外，<a href="https://github.com/ideawu/ssdb">SSDB</a>是基于SSD硬盘的KV数据库，支持的数据接口很丰富，是Redis的另外一个选择。</li>
</ul>


<p>从存储数据类型、数据模式上，数据库可以分为：</p>

<ul>
<li>关系型数据库：MySQL、Oracle、Postgresql都是关系型数据库的，是采用关系模型(关系模型指的就是二维表格模型，而一个关系型数据库就是由二维表及其之间的联系所组成的一个数据组织)来组织数据的数据库。</li>
<li>非关系型数据库：非关系型数据库是相对关系型数据库来讲的。以键值对存储，且结构不固定，每一个元组可以有不一样的字段，每个元组可以根据需要增加一些自己的键值对，这样就不会局限于固定的结构，可以减少一些时间和空间的开销。但是，其没有关系型数据库那种严格的数据模式，并不适合复杂的查询以及需要强事务管理的业务。非关系型数据库又可以分为：

<ul>
<li>KV数据库：主要以(key,value)键值对存储数据的数据库。以Redis、RocksDB(levelDB)、SSDB为代表。</li>
<li>文档数据库：总体形式上也是键值对的形式，但是值里面又可以有各种数据结构：数组、键值对、字符串等等。以mongodb、couchdb为代表。</li>
<li>列数据库：也叫作稀疏大数据库，一般是用来存储海量数据的。相对于行数据库，这种数据库是以列为单位存储数据在介质上的。以Hbase、Cassendra为代表。</li>
</ul>
</li>
</ul>


<p>和数据库相关的一个很重要的就是数据库的索引。有一种说法是：“掌握了索引就等于掌握了数据库”。暂且不去评判此说法是否真的准确，但索引的确关系着数据库的读写性能。需要对数据库的索引原理做到足够的了解才能更好的使用各种数据库。通常来说，Mysql、Oracle、Mongodb这些都是使用的B树作为索引，是考虑到传统硬盘的特点后兼顾了读写性能以及范围查找需求的选择，而Hbase用得LSM则是为了提高写性能对读性能做了牺牲。</p>

<h3>搜索引擎</h3>

<p>搜索引擎也是后端应用中一个很关键的组件，尤其是对内容类、电商类的应用，通过关键词、关键字搜索内容、商品是一个很常见的用户场景。比较成熟的开源搜索引擎有Solr和Elasticsearch，很多中小型互联网公司搜索引擎都是基于这两个开源系统搭建的。它们都是基于Lucence来实现的，不同之处主要在于termIndex的存储、分布式架构的支持等等。</p>

<p>对于搜索引擎的使用，从系统熟悉、服务搭建、功能定制，需要花费较长时间。在这个过程中，需要注意以下问题：</p>

<ul>
<li>搜索引擎与公司现有数据系统的集成。现有的持久化、供搜索的数据的载体是什么, 如何让搜索引擎在全量和增量建索引过程中无缝集成原来的数据载体，才能发挥搜索引擎自身的实时性, 水平扩展性(性能与容量和机器数量成正比)等优势。</li>
<li>和数据库一样，对搜索引擎的索引机制也需要做到深入的了解。</li>
</ul>


<p>更为详细的对于搜索引擎的工程化实践可以参考有赞工程师的这篇文章：<a href="http://www.cnblogs.com/hsydj/p/5303050.html">有赞搜索引擎实践(工程篇)</a></p>

<p>另外，搜索引擎还可以用在数据的多维分析上，就是<a href="https://www.growingio.com/">GrowingIO</a>、<a href="https://mixpanel.com/">MixPanel</a>中的可以任意维度查询数据报表的功能。当然，<a href="http://druid.io/">druid</a>也许是一个更好的实现多维分析的方案，官方也有其与es的比较：<a href="http://druid.io/docs/latest/comparisons/druid-vs-elasticsearch.html">http://druid.io/docs/latest/comparisons/druid-vs-elasticsearch.html</a>。</p>

<h3>消息队列</h3>

<p>软件的组织结构，从开始的面向组件到SOA、SAAS是一个逐渐演变的过程。而到了今天微服务盛行的时代，你都不好意思说自己的系统只是单一的一个系统而没有解耦成一个个service。当然，小的系统的确没有拆分的必要性，但一个复杂的系统拆成一个个service做微服务架构确实是不得不做的事情。</p>

<p>那么问题就来了，service之间的通信如何来做呢？使用什么协议？通过什么方式调用？都是需要考虑的问题。</p>

<p>先抛开协议不谈，service之间的调用方式可以分为同步调用以及异步调用。同步调用的方式无需多说，那么异步调用是怎么进行的呢？一种很常见的方式就是使用消息队列，调用方把请求放到队列中即可返回，然后等待服务提供方去队列中去获取请求进行处理，然后把结果返回给调用方即可（可以通过回调）。</p>

<p>异步调用就是消息中间件一个非常常见的应用场景。此外，消息队列的应用场景还有以下：</p>

<ul>
<li>解耦：一个事务，只关心核心的流程，需要依赖其他系统但不那么重要的事情，有通知即可，无须等待结果。</li>
<li>最终一致性：指的是两个系统的状态保持一致，要么都成功，要么都失败，可以有一定的延迟，只要最终达到一致性即可。</li>
<li>广播：这是消息队列最基本的功能。生产者只需要发布消息，无须关心有哪些订阅者来消费消息。</li>
<li>错峰与流控：当上下游系统处理能力不同的时候就需要类似消息队列的方式做为缓冲区来隔开两个系统。</li>
</ul>


<p>目前主流的消息队列软件，主要有以下几种：</p>

<ul>
<li>ActiveMQ：Java中最为简单的消息队列，是对JMS的实现，没有规定消息的顺序、安全、重发等特性。</li>
<li>RabbitMQ：是对AMQP协议的实现，对于消息的顺序性、安全、重发等都做了很好的支持。比较适合不允许数据丢失、有事务需求的业务场景下的消息传输。</li>
<li>Kafka：是基于Log的消息队列，底层依赖于文件的顺序读取，是append-only的。适合对数据丢失不敏感、强调性能的一些海量日志传输场景中。是最近几年大数据领域很火的一个技术。</li>
<li>ZeroMQ：是一个网络编程的Pattern库，将常见的网络请求形式（分组管理，链接管理，发布订阅等）模式化、组件化，简而言之socket之上、MQ之下。对于MQ来说，网络传输只是它的一部分，更多需要处理的是消息存储、路由、Broker服务发现和查找、事务、消费模式（ack、重投等）、集群服务等。</li>
</ul>


<h2><a name='文件存储'></a>文件存储</h2>

<p>不管是业务应用、依赖的后端服务还是其他的各种服务，最终还是要依赖于底层文件存储的。通常来说，文件存储需要满足的特性有：可靠性、容灾性、稳定性，即要保证存储的数据不会轻易丢失，即使发生故障也能够有回滚方案，也要保证高可用率。在底层可以采用传统的RAID作为解决方案，再上一层，目前hadoop的hdfs则是最为普遍的分布式文件存储方案，当然还有NFS、Samba这种共享文件系统也提供了简单的分布式存储的特性。</p>

<p>此外，如果文件存储确实成为了应用的瓶颈或者必须提高文件存储的性能从而提升整个系统的性能时，那么最为直接和简单的做法就是抛弃传统机械硬盘，用SSD硬盘替代。像现在很多公司在解决业务性能问题的时候，最终的关键点往往就是SSD。这也是用钱换取时间和人力成本最直接和最有效的方式。在数据库部分描述的SSDB就是对LevelDB封装之后，利用SSDB的特性的一种高性能KV数据库。</p>

<p>至于HDFS，如果要使用上面的数据，是需要通过hadoop的。类似xx on yarn的一些技术就是将非hadoop技术跑在hdfs上的解决方案(当然也是为了使用MR)。</p>

<h2><a name='统一认证中心'></a>统一认证中心</h2>

<p>统一认证中心，主要是对app用户、内部用户、app等的认证服务，包括</p>

<ul>
<li>用户的注册、登录验证、token鉴权</li>
<li>内部信息系统用户的管理和登录鉴权</li>
<li>App的管理，包括app的secret生成，app信息的验证(如验证接口签名)等。</li>
</ul>


<p>之所以需要统一认证中心，就是为了能够集中对这些所有app都会用到的信息进行管理，也给所有应用提供统一的认证服务。尤其是在有很多业务需要共享用户数据的时候，构建一个统一认证中心是非常必要的。此外，通过统一认证中心构建移动app的单点登录也是水到渠成的事情(模仿web的机制，将认证后的信息加密存储到本地磁盘中供多个app使用)。</p>

<h2><a name='单点登录系统'></a>单点登录系统</h2>

<p>目前很多大的在线web网站都是有单点登录系统的，通俗的来说就是只需要一次用户登录，就能够进入多个业务应用(权限可以不相同)，非常方便用户的操作。而在移动互联网公司中，内部的各种管理、信息系统同样也需要单点登录系统。目前，比较成熟的、用的最多的单点登录系统应该是耶鲁大学开源的<a href="https://github.com/Jasig/cas">CAS</a>, 可以基于<a href="https://github.com/apereo/cas/tree/master/cas-server-webapp">https://github.com/apereo/cas/tree/master/cas-server-webapp</a>来定制开发的。此外，国人开源的<a href="http://git.oschina.net/juapk/kisso">kisso</a>的这个也不错。基本上，单点登录的原理都类似下图所示：</p>

<p><img src="http://www.rowkey.me/images/blog_images/server_basic_stack/cas.jpg" alt="cas" /></p>

<h2><a name='统一配置中心'></a>统一配置中心</h2>

<p>在Java后端应用中，一种读写配置比较通用的方式就是将配置文件写在propeties、yaml、HCON文件中，修改的时候只需要更新文件重新部署即可，可以做到不牵扯代码层面改动的目的。统一配置中心，则是基于这种方式之上的统一对所有业务或者基础后端服务的相关配置文件进行管理的统一服务, 具有以下特性：</p>

<ul>
<li>能够在线动态修改配置文件并生效</li>
<li>配置文件可以区分环境(开发、测试、生产等)</li>
<li>使用方便: 在java中可以通过注解、xml配置的方式引入相关配置</li>
</ul>


<p><a href="https://github.com/knightliao/disconf">disconf</a>是可以在生产环境使用的一个方案，也可能根据自己的需求开发自己的配置中心(可以选择zookeeper作为配置存储)。</p>

<h2><a name='服务治理框架'></a>服务治理框架</h2>

<p>对于外部API调用或者客户端对后端api的访问，可以使用http协议或者说restful(当然也可以直接通过最原始的socket来调用)。但对于内部服务间的调用，一般都是通过RPC机制来调用的。目前主流的RPC协议有：</p>

<ul>
<li>RMI</li>
<li>Hessian</li>
<li>Thrift</li>
<li>Dubbo</li>
</ul>


<p>这些RPC协议各有优劣点，需要针对业务需求做出相应的最好的选择。</p>

<p>这样，当你的系统服务在逐渐增多，RPC调用链越来越复杂，很多情况下，需要不停的更新文档来维护这些调用关系。一个对这些服务进行管理的框架可以大大节省因此带来的繁琐的人力工作。</p>

<p>传统的ESB(企业服务总线)本质就是一个服务治理方案，但esb作为一种proxy的角色存在于client和server之间，所有请求都需要经过esb，使得esb很容易成为性能瓶颈。因此，基于传统的esb，更好的一种设计如下图所示：</p>

<p><img src="http://www.rowkey.me/images/blog_images/server_basic_stack/esb.png" alt="esb" /></p>

<p>如图，以配置中心为枢纽，调用关系只存在于client和提供服务的server之间，就避免了传统esb的性能瓶颈问题。对于这种设计，esb应该支持的特性如下：</p>

<ul>
<li>服务提供方的注册、管理</li>
<li>服务消费者的注册、管理</li>
<li>服务的版本管理、负载均衡、流量控制、服务降级等</li>
<li>服务的容错、熔断等</li>
</ul>


<p>阿里开源的<a href="https://github.com/alibaba/dubbo">dubbo</a>则对以上做了很好的实现，也是目前很多公司都在使用的方案。但由于某些原因，dubbo现已不再维护，推荐大家使用当当后来维护的<a href="https://github.com/dangdangdotcom/dubbox">dubbox</a>。</p>

<h2><a name='统一调度中心'></a>统一调度中心</h2>

<p>在很多业务中，定时调度是一个非常普遍的场景，比如定时去抓取数据、定时刷新订单的状态等。通常的做法就是针对各自的业务依赖Linux的cron机制或者java中的quartz。统一调度中心则是对所有的调度任务进行管理，这样能够统一对调度集群进行调优、扩展、任务管理等。<a href="https://github.com/azkaban/azkaban">azkaban</a>和<a href="https://github.com/yahoo/oozie">oozie</a>是hadoop的流式工作管理引擎，也可以作为统一调度中心来使用。当然，你也可以使用cron或者quartz来实现自己的统一调度中心。</p>

<ul>
<li>根据cron表达式调度任务</li>
<li>动态修改、停止、删除任务</li>
<li>支持任务工作流：比如一个任务完成之后再执行下一个任务</li>
<li>任务支持脚本、代码、url等多种形式</li>
<li>任务执行的日志记录、故障报警</li>
</ul>


<p>对于Java的quartz这里需要说明一下：这个quartz需要和spring quartz区分，后者是spring对quartz框架的简单实现也是目前使用的最多的一种调度方式。但是，其并没有做高可用集群的支持。而quartz虽然有集群的支持，但是配置起来非常复杂。现在很多方案都是使用zookeeper来实现spring quartz集群的。这里有一个国人开源的<a href="http://git.oschina.net/uncode/uncode-schedule">uncode-shcedule</a>对此实现的还不错，可以根据自己的业务需求做二次开发。</p>

<h2><a name='统一日志服务'></a>统一日志服务</h2>

<p>日志是开发过程必不可少的东西。有时候，打印日志的时机、技巧是很能体现出工程师编码水平的。毕竟，日志是线上服务能够定位、排查异常最为直接的信息。</p>

<p>通常的，将日志分散在各个业务中非常不方便对问题的管理和排查。统一日志服务则使用单独的日志服务器记录日志，各个业务通过统一的日志框架将日志输出到日志服务器上。</p>

<p>可以通过实现log4j后者logback的appender来实现统一日志框架，然后通过RPC调用将日志打印到日志服务器上。</p>

<h2><a name='数据基础设施'></a>数据基础设施</h2>

<p>数据是最近几年非常火的一个领域。从《精益数据分析》到《增长黑客》，都是在强调数据的非凡作用。很多公司也都在通过数据推动产品设计、市场运营、研发等。详细的可见之前的一篇<a href="http://www.rowkey.me/blog/2016/02/23/data-talk/">《数据杂谈》</a>，对数据相关的东西做过一些总结。这里需要说明的一点是，只有当你的数据规模真的到了单机无法处理的规模才应该上大数据相关技术，千万不要为了大数据而大数据。很多情况下使用单机程序+mysql就能解决的问题非得上hadoop即浪费时间又浪费人力。</p>

<p>这里需要补充一点的是，对于很多公司，尤其是离线业务并没有那么密集的公司，在很多情况下大数据集群的资源是被浪费的。因此诞生了<strong>xx on yarn</strong>一系列技术让非hadoop系的技术可以利用大数据集群的资源，能够大大提高资源的利用率，如Docker on yarn(Hulu的VoidBox)。</p>

<h3>数据高速公路</h3>

<p>接着上面讲的统一日志服务，其输出的日志最终是变成数据到数据高速公路上供后续的数据处理程序消费的。这中间的过程包括日志的收集、传输。</p>

<ul>
<li><p>收集：统一日志服务将日志打印在日志服务上之后，需要日志收集机制将其集中起来。目前，常见的日志收集方案有：scribe、Chukwa、Kakfa和Flume。对比如下图所示：</p>

<p>  <img src="http://www.rowkey.me/images/blog_images/server_basic_stack/data-collect.png" alt="dc" /></p></li>
<li><p>传输：通过消息队列将数据传输到数据处理服务中。对于日志来说，通常选择kafka这种消息队列即可。</p></li>
</ul>


<p>此外，这里还有一个关键的技术就是数据库和数据仓库间的数据同步问题，即将需要分析的数据从数据库中同步到诸如hive这种数据仓库时使用的方案。比较简单的、用的也比较多的可以使用<a href="http://hortonworks.com/apache/sqoop/">sqoop</a>进行基于时间戳的数据同步，此外，阿里开源的<a href="https://github.com/alibaba/canal">canal</a>实现了基于binlog增量同步，更加适合通用的同步场景，但是基于canal你还是需要做不少的业务开发工作的。推荐另一款国人开源的<a href="http://git.oschina.net/qiangzigege/MySQL-Binlog">MySQL-Binlog</a>，原理和canal类似，默认提供了任务的后台管理功能，只需要实现接收到binlog后的处理逻辑即可。</p>

<h3>离线数据分析</h3>

<p>离线数据分析是可以有延迟的，一般针对是非实时需求的数据分析工作，产生的也是T-1的报表。目前最常用的离线数据分析技术除了hadoop还有spark。相比hadoop，spark性能上有很大优势，当然对硬件资源要求也高。</p>

<p>对于hadoop，传统的MR编写很复杂，也不利于维护，可以选择使用hive来用sql替代编写mr，但是前提务必要对hive的原理做到了解。可以参见美团的这篇博文来学习:<a href="http://tech.meituan.com/hive-sql-to-mapreduce.html">Hive SQL的编译过程</a>。而对于spark，也有类似hive的spark sql。</p>

<p>此外，对于离线数据分析，还有一个很关键的就是数据倾斜问题。所谓数据倾斜指的是region数据分布不均，造成有的结点负载很低，而有些却负载很高，从而影响整体的性能。因此，处理好数据倾斜问题对于数据处理是很关键的。对于hive的数据倾斜，可见:<a href="http://www.cnblogs.com/ggjucheng/archive/2013/01/03/2842860.html">hive大数据倾斜总结</a>。对于spark的倾斜问题，可见：<a href="http://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&amp;mid=2651745207&amp;idx=1&amp;sn=3d70d59cede236eb1cb4f7374387a235&amp;scene=0#rd">Spark性能优化指南——高级篇</a>。</p>

<h3>实时数据分析</h3>

<p>相对于离线数据分析，实时数据分析也叫在线数据分析，针对的是对数据有实时要求的业务场景，如广告结算、订单结算等。目前，比较成熟的实时技术有storm和spark streaming。相比起storm，spark streaming其实本质上还是基于批量计算的。如果是对延迟很敏感的场景，还是应该使用storm。</p>

<p>对于实时数据分析，需要注意的就是实时数据处理结果写入存储的时候，要考虑并发的问题，虽然对于storm的bolt程序来说不会有并发的问题，但是写入的存储介质是会面临多任务同时读写的。通常采用的方案就是采用时间窗口的方式对数据做缓冲后批量写入。</p>

<p>此外，实时数据处理一般情况下都是基于增量处理的，相对于离线来说并非可靠的，一旦出现故障(如集群崩溃)或者数据处理失败，是很难对数据恢复或者修复异常数据的。因此结合离线+实时是目前最普遍采用的数据处理方案。<a href="http://www.csdn.net/article/2014-07-08/2820562-Lambda-Linkedln">Lambda架构</a>就是一个结合离线和实时数据处理的架构方案。</p>

<h3>数据即席分析</h3>

<p>离线和实时数据分析产生的一些报表是给数据分析师、产品经理参考使用的，但是很多情况下，线上的程序并不能满足这些需求方的需求。这时候就需要需求方自己对数据仓库进行查询统计。针对这些需求方，SQL上手容易、易描述等特点决定了其可能是一个最为合适的方式。因此提供一个SQL的即席查询工具能够大大提高数据分析师、产品经理的工作效率。Presto、Impala、Hive都是这种工具。如果想进一步提供给需求方更加直观的ui操作界面，可以搭建内部的<a href="https://github.com/cloudera/hue">Hue</a>。</p>

<p><img src="http://www.rowkey.me/images/blog_images/server_basic_stack/hue.jpg" alt="hue" /></p>

<h2><a name='故障监控'></a>故障监控</h2>

<p>对于面向用户的线上服务，发生故障是一件很严重的事情。因此，做好线上服务的故障检测告警是一件非常重要的事情。可以将故障监控分为以下两个层面的监控：</p>

<ul>
<li>系统监控：主要指的对主机的带宽、cpu、内存、硬盘、io等硬件资源的监控。这可以使用开源的nagios、cacti等开源软件进行监控。目前，市面上也有很多第三方服务能够提供对于主机资源的监控，如监控宝等。对于分布式服务集群(如hadoop、storm、kafka、flume等集群)的监控则可以使用<a href="http://ganglia.github.io/">ganglia</a>。此外，小米开源的<a href="https://github.com/open-falcon">OpenFalcon</a>也很不错，涵盖了系统监控、JVM监控等，也支持自定义的监控机制。</li>
<li>业务监控：是在主机资源层面以上的监控，比如app的pv、uv数据异常、交易失败等。需要业务中加入相关的监控代码，比如在异常抛出的地方，加一段日志记录。</li>
</ul>


<p>监控还有一个关键的步骤就是告警。告警的方式有很多种：邮件、im、短信等。考虑到故障的重要性不同、告警的合理性、便于定位问题等因素，有以下建议：</p>

<ul>
<li>告警日志要记录发生故障的机器id，尤其是在集群服务中，如果没有记录机器id，那么对于后续的问题定位会很困难。</li>
<li>要对告警做聚合，不要每一个故障都单独进行告警，这样会对工程师造成极大的困扰。</li>
<li>要对告警做等级划分，不能对所有告警都做同样的优先级处理。</li>
<li>使用微信做为告警软件，能够在节省短信成本的情况下，保证告警的到达率。</li>
</ul>


<p>故障告警之后，那么最最关键的就是应对了。对于创业公司来说，24小时待命是必备的素质，当遇到告警的时候，需要尽快对故障做出反应，找到问题所在，并能在可控时间内解决问题。对于故障问题的排查，基本上都是依赖于日志的。只要日志打的合理，一般情况下是能够很快定位到问题所在的，但是如果是分布式服务，并且日志数据量特别大的情况下，如何定位日志就成为了难题。这里有几个方案：</p>

<ul>
<li>建立ELK(Elastic+Logstash+Kibana)日志集中分析平台，便于快速搜索、定位日志。对于ELK的介绍，可以见：<a href="https://xiequan.info/%E4%BD%BF%E7%94%A8elasticsearch-logstash-kibana%E6%90%AD%E5%BB%BA%E6%97%A5%E5%BF%97%E9%9B%86%E4%B8%AD%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0%E5%AE%9E%E8%B7%B5/">使用Elasticsearch + Logstash + Kibana搭建日志集中分析平台实践</a></li>
<li>建立分布式请求追踪系统(也可以叫全链路监测系统)，对于分布式系统尤其是微服务架构，能够极大的方便在海量调用中快速定位并收集单个异常请求信息，也能快速定位一条请求链路的性能瓶颈。Google的<a href="http://www.cnblogs.com/LBSer/p/3390852.html?spm=5176.100239.blogcont58408.6.xuC3MP">Dapper</a>、唯品会的<a href="http://mp.weixin.qq.com/s?__biz=MzAwMDU1MTE1OQ==&amp;mid=2653547643&amp;idx=1&amp;sn=c06dc9b0f59e8ae3d2f9feb734da4459&amp;scene=1&amp;srcid=0808MaLgymxNlsh4Z31oWKUi#rd">Mercury</a>、阿里的<a href="https://bigbully.github.io/Dapper-translation/?spm=5176.100239.blogcont58408.7.xuC3MP">鹰眼</a>、新浪的<a href="http://ishare.iask.sina.com.cn/f/68869649.html">WatchMan</a>都是类似的思路。此外，<a href="https://www.zhihu.com/question/20292868">腾讯的染色日志机制</a>本质上也是在链路追踪之上根据响应信息做了染色机制。</li>
</ul>


<p><strong><em>以上是本人实践的一些经验。由于知识有限，难免有纰漏，敬请指出。</em></strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[研发体系这点事]]></title>
    <link href="http://www.rowkey.me/blog/2016/08/17/dev-manage/"/>
    <updated>2016-08-17T20:13:34+08:00</updated>
    <id>http://www.rowkey.me/blog/2016/08/17/dev-manage</id>
    <content type="html"><![CDATA[<p><strong><em>&ndash;本文于2016.08.23最新更新&ndash;</em></strong></p>

<p>早在读研究生的时候，自己负责着实验室的项目，就一直在思索如何建立一套简单又高效的研发管理体系，能够在保证项目高质量顺利进行的同时还能够提升团队成员的技术level。后来在自己在校的几次小的创业中，也做过一些尝试。直到毕业后进入前东家，在几个项目的参与过程中，见到了大公司的研发管理是如何进行的。直至加入目前的公司，将研发体系梳理一遍，且学且抄且实践，对这一套东西算是有了一定的实践感悟。</p>

<p>对于一个研发管理体系，其核心是围绕着产品的整个生命周期来进行的。因此，根据一个产品的生命周期，可以把研发体系划分为几个关键的环节，如图所示：</p>

<p><a href="http://www.rowkey.me/images/blog_images/dev-system-overview.png" target="_blank"><img src="http://www.rowkey.me/images/blog_images/dev-system-overview.png"/></a></p>

<p>更为具体的一个研发流程则如下图所示，标注了每一个环节的参与角色。</p>

<p><img src="http://www.rowkey.me/images/blog_images/prject_manage_detail.png" alt="prject_manage_detail.png" /></p>

<p>可知，即时沟通和技术提升虽然不属于研发流程中的某一个环节，但它们是贯穿整个研发体系不可或缺的一部分，有着不可替代的作用。此外，任务管理需要对任务做整个研发生命周期的管理，除了作为其中的一个关键环节，也是贯穿整个研发流程的。</p>

<ul>
<li><a href="#%E4%BB%BB%E5%8A%A1%E7%AE%A1%E7%90%86">任务管理</a></li>
<li><a href="#%E6%96%87%E6%A1%A3%E5%8D%8F%E4%BD%9C">文档协作</a></li>
<li><a href="#%E4%BB%A3%E7%A0%81%E5%8D%8F%E4%BD%9C">代码协作</a></li>
<li><a href="#%E8%B4%A8%E9%87%8F%E4%BF%9D%E8%AF%81">质量保证</a></li>
<li><a href="#%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2">自动化部署</a></li>
<li><a href="#%E6%95%85%E9%9A%9C%E7%AE%A1%E7%90%86">故障管理</a></li>
<li><a href="#%E5%8D%B3%E6%97%B6%E6%B2%9F%E9%80%9A">即时沟通</a></li>
<li><a href="#%E6%8A%80%E6%9C%AF%E6%8F%90%E5%8D%87">技术提升</a></li>
</ul>


<!--more-->


<h2><a name='任务管理'></a>任务管理</h2>

<p>任务管理是产品整个生命周期首要的环节，其对研发体系也是至关重要的。项目生命周期模型，传统的有五种：瀑布模型、原型模型、螺旋模型、增量模型、V模型，而现在最为流行的是迭代开发模型，敏捷开发则是采用迭代模型的一种典型项目管理方法集合。Scrum是目前敏捷开发中最为大家熟知的开发模式(XP极限编程也是一种比较常见的敏捷开发模式)，其开发流程的概览如下图所示：</p>

<p><img src="http://www.rowkey.me/images/blog_images/ScrumModel.jpg" style="width:500px"/></p>

<p>简单来说，Scrum是依赖于三种角色、四种会议的自组织、信息透明化、成员平等的一种敏捷开发流程。更为详细的描述，可参见此篇文章：<a href="http://blog.devtang.com/2014/09/13/scrum-introduction/">http://blog.devtang.com/2014/09/13/scrum-introduction/</a>。</p>

<p>除了Scrum之外，看板是最近兴起的另一种开发模式，在最近很火的美剧《硅谷》里面“魔笛手”就是采用的这种方式。看板将工作流程形象化，首先把工作细分成任务并根据需要将任务分为Pending、Analysis、Development、Test、Deploy等状态，然后根据任务的进行，在几种状态之间进行转换。对比Scrum，看板使用开发周期作为计划和过程改进的度量数据，不强调迭代的概念，也没有很强的时间期间概念，也不需要制定任何团队角色。对于看板方法论的详细介绍可见此篇文章：<a href="http://kanbanblog.com/explained/">http://kanbanblog.com/explained/</a>，<a href="http://www.jianshu.com/p/e44b1038c9cf">http://www.jianshu.com/p/e44b1038c9cf</a>这篇则做了比较形象具体的说明。这里有一点需要注意，Scrum和看板并非是对立的，它们是可以结合起来使用的。使用看板来管理每一次迭代的任务是一种可取也是很常见的精益实践。</p>

<p>依赖于任务管理方法论，市面上很多软件都做了相应的支撑，自己曾经使用过的任务管理软件如下：</p>

<ul>
<li><a href="http://www.redmine.org/">Redmine</a>: 这个是自己最开始接触的任务管理软件，使用也比较广泛。比较遗憾的是，redmine安装有点繁琐，而且基于ROR，如果需要二次开发，需要重新学习ROR。</li>
<li><a href="http://tower.im">Tower.im</a>: 这是一个任务管理云服务，界面设计的简单优雅，一目了然。很多小的私有项目，我都会用这个进行任务管理。类似的还有teambeation等。</li>
<li><a href="http://www.atlassian.com/software/jira/">Jira</a>: 这款软件是商业版的任务管理软件，对于这一块做的是非常专业的，很多大公司都在使用。但是，它是收费的。所以，如果你要用，要么付钱，要么去破解。。。</li>
<li><a href="http://www.zentao.net/">禅道</a>：这款软件最早是叫做bugfree, 是开源且主要针对Bug管理的，后面慢慢发展成现在的集任务管理、bug管理、团队管理等的项目管理软件，并开启了收费策略。总体来说，功能很全，也比较专业，但是ui上有种传统it系统的感觉，流程上也不具有现在敏捷开发的一些优势。</li>
<li><a href="https://github.com/kanboard/kanboard">Kanboard</a>: 是实现了Kanban方法论的任务管理软件。</li>
</ul>


<p>对于个人的项目，其实依赖于tower.im这种第三方云服务完全足够了。如果担心数据安全的话，那么推荐在内网搭建Kanboard进行看板任务管理。</p>

<h2><a name='文档协作'></a>文档协作</h2>

<p>研发中首当其冲的就是文档撰写，这个很多情况下都决定了项目的可维护、可管理性。有人会说现在流行的是敏捷开发，根本不需要写文档，但其实这是对敏捷的误解。敏捷开发强调的是快速试错、快速迭代，而非简单粗暴，<strong><em>对比传统开发模型虽然并不强调文档，但并不代表不需要</em></strong>。对于一个项目，从开始就需要需求文档、产品原型文档、项目进度文档等等，而到了研发这一步，在系统实现、写代码之前最好的就是先“想”再做，而“想”的一种比较好的输出形式就是文档。对于一个软件系统，一般来说需要写的文档有以下几种：</p>

<ul>
<li>系统业务流程文档：描述系统业务逻辑的文档，能清晰的说明真个业务的流程。</li>
<li>系统架构设计文档：对整个系统的架构的描述，需要包含系统的各个关键组成模块以及相关的各个关键技术点等。</li>
<li>系统功能模块概要设计/详细设计文档：对于某一个模块的流程、逻辑的描述。</li>
<li>数据DDL/DML文档: 与系统相关的数据库的DDL和DML文档，对于前者，是需要包含所有的操作的，而对于后者，必不可少的是查询语句，用来提供给DBA，来做查询sql的review，以保证索引的正确建立和查询语句的合理等。</li>
<li>系统部署文档：描述系统关键部分部署在哪里，需要做哪些配置。</li>
<li>系统发布ChangeLog：对系统每次发布的改动进行描述，包括数据库、缓存、数据队列、新增/变动了哪些依赖服务等。此外，对数据库、缓存这种关键服务的量化分析也可以写在这里。</li>
</ul>


<p>尤其对于一些相对复杂的功能来说，整理思路形成文档，不仅可以让自己逻辑清晰，也让后续维护的人能够更快地接手。当然，这些并不是死板要求的，应该根据实际的业务选择，不一定所有的文档都是必须的，也不一定要分开这几个文档写(可以将这些内容集成在一个文档中，这也是目前我经常采用的方式)。这些文档的范例可以见：<a href="https://github.com/superhj1987/awesome-tech-collections/tree/master/document">https://github.com/superhj1987/awesome-tech-collections/tree/master/document</a>。</p>

<p>而对于文档撰写协作的方式，我自己经历过的有以下几种：</p>

<ul>
<li>使用word撰写各种文档，提交到svn等版本管理工具上</li>
<li>使用google doc进行协作</li>
<li>使用word撰写文档，然后提交到项目管理软件中进行管理</li>
<li>使用markdown撰写文档，提交到版本管理工具上</li>
</ul>


<p>我自己比较推崇的是使用markdown撰写文档，然后使用git、svn版本管理工具或者是其他团队协作工具做版本管理。之所以使用markdown, 能够极大地节省使用word时调各种格式、样式耗费的时间。对于程序员来说真的是如虎添翼。如果是对文档多人协同编辑有刚需的团队，可以选择使用google doc或者国内的石墨(<a href="http://shimo.im">http://shimo.im</a>)。</p>

<p>此外，在移动app开发中，还有一个非常关键的文档就是<strong>api文档</strong>，是服务端提供给客户端调用接口的说明文档。比较简单直接的方法就是定制一套api文档模板，然后在写接口代码之前或者之后，按照模板编写接口文档。此外，可以实现一套根据源码自动生成文档的机制，在代码编写的同时就能自动生成相应的接口说明文档。在使用Spring MVC开发的后端应用中，个人推荐<a href="https://github.com/springfox/springfox">SpringFox</a>，使用此项目能够通过在Controller中加入相应的注解信息从而自动生成Api接口文档，同时也提供了在线调试的功能，极大减少了api文档的工作量。</p>

<h2><a name='代码协作'></a>代码协作</h2>

<p>对于一个技术团队，最最关键的肯定是写代码。一个人单打独斗那倒好说，但是这就像篮球场上，一对一靠个人硬实力，但是5对5，那就不仅仅是一个人实力强就赢得了的了。因此对于技术团队来说，代码协作是至关重要的一个部分。</p>

<ul>
<li><p>代码版本管理：Git + SVN</p>

<p>  几年前最流行的代码版本管理工具是svn（当然此前，更加古老的还有cvs之流），的确为程序员们的代码管理带来了很多便捷。但到了现在，相比起这种集中式代码管理，目前最为火热的当属git这种分布式代码管理工具，在Linux上直接搭建git服务器来构建项目的git系统的。而这几年随着Github以及类似系统的涌现，对于很多私人项目我都是采用oschina或者gitcafe提供的git私有代码管理来做代码版本管理的。当然，对于公司来说，有很多开源类github系统可以搭建在企业内网。详细的可以参见：<a href="http://www.rowkey.me/blog/2015/11/13/your-own-github/">搭建自己的github</a>。当然，对比svn，git也是有缺点的。无法天然的支持对于目录级别的权限管理和基于目录的版本管理操作是目前不得不结合svn和git一起使用的重要原因。通常情况下，可以使用git做版本管理，辅以svn做基于目录级别的发布包管理。</p></li>
<li><p>代码分支/Tag管理： Git Flow</p>

<p>  其实分支/Tag管理是代码版本管理包含的内容，之所以单独出来，是因为对于分支的使用其实还是有一定的原则和技巧的。并非如很多人一样，所有项目就一个master分支，所有修改都往这里塞。目前，最为流行的一种基于分支的工作方式就是:Git flow。介绍可以见: <a href="http://www.ituring.com.cn/article/56870">基于git的源代码管理模型——git flow</a>。简单概括就是：</p>

<ul>
<li>master和develop作为主分支。主分支是所有开发活动的核心分支。所有的开发活动产生的输出物最终都会反映到主分支的代码中。master是可以随时发布的分支，而develop则时刻保持最新的开发代码。</li>
<li>辅助分支是用于组织解决特定问题的各种软件开发活动的分支。辅助分支主要用于组织软件新功能的并行开发、简化新功能开发代码的跟踪、辅助完成版本发布工作以及对生产代码的缺陷进行紧急修复工作。这些分支与主分支不同，通常只会在有限的时间范围内存在。包括：

<ul>
<li>用于开发新功能时所使用的feature分支；</li>
<li>用于辅助版本发布的release分支；</li>
<li>用于修正生产代码中的缺陷的hotfix分支。
对于此种开发模型，这里也提供了一个命令行工具：<a href="https://github.com/nvie/gitflow">https://github.com/nvie/gitflow</a></li>
</ul>
</li>
</ul>
</li>
<li><p>代码质量保证：结对编程 + 定期review + PR目前一种比较好的方式。结对编程这个是一个老生常谈的方式，两个人共同承担某一开发任务，互相保证对方的代码质量，在很大程度上能够提高代码质量。而定期review则是让团队所有的成员都能够参与到这个过程中，不仅仅能够保证被review者的代码质量，也能够让团队成员学习到好的代码是怎样的而差的代码又是怎样的。PR是Pull Request的简写，当开发完成的代码提交到主分支时，需要发起pull request，此时团队负责人需要review相关代码，确保没有问题之后，才能accept此次pr。当然，上面讲述的是如何通过人来保证代码质量。除此之外，还可以通过技术上的手段在一定程度上保障代码的质量，这一部分在后续的自动化测试机制会讲述。</p></li>
</ul>


<p>此外，在移动app项目中，一个很普遍的问题就是：<strong>在定义好Api文档之后，客户端如何在后端并没有完成接口开发的情况下开发或者调试程序？</strong>这里有两种方案：</p>

<ul>
<li>客户端做好接口封装，在后端接口未完成前，客户端不经过网络io直接返回静态格式数据。这种方式最好是由客户端定义接口格式数据。</li>
<li>后端将示例接口返回数据写在文件里，接口直接返回静态文件数据。此种方式，由后端定义接口数据格式。另外，有一个开源的工具: <a href="https://github.com/Runscope/httpbin">httpbin</a>可以用来提供接口返回指定格式的数据，中文介绍可见: <a href="https://blog.phpgao.com/how-to-httpbin.html">https://blog.phpgao.com/how-to-httpbin.html</a>。</li>
</ul>


<p>关于客户端和后端的接口代码协作，还有一个Chrome插件<a href="https://www.getpostman.com/">POSTMAN</a>可以使用。后端可以使用此插件在编写完接口后进行自我功能测试，测试无误后可以将接口以文件或者url的形式分享给客户端供客户端参考和调试。</p>

<h2><a name='质量保证'></a>质量保证</h2>

<p>当代码开发完成之后，需要质量保证机制的介入来保证功能的正常运行，从而保证代码是可发布的。一般来说，质量保证的手段就是测试，分为：</p>

<ul>
<li>代码质量测试</li>
<li>功能测试</li>
<li>性能测试</li>
</ul>


<p>代码质量测试一般是在编译打包代码之前进行，通常是自动化进行的。针对Java项目，自动化代码质量测试可以分为以下几步：</p>

<ul>
<li>源代码规范检查：对于Java来说，代码规范的检查一般使用checkstyle来检查。默认的规范非常严格，这里大家可以根据需要放宽一些规范。</li>
<li>源代码静态质量检查: 常用的工具是pmd, 可以检查Java源文件中的潜在问题, 比如空try/catch/finally/switch语句块等。</li>
<li>字节码bug检查：常用工具是findbugs,基于Bug Patterns概念，查找javabytecode（.class文件）中的潜在bug。如NullPoint空指针检查、没有合理关闭资源、字符串相同判断错（==，而不是equals）。</li>
<li>单元测试：使用junit即可，当然在这里当使用mvn时，其test phrase会默认生成测试报告到${project.build.directory}/surefile-reports文件夹中。这里建议使用coverage生成单元测试报告，其中一个关键的单元测试覆盖率指标达到98%以上才为合格(根据需要自己调整即可)。</li>
</ul>


<p>以上提到的工具，都是有maven插件的。通常情况下，也推荐使用这些工具的maven插件来调用。目前流行的自动化ci工具jenkins、QuickBuild等结合各种丰富的插件可以提供这些功能，将他们集成到一个测试流程并形成最终的测试结果报表。</p>

<p>在代码发布到线上环境之前，一个关键的步骤就是功能测试，通常都是工程师来进行的。需要测试工程师根据产品需求，形成测试用例，然后根据这些用例做相应的测试。测试用例的一个模板如下：</p>

<table>
<thead>
<tr>
<th>用例ID </th>
<th> 功能名称 </th>
<th> 用例名称 </th>
<th> 测试数据 </th>
<th> 前置条件 </th>
<th> 操作步骤 </th>
<th> 预期结果 </th>
<th> 测试结果 </th>
<th> 备注 </th>
<th> review说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>- </td>
<td> - </td>
<td> - </td>
<td> - </td>
<td> - </td>
<td> - </td>
<td> - </td>
<td> - </td>
<td> - </td>
<td> -</td>
</tr>
</tbody>
</table>


<p>需要测试工程师根据需求创建并经过研发人员reivew确定测试用例，待到发布前进行测试以及反馈，直到所有测试用例都通过。</p>

<p>对于移动app功能的测试，目前市场上有类似<a href="https://www.bugtags.com/">bugtags</a>这种所见即所得提交测试工具，可以很方便的提交bug。</p>

<p>功能测试通过之后，对于一些对性能有要求的项目，还需要进行性能测试。对于这种测试来说，通常有以下几种方式：</p>

<ul>
<li>测试工程师写性能测试代码来进行测试</li>
<li>使用性能测试工具测试，如LoadRunner、ab等</li>
</ul>


<p>当然，所有这些测试都是在项目发布上线之前进行的，通常是在项目的测试、预发布环境中进行的。</p>

<p>此外，对于测试任务的管理工作一般在任务管理软件中都做了集成。也有类似Mantis这种事专门做缺陷管理的。</p>

<h2><a name='自动化部署'></a>自动化部署</h2>

<p>对于Java项目的发布流程，如下图所示：</p>

<p><img src="http://www.rowkey.me/images/blog_images/deploy_process.jpg" alt="deploy" /></p>

<p>使用ci软件可将以上步骤自动化的。</p>

<p>如上图所示，对于一个项目，我们是划分为三种或者四种环境的。</p>

<ul>
<li>测试环境: 这个环境是一个相对来说比较宽松的环境，所有代码的提交都会触发jenkins的自动代码质量检查和部署。测试工程师也是首先在这个环境下进行功能、性能测试的。只有通过了，才能部署到后续的下一个环境。</li>
<li><strong><em>集成环境</em></strong>：这个环境不是必须的，只有当项目出现了两个大的分支并行开发，发布前需要集成两部分代码时才需要这样一个环境。一般来说只使用jenkins进行部署前的打包流程，部署流程由相关人员进行。这个环境也是需要测试工程师进行测试的。</li>
<li>预发布环境：这个环境和线上环境是一模一样的，不同的是此环境下的服务器是不在线上服务器集群中的，并不为用户提供服务。此环境下的项目发布也是需要人工参与的，也必须由测试保证功能和性能的正常。</li>
<li>线上环境：这个环境是比较严格的一个环境。在发布前，一般来说会进行发布确认等一系列上线评审工作后，由项目负责人或者运维人员部署发布。

<ul>
<li>功能列表 vs 实现情况：检查是否已经实现所有计划的功能？如果有某些功能没有实现需要说明原因。</li>
<li>软件演示</li>
<li>测试结果和遗留问题列表：测试用例的情况，遗留的Bug以及情况说明</li>
<li>上线确认</li>
<li>后续任务计划</li>
</ul>
</li>
</ul>


<p>其中，<strong>上线确认书</strong>的一个例子如下：</p>

<table>
<thead>
<tr>
<th>&ndash; </th>
<th> xx项目上线确认书 </th>
<th> &ndash;</th>
</tr>
</thead>
<tbody>
<tr>
<td>需求方验证结果 </td>
<td> 意见： </td>
<td> 确认人：[由各个负责人签字]</td>
</tr>
<tr>
<td>开发确认 </td>
<td> 意见： </td>
<td> 确认人：</td>
</tr>
<tr>
<td>测试确认 </td>
<td> 意见： </td>
<td> 确认人：</td>
</tr>
<tr>
<td>服务器是否需要重启 </td>
<td> [是否需要自动更新那些App?] </td>
<td> 确认人：</td>
</tr>
<tr>
<td>服务器配置影响 </td>
<td> [是否需要增加新的服务器ip,是否需要修改nginx/tomcat，是否新装软件，是否新建域名？] </td>
<td> 确认人：</td>
</tr>
<tr>
<td>数据库更改 </td>
<td> [是否需要修改线上数据库？是否有初始化语句？索引是否正确建立？查询语句是否合理？量化分析数据(包括缓存)是否无误？] </td>
<td> 确认人：</td>
</tr>
<tr>
<td>数据初始化 </td>
<td> [是否有初始化数据？如价格，默认分类等] </td>
<td> 确认人：</td>
</tr>
<tr>
<td>上线评审结论 </td>
<td> [ ]通过 <br/> [ ] 未通过，不能上线 <br/>  [ ] 未通过，但修改完制定Bug后可直接上线</td>
<td> 确认人：</td>
</tr>
<tr>
<td>计划上线时间 </td>
<td> 2016-08-01</td>
<td></td>
</tr>
</tbody>
</table>


<p><strong>后续任务计划</strong>，示例如下：</p>

<table>
<thead>
<tr>
<th>问题描述 </th>
<th> 责任人 </th>
<th> 计划完成时间 </th>
<th> 状态</th>
</tr>
</thead>
<tbody>
<tr>
<td>xx </td>
<td> xx </td>
<td> xx    </td>
<td> xx</td>
</tr>
</tbody>
</table>


<h2><a name='故障管理'></a>故障管理</h2>

<p>由于各种客观原因如带宽、主机配置、流量异常或者程序逻辑不够严谨等原因，线上服务并非100%可用的。研发体系中最后把关的就是这一道故障应急机制。也就是说，一旦发生线上故障，如何快速反应并修复问题，如何避免下一次犯同样的错误。</p>

<p>对故障的快速反应需要依赖于运维的监控机制，包括基础设施层面的监控以及业务层面的监控，一旦发生故障应该立刻发出告警到相关人员。这里可以使用nagios、cacti或者第三方服务(如:<a href="http://www.jiankongbao.com/">监控宝</a>)实现，当然，如果你使用的是云服务，一般也会有相应的云监控服务提供给你的。后续的故障问题定位很多情况下则是取决于你的应用日志打的是否合理，是否有足够的覆盖面的，是否有足够的信息。<a href="https://xiequan.info/%E4%BD%BF%E7%94%A8elasticsearch-logstash-kibana%E6%90%AD%E5%BB%BA%E6%97%A5%E5%BF%97%E9%9B%86%E4%B8%AD%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0%E5%AE%9E%E8%B7%B5/">ELK</a>和请求链路监测系统(同染色日志系统)是目前比较流行的基于日志的故障定位解决方案。问题一旦定位到了，那么修复就是水到渠成的事情了。</p>

<p>这里需要说明的一点是，上面讲述的是后端的故障快速反应和修复。针对客户端的故障，一般情况下都是由用户发现的。但是由于客户端发布流程的繁琐，很难及时修复一次发布版本的故障，只能等到下次解决。但是，目前一些客户端使用混合开发，其中的h5页面是可以在线修复的，另外，很多安卓app热更新方案也都能在线修复一些代码故障，如<a href="https://github.com/jasonross/Nuwa">Nuwa</a>、<a href="https://github.com/dodola/HotFix">HotFix</a>、<a href="https://github.com/alibaba/dexposed">dexposed</a>。</p>

<p>故障解决完并非最终的结果，之后的故障总结也是故障管理尤为关键的一点。大公司会根据故障产生的影响不同定义不同的故障级别，从而追责到个人，再进一步影响个人的职级评定或者绩效考核、奖金之类的。但这一套却并不适用于小公司，毕竟大多数小公司没有那么完善或者说根本就没有职级和绩效这么一说。其实，追责并不是主要目的，最主要的是如何避免再次出现问题。因此，对于小的创业公司来说，最需要做的就是如何对已经发生的故障做总结，吸取教训。构建一套故障总结wiki则是一种很好的方式。下面是一次故障总结模板；</p>

<table>
<thead>
<tr>
<th>&ndash; </th>
<th> 2016.08.01xxx故障总结 </th>
<th> &ndash;</th>
</tr>
</thead>
<tbody>
<tr>
<td>故障等级 </td>
<td> [故障等级]</td>
<td></td>
</tr>
<tr>
<td>故障描述 </td>
<td> [描述故障发生的现象]</td>
<td></td>
</tr>
<tr>
<td>故障发现时间及发现人 </td>
<td> [xxx于xxxx年xx月xx日 HH:mm 如何发现该问题。]</td>
<td></td>
</tr>
<tr>
<td>故障影响 </td>
<td> [影响时间范围、影响版本范围、影响产品范围]</td>
<td></td>
</tr>
<tr>
<td>故障原因 </td>
<td> [阐述故障发生的原因]</td>
<td></td>
</tr>
<tr>
<td>解决方案 </td>
<td> [详细记录如何解决此次的故障]</td>
<td></td>
</tr>
<tr>
<td>故障教训 </td>
<td> [如何避免下次出现类似的事故]</td>
<td></td>
</tr>
<tr>
<td>责任人   </td>
<td> [责任人签名]</td>
<td></td>
</tr>
</tbody>
</table>


<h2><a name='即时沟通'></a>即时沟通</h2>

<p>显而易见，即时沟通是任何团队都必不可少的一个机制，同样也是研发团队必不可缺的。常用的就是QQ、钉钉或者企业内部的im软件。那么对于小公司或者创业公司，不想用第三方服务的该怎么办呢？之前蘑菇街开源过一个teamtalk的软件，不过后来由于某些原因已经下线。目前，有一款开源的web im软件可以供大家选择：<a href="https://github.com/RocketChat/Rocket.Chat">Rocket.Chat</a>，能够搭建出内网的slack服务(将分散的沟通方式聚集到一个地方，融入到一个信息流中)。</p>

<p>此外，我自己还尝试过使用intellij自带的IDE TALK来进行研发团队的在线交流。使用这个比较好的一点是可以直接做基于代码的即时交流，比如能够发送一个代码片段给同事，他那边接收到之后是直接能在他的项目里相关代码处进行操作的。</p>

<h2><a name='技术提升'></a>技术提升</h2>

<p>一个研发团队，很重要的一点是如何提高团队的战斗力。对于个人来说，在平时的工作中，提高技术的熟练度和深度，在业余补充学习专业知识，提升技术广度，这些都无须多言。那么如何在整体层面或者说是管理上促进团队成员的技术提升呢？可以采取的方式有以下几种：</p>

<ul>
<li>构建内部的技术wiki并建立技术分享机制，鼓励大家以演讲或者技术博客的方式分享自己的技术经验或者教训，既可以对自己进行review又可以给其他成员以启示。这一点，很多公司都是纳入绩效中的。</li>
<li>将一些项目开源，让团队成员能够享受到开源项目带来的各种好处，比如提升个人在业界的知名度、提高编码的水准(毕竟不好的代码，你也不好意思放出去)。</li>
<li>定期举办类似黑客马拉松的比赛，提高团队成员的凝聚力，也能够提升成员解决实际问题的技术能力。</li>
</ul>


<p>自己比较推崇的是第一种方式，但是开始的时候往往会发现很多人是不会主动去分享的。要么是觉得自己的东西技术含量都很低，要么就觉得自己的知识为何要分享给别人。可以采取的办法就是从最初的周期性安排人员进行技术分享，然后慢慢形成一种氛围和习惯，再到后续鼓励大家主动分享。当然，辅以奖品激励或者绩效奖励也是一种不错的方式，但切忌不要忽视一些业务能力很强但不爱或者不善于分享的工程师。</p>

<p>至于项目开源，前提一定是团队的项目真的是高质量并且会对开源社区有贡献的，不能为了开源而开源。尤其是对于一个公司来说，一个开源的项目直接体现了公司技术水准的高低，会对公司的pr、招聘等都带来一定程度的影响。</p>

<p>而黑客马拉松比赛这种方式，尤为关键的一点是要选择合适的主题。一般来说，围绕现实的业务场景来出题不仅能够提升大家解决问题的能力，也能顺便解决实际问题。比如&#8221;根据用户已有行为日志预测用户未来的行为&#8221;、“怎样构建合适的用户质量模型”都是比较合适的主题。此外，借鉴黑客马拉松的这种形式，可以采取类似“每周一题”的做法：在每周例会上给出一道和线上业务相关的问题，如“如何提高信息流的点击转化率”，然后每个人发散思维给出自己的解决方案，最终形成文章发布在内部的技术wiki上。对于每次主题，都会在下周的例会上针对每个人的解决方案进行讨论。</p>

<p>此外，在安排团队成员去调研一种将要使用的新技术的时候，务必要深入到源码层面，这个观念是需要灌输到团队每一个人的意识中去的。去使用一个没有看过源码或者没有掌握其运行原理的开源软件是一件风险非常大的事情，极有可能造成巨大的线上故障。</p>

<p><strong>以上，是自己对于研发体系的一些实践和感悟，很多地方仍然有所欠缺或者并非最佳实践，也一直在探索更好的方案。</strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[译]Java8 Top Tips]]></title>
    <link href="http://www.rowkey.me/blog/2016/08/03/java-8-top-tips/"/>
    <updated>2016-08-03T22:30:34+08:00</updated>
    <id>http://www.rowkey.me/blog/2016/08/03/java-8-top-tips</id>
    <content type="html"><![CDATA[<p>原文：<a href="https://dzone.com/articles/java-8-top-tips">https://dzone.com/articles/java-8-top-tips</a></p>

<p>本文包含了对于Java8的一些最佳实践，包括Stream和Lambda表达式的一些基础。</p>

<p>笔者已经使用Java8工作许多年，包括新的应用开发以及迁移旧的应用，感觉是时候总结Java8中一些有用东西的最佳实践。笔者个人不太喜欢“最佳实践”这个词，因为字面上传达了一种“one size fit all”的概念，当然，编码肯定不是这样的而是不同的场景有不同的解决方案。但是笔者觉得在如何使用Java 8让自己的生活变得更加容易上还是有一些特殊的经验值得分享的。</p>

<!--more-->


<h2>Optional</h2>

<p>Optional是一个评价过低的特性，它可以显著的降低代码抛出NullPointerException的可能。它在边界代码(你正在使用的API或者你发布的API)中特别有用。</p>

<p>但是对于它的不适当的使用和设计很容易使一个小的变动影响到很多的类，或者降低代码的可阅读性。这里有一些如何更加高效使用Optional的建议。</p>

<h3>Optional应该仅仅用在返回类型中</h3>

<p>不要用在参数或者域中。<a href="http://blog.joda.org/2015/08/java-se-8-optional-pragmatic-approach.html">阅读这篇博文</a>可以看到如何正确使用Optional进行编码。幸运的是，IntelliJ IDEA可以打开inspections去检查你是否遵循了这些推荐规范。</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/OptionalParamWarning.png" alt="OptionalParamWarning.png" /></p>

<p>要尽早在Optional出现的地方对它进行处理。IntelliJ IDEA会阻止Optional出现在你代码的各个地方，所以记住一定要在Optional出现的地方就对他进行处理。</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/OptionalUseImmediately.png" alt="OptionalUseImmediately.png" /></p>

<h3>不能简单地调用get()方法</h3>

<p>Optional是用来表示这个值是有可能为空的，让你做好应对的准备。因此，很重要的一点就是在使用这个值之前务必要检查其是否存在。简单地调用get方法而不是先调用isPresent可能会导致产生空指针异常。幸运的是，IntelliJ IDEA再一次提供了对此种方案的检查。</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/OptionalGetWithoutIsPresent.png" alt="OptionalGetWithoutIsPresent.png" /></p>

<h3>更加优雅的方案</h3>

<p>如下代码，isPresent和get当然能够解决这个问题。</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/OptionalSimple.png" alt="OptionalSimple.png" /></p>

<p>但是这里有更加优雅的方式，你可以使用orElse来设置一个默认值。</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/OptionalOrElse.png" alt="OptionalOrElse.png" /></p>

<p>或者你可以使用orElseGet来设置当值为null的时候去调用的方法。虽然看着和前面的方案没有什么大的不同。但是提供的方法应该仅仅在需要调用的时候才被调用。那么当这是个代价昂贵的方法时，那么使用lambda会带来更好的性能提升。</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/OptionalOrElseGet.png" alt="OptionalOrElseGet.png" /></p>

<h2>使用Lambda表达式</h2>

<p>Lambda表达式是Java8最主要的卖点。即使你现在用不到Java8，你也应该对它有了一些基本的了解。下面讲述了一种新的方式使用Java编程，虽然这并不是一个“最佳实践”，仅仅是一个使用的指导。</p>

<h3>保持简短</h3>

<p>函数式编程对于长的lambda表达式是欢迎的，但是对于仅仅使用Java开发很多年的人发现编写短的lambda表达式会更容易一些。你甚至会想把表达式缩减到一行，也很容易把长的表达式重构成一个方法。</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/LambdaExtractMethod.png" alt="LambdaExtractMethod.png" /></p>

<p>当然，这些甚至牵扯到了方法引用(Method References)。方法引用可能看着有点陌生，但是由于其能让代码达到更好的阅读性，还是有很大应用价值的。后面，我会讲到这个概念。</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/LambdaCollapseToMethodRef.png" alt="LambdaCollapseToMethodRef.png" /></p>

<h3>显式声明</h3>

<p>在lambda表达式中是没有类型信息的，所以你会发现在参数中包含类型信息是非常有用的。</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/LambdaExplicitParamTypes.png" alt="LambdaExplicitParamTypes.png" /></p>

<p>如你所见，这会变得很笨重。所以我更喜欢赋予参数有意义的名字。当然，无论你是否这么做，IntelljJ IDEA都会让你可以看到参数的类型信息。</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/LambdaParamTypes.png" alt="LambdaParamTypes.png" /></p>

<p>甚至lambda表示的函数接口也能看到。</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/LambdaFunctionalInterface.png" alt="LambdaFunctionalInterface.png" /></p>

<h2>设计Lambda表达式</h2>

<p>我认为lambda表达式有一点类似于泛型-我们会经常使用到泛型(例如，添加类型信息到List&lt;>)，但是比较罕见的是我们去设计一个具有泛型的方法或者一个类(比如像Person\&lt;T>)。同样的，我们经常会在使用诸如Streams API的时候传递lambda，但是却很少会创建一个需要传递lambda参数的方法。</p>

<p>如果你发现自己处在这样一种境况，那么这里有一些提示。</p>

<h3>IntelliJ IDEA能够帮助你引入函数参数</h3>

<p>能够让你创建一个参数，这个参数是一个lambda而不是一个Object。这个特性的最好的一点就是它会智能建议一个匹配规格的已存在的函数接口。</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/LambdaExtractFunctionalParameter.png" alt="LambdaExtractFunctionalParameter.png" /></p>

<h3>使用存在的函数接口</h3>

<p>随着开发者变得对Java8越来越熟悉，当使用Supplier和Consumer这些接口时，我们将会知道什么是我们所期望的，比如创建一个ErrorMessageCreator(例子)会是令人迷惑和浪费的。可以看一下<a href="https://docs.oracle.com/javase/8/docs/api/java/util/function/package-summary.html">函数包</a>获取已经存在的那些函数接口。</p>

<h3>给你的函数接口添加@FunctionalInterface</h3>

<p>如果你确实需要创建自己的函数接口，那么用这个注解去标记它。看起来不需要这么做，但是Inteelij IDEA会在你的函数接口没有符合规范时提示你。当没有方法实现这个接口时，它会提示你。</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/LambdaFunctionalInterfaceNoMethod.png" alt="LambdaFunctionalInterfaceNoMethod.png" /></p>

<p>当方法太多时，也会提示你。</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/LambdaFunctionalInterfaceTooManyMethods.png" alt="LambdaFunctionalInterfaceTooManyMethods.png" /></p>

<p>当你把注解应用到一个类而不是接口时，也会发出提醒。</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/LambdaFunctionalInterfaceNotInterface.png" alt="LambdaFunctionalInterfaceNotInterface.png" /></p>

<p>Lambda表达式可以被用在任何有一个Single Abstract Method的接口中，但是它不能够应用到一个抽象类中。看起来没有啥逻辑，但它就是这样的。</p>

<h2>Streams</h2>

<p><a href="https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html">Stream API</a>是Java8另一个最大的卖点，我认为我们到现在还是没有真的搞清楚这会如何改变我们的代码。这里有一些我自己发现很有用的东西。</p>

<h3>将点号对齐</h3>

<p>我个人比较喜欢对齐Stream操作。当然，你不需要非得这么做，但是我发现这样还是有很大好处的。</p>

<ul>
<li>一眼就看到所有的操作以及他们的顺序</li>
<li>更容易调试(虽然Intellij IDEA提供了在一行中的lambda表达式中任意地方打断点的功能，但是将他们分隔成单独的行会更简单)</li>
<li>可以很容易地注释掉一些操作以供测试</li>
<li>很容易地插入peek()供调试或者测试</li>
</ul>


<p><img src="http://www.rowkey.me/images/blog_images/java8/StreamWrappingExample.png" alt="StreamWrappingExample.png" /></p>

<p>这样做也会让代码看起来很舒服。当然，如果这么做，会增加代码的行数。</p>

<p>你可以修改一下格式化设置使点号对齐。</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/StreamWrapping.png" alt="StreamWrapping.png" /></p>

<h3>使用方法引用(Method References)</h3>

<p>你可能需要一会儿才能习惯这个奇怪的语法。但是，当我们能够正确地使用，它确实能够提高程序的可阅读性。考虑下面的代码：</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/StreamSimpleFilter.png" alt="StreamSimpleFilter.png" /></p>

<p>对比一下使用新引入的Objects类的辅助方法(helper methods)：</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/StreamMethodRefFilter.png" alt="StreamMethodRefFilter.png" /></p>

<p>后面的代码能够更加明显地表明它想保存的值。IntelliJ IDEA会提示你何时一个lambda可以被替换成方法引用。</p>

<h3>当迭代一个集合，尽可能地使用Streams API</h3>

<p>使用新的集合方法：forEach。IDEA会提示你。</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/StreamRepalceForWithForEach.png" alt="StreamRepalceForWithForEach.png" /></p>

<p>使用Streams API相比起使用循环和if语句更加清晰明了。例如：</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/StreamBefore.png" alt="StreamBefore.png" /></p>

<p>IDEA会建议重构为：</p>

<p><img src="http://www.rowkey.me/images/blog_images/java8/StreamAfter.png" alt="StreamAfter.png" /></p>

<p>我做的性能测试表示这个重构是令人惊奇的-无论性能是不变、提升还是变差，都是不能被预测的。因此，当你的应用对性能要求很苛刻的话，重构的时候务必做好测试。</p>

<h3>当遍历数组时使用循环</h3>

<p>使用Java8并不是意味着你必须到处都使用Stream和新的集合方法。IDEA会智能提示哪些地方可以转换为Stream操作，但是并不意味着你必须这么做。</p>

<p>特别是当遍历一个保存基本数据类型的小数组时，使用loop循环的性能是更加好的，而且更加可阅读(至少对哪些Stream的新手来说是这样的)。</p>

<p>以上的建议，并非是固定不变，也不是必须要遵守的。但是无论你倾向于继续使用loops做某些操作还是在能够使用的地方使用Stream API, 你都要做出你自己的决定。</p>

<h2>总结</h2>

<p>每天我都会发现新的东西，我推崇的东西有时也会改变 - 例如方法引用，我曾经讨厌使用它，也避免在代码中使用。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[译]StackOverflow: 你没见过的七个最好的Java答案]]></title>
    <link href="http://www.rowkey.me/blog/2016/08/03/so-java-7-answers/"/>
    <updated>2016-08-03T20:30:34+08:00</updated>
    <id>http://www.rowkey.me/blog/2016/08/03/so-java-7-answers</id>
    <content type="html"><![CDATA[<p>原文：<a href="https://dzone.com/articles/stackoverflow-7-of-the-best-java-answers-that-you">https://dzone.com/articles/stackoverflow-7-of-the-best-java-answers-that-you</a></p>

<p>StackOverflow(后边简称so)发展到目前，已经成为了全球开发者的金矿。它能够帮助我们找到在各个领域遇到的问题的最有用的解决方案，同时我们也会从中学习到很多新的东西。这篇文章是在我们审阅了so上最流行的Java问题以及答案后从中挑出来的。即使你是一个有丰富经验的开发者，也能从中学到不少东西。</p>

<!--more-->


<h2>分支预测</h2>

<p>SO上最多投票的一个Java问题是：<a href="http://stackoverflow.com/questions/11227809/why-is-it-faster-to-process-a-sorted-array-than-an-unsorted-array">为什么处理一个排序数组要比非排序数组快的多</a>。为了回答这个问题，你需要使用分支预测(branch prediction)。分支预测是一种架构，旨在通过在真实的路径发生前猜测某一分支的下一步来提升处理过程。</p>

<p>分支在这里即一个if语句。这样的话，如果是一个排序数组，那么分支预测将会进行，否则不会进行。<a href="http://stackoverflow.com/questions/11227809/why-is-it-faster-to-process-a-sorted-array-than-an-unsorted-array/11227902#11227902">Mysticial</a>(so上的一个回答者)试图使用铁路和火车来简单介绍这个概念。假设你在铁轨连接处要决定火车要走哪条路，你会选择左边还是右边？你可以拦住火车，然后问司机该往那里，但是这样会让整个过程变慢。因此你只能去猜正确的方向，那么如何去猜呢？最好的办法就是通过观察目前这个火车每次经过时的路线，推测出正确的方向。</p>

<p>这就是分支预测：识别模式并使用它。</p>

<p>不幸的是，这个问题的提问者是分支预测失败的受害者。因为他的分支没有任何可以识别出的模式，所以预测出的行为是随机的。</p>

<h2>Java中的安全</h2>

<p>另一个流行的Java问题是：<a href="http://stackoverflow.com/questions/8881291/why-is-char-preferred-over-string-for-passwords-in-java">为什么在Java中有关密码的地方更加喜欢使用char[]而不是String</a>？其实原始的问题更加具体一些，就是问的在Swing中，password控件有一个getPassword方法(返回char[]而不是getText()返回的String)。</p>

<p>其实这里不用惊讶-这是一个安全问题。String是不可变的，意味着一旦它被创建了，那么你就不可能去修改它。这也意味着在GC之前，你对这些数据不能做任何处理。因此，只要有人能够访问你的内存，那么String就有可能被他获取到。</p>

<p>这也就是为什么要使用char数组。你可以显示地清除数据或者覆盖它。这样密码这种敏感数据即使GC还没有进行也不会再在系统留下痕迹。</p>

<h2>异常</h2>

<p>即使很多开发者倾向于忽略对受检异常的处理，SO上仍然有很多关于异常的问题。其中一个最流行的问题是：什么是NullPointerException，我该怎么处理它？对此，我们并没有感到惊讶，因为这个问题也是<a href="http://blog.takipi.com/the-top-10-exceptions-types-in-production-java-applications-based-on-1b-events/">在生产环境的Java应用中排名第一的异常</a>。</p>

<p>实际上，当NullPointerException(或者其他exception)在系统出现的时候，我们可以发出一个告警。因为这种异常一般情况下都是业务代码逻辑有问题造成(笔者注)。</p>

<h2>为什么这段代码使用随机字符串打印出了&#8221;hello world&#8221;</h2>

<p>问题链接：<a href="http://stackoverflow.com/questions/15182496/why-does-this-code-using-random-strings-print-hello-world">http://stackoverflow.com/questions/15182496/why-does-this-code-using-random-strings-print-hello-world</a></p>

<p>这个问题给出了下面的代码，并打印出了&#8221;hello world&#8221;。</p>

<pre><code>System.out.println(randomString(-229985452) + " " + randomString(-147909649));

public static String randomString(int i){
    Random ran = new Random(i);
    StringBuilder sb = new StringBuilder();
    while (true)
    {
        int k = ran.nextInt(27);
        if (k == 0)
            break;

        sb.append((char)('`' + k));
    }

    return sb.toString();
}
</code></pre>

<p>其实，选择一组随机的整数并不是随机的。给定一个seed参数(在这个例子中是-229985452和-147909649), 那么每次随机，同样的seed则会产生同样的输出。</p>

<p>Random(-229985452).nextInt(27)产生的前六个数字：8, 5, 12, 12, 15, 0</p>

<p>Random(-147909649).nextInt(27)产生的前六个数字：23, 15, 18, 12, 4, 0</p>

<p>这样，最终输出的就是&#8221;hello world&#8221;。</p>

<h2>为什么两个时间戳相减(in 1927)得出一个奇怪的结果？</h2>

<p>问题链接：<a href="http://stackoverflow.com/questions/6841333/why-is-subtracting-these-two-times-in-1927-giving-a-strange-result">http://stackoverflow.com/questions/6841333/why-is-subtracting-these-two-times-in-1927-giving-a-strange-result</a></p>

<pre><code>public static void main(String[] args) throws ParseException {
    SimpleDateFormat sf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");  
    String str3 = "1927-12-31 23:54:07";  
    String str4 = "1927-12-31 23:54:08";  
    Date sDt3 = sf.parse(str3);  
    Date sDt4 = sf.parse(str4);  
    long ld3 = sDt3.getTime() /1000;  
    long ld4 = sDt4.getTime() /1000;
    System.out.println(ld4-ld3);
}
</code></pre>

<p>按说上面的代码最后的结果应该是1，但实际的输出却是353。其实，这是一个时区的问题。1927年12月31号24:00，上海时间往回调整了5分钟52秒，因此&#8221;1927-12-31 23:54:08&#8221;发生了两次，Java将后面一次实例化成了本地的这个时间。因此和前一秒的差距成了353。</p>

<p>我们需要指出，如果你试着来运行这段代码，结果并不一定是353。<a href="http://stackoverflow.com/a/6841479/5982245">Jon Skeet指出了这一点</a>，在时区数据库项目2014版中，这个改变的时间点改到了1900-12-31，因此成了344秒的差距。</p>

<h2>无法被捕获的ChuckNorrisException</h2>

<p>问题链接：<a href="http://stackoverflow.com/questions/13883166/uncatchable-chucknorrisexception">http://stackoverflow.com/questions/13883166/uncatchable-chucknorrisexception</a></p>

<p>这里有一个很明显的问题：如果有exception被抛出，但是没有任何办法去catch，那么应用会崩溃吗？或者如这个问题所问：是否可以写一段Java代码让一个假设的java.lang.ChuckNorrisException无法被捕获。</p>

<p>答案是可以，但是这里有一个&#8221;但是&#8221;。你可以编译一段代码抛出一个ChuckNorrisException，但是在Runtime时动态生成一个并不继承于Throwable接口的ChuckNorrisException类。当然，为了让这个过程可以进行，你需要关闭掉字节码验证。<a href="http://stackoverflow.com/a/13883510/5982245">jtahlborn</a>给出了完整的解决办法。</p>

<h2>哈希表</h2>

<p>哈希表是另外一个在SO上流行的问题系列。许多用户都想要知道所有集合类之间的区别，什么时候该使用哪种集合。</p>

<p>迭代顺序是主要考虑的因素。使用HashMap则忽略了所有的顺序信息，也就是获取元素的顺序和你插入元素的顺序是没有任何关系的；使用TreeMap则会得到一个排序好的迭代集合；使用LinkedHashMap则是一个FIFO的顺序。</p>

<p>如果你还是对这些感到困惑，这里有一个相关说明的图表可以<a href="http://zeroturnaround.com/wp-content/uploads/2016/04/Java-Collections-cheat-sheet.png">参考</a>(Rebel Labs制作)。</p>

<p><img src="http://www.rowkey.me/images/blog_images/Java-Collections-cheat-sheet.png" alt="Java-Collections-cheat-sheet" /></p>

<h2>总结</h2>

<p>对于Java，其实关键的不在于你懂多少，而是在于你可以一直学到更多的东西。StackOverflow不仅在code上的一些问题可以帮助我们，也有助于我们回过头来去深入地学习一些我们已经知道的知识。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java后端工程师学习大纲]]></title>
    <link href="http://www.rowkey.me/blog/2016/06/27/java-backend-study/"/>
    <updated>2016-06-27T21:39:34+08:00</updated>
    <id>http://www.rowkey.me/blog/2016/06/27/java-backend-study</id>
    <content type="html"><![CDATA[<p>之前自己总结过的<a href="http://www.rowkey.me/blog/2016/06/17/java-skill-tree/">Java后端工程师技能树</a>，其涵盖的技术点比较全面，并非一朝一夕能够全部覆盖到的。对于一些还没有入门或者刚刚入门的Java后端工程师，如果一下子需要学习如此多的知识，想必很多人会望而却步。</p>

<p>本文截取了技能树中的一些关键技能点，并辅以学习资料和书籍推荐，做为Java后端工程师的一个入门或者入职学习计划，基本上涵盖了一个合格的Java后端工程师必备的技能点，是一个相对完整的从基础到高级的修炼过程。当然，这只是一个大纲性指引的东西，也主要针对的是Java后端这个职位，并不会面面俱到，也不会很详细的讲述。毕竟其中每一个知识点深入下去都是可以成书的。另外，像数据结构、计算机网络等计算机科学基础知识，我认为是从事计算机专业的人必备的知识点，因此并不包括在内。如果要一个很全的知识点可以移步<a href="http://www.rowkey.me/blog/2016/06/17/java-skill-tree/">Java后端工程师技能树</a>。</p>

<p><strong>本大纲于2016.07.07最新更新</strong>^_^&hellip;</p>

<!--more-->


<h2>一. Git版本管理+Maven工程管理</h2>

<p><a href="http://weibo.com/p/1001643874239169320051">微博新兵训练营课程——环境与工具</a></p>

<h2>二. Java编程</h2>

<h3>书籍</h3>

<ul>
<li><a href="https://book.douban.com/subject/3146174/">《Java核心技术(卷1)》</a>：学习java必备的黄皮书，入门推荐书籍</li>
<li><a href="https://book.douban.com/subject/3360866/">《Java核心技术(卷2)》</a>：黄皮书之高级特性</li>
<li><a href="https://book.douban.com/subject/10484692/">《Java并发编程实战》</a>: 对java并发库讲得非常透彻</li>
<li><a href="https://book.douban.com/subject/3360807/">《Effective Java》</a>：Java之父高司令都称赞的一本java进阶书籍</li>
<li><a href="https://book.douban.com/subject/26274206/">《写给大忙人看的Java SE 8》</a>:涵盖了java8带来以及java7中被略过的新的java特性，值得一看</li>
</ul>


<h3>资料</h3>

<ul>
<li>Socket编程: <a href="http://ifeve.com/java-socket/">http://ifeve.com/java-socket/</a></li>
<li>NIO: <a href="http://ifeve.com/java-nio-all/">http://ifeve.com/java-nio-all/</a></li>
<li>序列化: <a href="http://ifeve.com/java-io-s-objectinputstream-objectoutputstream/">http://ifeve.com/java-io-s-objectinputstream-objectoutputstream/</a></li>
<li>RPC框架: <a href="http://dubbo.io">http://dubbo.io</a></li>
<li>并发编程：<a href="http://ifeve.com/java-concurrency-constructs/">http://ifeve.com/java-concurrency-constructs/</a></li>
</ul>


<h2>三. 开发框架</h2>

<ul>
<li>Spring: <a href="http://www.open-open.com/doc/view/5407635b943d410c9cfde409c90450b7">跟开涛学Spring3</a></li>
<li>Spring MVC: <a href="http://www.cnblogs.com/kaitao/archive/2012/07/16/2593441.html">跟开涛学SpringMvc</a></li>
<li>MyBatis: <a href="http://www.yihaomen.com/article/java/302.htm">MyBatis实战教程</a> <a href="http://limingnihao.iteye.com/blog/781671">MyBatis学习</a></li>
</ul>


<p>对于这些框架或者是一些常用的软件，个人最推崇的还是阅读<strong>官方文档</strong>来学习。当然，看这些资料能让你入门地更加快速一些。</p>

<p>更进一步的，在学会使用之后，去阅读这些框架或软件的源码是必不可少的一步。阅读源码的一种比较好的步骤如下：</p>

<ul>
<li>1) 先阅读架构文档</li>
<li>2) 根据架构，将源码文件以模块（或上下层级）分类</li>
<li>3) 从最独立（依赖性最小）的模块代码读起</li>
<li>4) 阅读该模块功能文档</li>
<li>5) 阅读该模块源代码</li>
<li>6) 一边阅读一边整理「调用关系表」</li>
<li>7) goto 3</li>
</ul>


<h2>四. 性能优化与诊断-系统</h2>

<p><a href="https://book.douban.com/subject/4027746/">《Linux服务器性能调整》</a></p>

<p>学习内容：</p>

<ul>
<li>Linux概述</li>
<li>性能分析工具</li>
<li>系统调优</li>
<li>Linux服务器应用的性能特征</li>
<li>调优案例分析</li>
</ul>


<h2>五. 性能优化与诊断-JVM</h2>

<ul>
<li><p><a href="https://book.douban.com/subject/25828043/">《Java性能优化权威指南》</a></p>

<p>  学习内容：</p>

<ul>
<li>JVM概述</li>
<li>JVM性能监控</li>
<li>JVM性能剖析与工具</li>
<li>JVM参数与调优步骤</li>
<li>JVM调优案例分析</li>
</ul>
</li>
<li><p><a href="https://book.douban.com/subject/24722612/">《深入理解Java虚拟机(第二版)》</a></p></li>
</ul>


<h2>六. 消息中间件</h2>

<h3>JMS</h3>

<p>最为经典，也比较简单的一个消息中间件规范，ActiveMQ是其一个实现。但由于自身的一些局限，不再推荐使用。</p>

<ul>
<li>大规模分布式消息中间件简介：<a href="http://blog.csdn.net/huyiyang2010/article/details/5969944">http://blog.csdn.net/huyiyang2010/article/details/5969944</a></li>
<li>JMS Overview: <a href="http://docs.oracle.com/javaee/6/tutorial/doc/bncdr.html">http://docs.oracle.com/javaee/6/tutorial/doc/bncdr.html</a></li>
<li>Basic JMS API Concepts: <a href="http://docs.oracle.com/javaee/6/tutorial/doc/bncdx.html">http://docs.oracle.com/javaee/6/tutorial/doc/bncdx.html</a></li>
<li>The JMS API Programming Model: <a href="http://docs.oracle.com/javaee/6/tutorial/doc/bnceh.html">http://docs.oracle.com/javaee/6/tutorial/doc/bnceh.html</a></li>
<li>Creating Robust JMS Applications:<a href="http://docs.oracle.com/javaee/6/tutorial/doc/bncfu.html">http://docs.oracle.com/javaee/6/tutorial/doc/bncfu.html</a></li>
<li>Using the JMS API in Java EE Applications: <a href="http://docs.oracle.com/javaee/6/tutorial/doc/bncgl.html">http://docs.oracle.com/javaee/6/tutorial/doc/bncgl.html</a></li>
<li>Further Information about JMS: <a href="http://docs.oracle.com/javaee/6/tutorial/doc/bncgu.html">http://docs.oracle.com/javaee/6/tutorial/doc/bncgu.html</a></li>
</ul>


<h3>RabbitMQ</h3>

<p>RabbitMQ是AMQP(The Advanced Message Queuing Protocol)协议的实现。适用于需要事务管理、对消息丢失很敏感的应用场景。对比kafka来看，RabbitMQ更为强调消息的可靠性、事务等。通过阅读官方文档学习即可：<a href="http://www.rabbitmq.com/documentation.html">官方文档</a></p>

<h3>Kafka</h3>

<p>基于日志的消息队列，首推当然是官方文档: <a href="http://kafka.apache.org/documentation.html">http://kafka.apache.org/documentation.html</a></p>

<ul>
<li><p><a href="http://www.orchome.com/kafka/index">kafka中文教程</a>：比较不错的中文教程</p>

<p>  学习内容：</p>

<ul>
<li>开始学习kafka</li>
<li>入门</li>
<li>接口</li>
<li>配置</li>
<li>设计</li>
<li>实现</li>
<li>什么是kafka</li>
<li>什么场景下使用kafka</li>
</ul>
</li>
<li><p><a href="https://github.com/superhj1987/kafka-study">kafka-study</a>: 笔者在学习kafka时的一些笔记</p></li>
</ul>


<h2>七. OAuth认证技术</h2>

<h3>原理</h3>

<p>OAuth是目前最为流行的第三方认证技术，即如何为第三方应用提供基于自己系统帐户体系的认证。目前，微博、微信、QQ、Facebook、Twitter基本上都是通过此协议让第三方应用集成的。简单的介绍可见百度百科简介: <a href="http://baike.baidu.com/link?url=Atszf_5BaipVU0_H2Gy8qZ9K0W9WnnmEmRwl6SXkHJyrbB5-GxZ_Kc57hjaCEfF-0wGkcblothOuji0Cabwvu_">OAuth</a>。</p>

<p>此外，这里有一篇博文讲的比较详细：<a href="https://www.baidu.com/link?url=dsh9gFpNCLJSQoBq13Pw_nND3XvhBEfuuWQIyDpSDahpKPARnW2b950PgL0ywr8f&amp;wd=&amp;eqid=921a63a50002869300000004577e6e05">OAuth的机制原理讲解及开发流程</a>。</p>

<h3>开源实现</h3>

<ul>
<li>Google oauth core：<a href="http://oauth.net/code/">http://oauth.net/code/</a></li>
<li>Spring oauth: <a href="http://projects.spring.io/spring-security-oauth/">http://projects.spring.io/spring-security-oauth/</a></li>
</ul>


<h2>八. Redis设计与实现</h2>

<ul>
<li><p><a href="http://redisdoc.com/">Redis命令</a>: 使用当然要看这份权威文档，也是平常开发中最常用的参考资料。</p></li>
<li><p><a href="http://redisbook.com/">Redis设计与实现</a>：可以通过此文档来学习Redis的原理。当然，自己去看redis的源代码更是不错的选择。</p>

<p>  学习内容：</p>

<ul>
<li>常用命令以及数据结构</li>
<li>内部数据结构</li>
<li>内存映射数据库结构</li>
<li>redis数据类型</li>
<li>功能的实现</li>
<li>内部运作机制</li>
</ul>
</li>
</ul>


<h2>九. 数据相关</h2>

<h3>理论基础</h3>

<ul>
<li><a href="http://blog.csdn.net/active1001/archive/2007/07/02/1675920.aspx">MapReduce</a>: 分布式计算的鼻祖，当然谷歌现在推出了新的计算模型。</li>
<li><a href="http://blog.csdn.net/xuleicsu/archive/2005/11/10/526386.aspx">GFS</a>: 分布式存储技术，开源实现为HDFS</li>
<li><a href="http://blog.csdn.net/accesine960/archive/2006/02/09/595628.aspx">Bigtable</a>: 稀疏大型数据库(列数据库)技术，开源实现为HBASE。</li>
</ul>


<p>作为业界良心的google还有其他许多先进的分布式技术，其论文也非常值得去研读。可以通过此链接获取一些论文的内容：<a href="http://www.chinacloud.cn/show.aspx?id=14382&amp;cid=11">http://www.chinacloud.cn/show.aspx?id=14382&amp;cid=11</a></p>

<h3>实时计算</h3>

<ul>
<li><a href="https://book.douban.com/subject/26312249/">《Storm分布式实时计算模式》</a>：虽然twitter推出了新一代的Heron，但Storm仍是目前应用最为广泛的实时计算技术。</li>
<li><a href="https://spark.apache.org/streaming/">Spark streaming: </a>：Spark带来了基于批处理的实时流计算技术，对比Storm各有优劣。</li>
</ul>


<h3>离线计算</h3>

<ul>
<li><a href="https://book.douban.com/subject/26206050/">《Hadoop权威指南》</a>：无须多言，Haoop是大数据必须要学习的技术，涵盖了HDFS+HBase+MapReduce。</li>
<li><a href="https://book.douban.com/subject/25791255/">《Hive编程指南》</a>：Hive降低了MapReduce程序编写的复杂度。</li>
<li><a href="https://book.douban.com/subject/26616244/">《Spark快速大数据分析》</a>： Spark引进的基于RDD的计算模型大大提高了离线计算的性能，相对于MR来说是更为领先的离线计算技术。</li>
</ul>


<h3>Lambda架构</h3>

<p>大数据领域的经典架构方案，融合了离线和实时计算模型，对外能够提供稳定可靠的数据。对此架构的剖析可见此篇文章：<a href="http://www.csdn.net/article/2014-07-08/2820562-Lambda-Linkedln">Linkedln技术高管Jay Kreps：Lambda架构剖析</a></p>

<h3>机器学习</h3>

<p>除了个性化推荐系统之外，CTR预估、广告推荐、预测模型都是机器学习的应用场景。</p>

<ul>
<li><a href="https://book.douban.com/subject/10769749/">《推荐系统实践》</a></li>
<li><a href="https://book.douban.com/subject/26596778/">《计算广告》</a></li>
<li><a href="https://book.douban.com/subject/3288908/">《集体智慧》</a></li>
<li><a href="https://book.douban.com/subject/26708119/">《机器学习》</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java开发的几个注意点]]></title>
    <link href="http://www.rowkey.me/blog/2016/06/19/java-tips/"/>
    <updated>2016-06-19T20:59:44+08:00</updated>
    <id>http://www.rowkey.me/blog/2016/06/19/java-tips</id>
    <content type="html"><![CDATA[<p>在Java工程师平常的开发过程中，由于业务的不同，可能关注的点有很多不一样的地方，但是在基础层面还是有一些共性的。此文概括了在Java开发、测试、部署、工程化方面一些需要注意的地方，供大家参考。</p>

<!--more-->


<h2>1. 将一些需要变动的配置写在属性文件中</h2>

<p>比如，没有把一些需要并发执行时使用的线程数设置成可在属性文件中配置。那么你的程序无论在DEV环境中，还是TEST环境中，都可以顺畅无阻地运行，但是一旦部署在PROD上，把它作为多线程程序处理更大的数据集时，就会抛出IOException，原因也许是线上环境并发造成也许是其他。如果线程数目可以在属性文件中配置，那么使它成为一个单线程应用程序就变得十分容易了。我们不再需要为了解决问题而反复地部署和测试应用了。这种方法也同样适用于配置 URL、服务器和端口号等。</p>

<p>这里推荐使用属性文件外化这些配置，文件格式使用properties、yaml、hocon、json都可以。下面的类实现了对这些格式的文件的spring注入支持，包括占位符支持。</p>

<p><a href="https://github.com/superhj1987/awesome-libs/blob/master/src/main/java/me/rowkey/libs/spring/config/AwesomePropertyPlaceholderConfigurer.java">https://github.com/superhj1987/awesome-libs/blob/master/src/main/java/me/rowkey/libs/spring/config/AwesomePropertyPlaceholderConfigurer.java</a></p>

<h2>2. 测试中尽可能模拟线上环境</h2>

<p>生产过程中一个典型的场景就是只使用1到3个帐户进行测试，而这个数量本应是1000到2000个的。在做性能测试时，使用的数据必须是真实并且未经裁剪的。不贴近真实环境的性能测试，可能会带来不可预料的性能、拓展和多线程问题。这里也可以采取预发布环境的方式来解决部分问题。</p>

<h2>3. 对于所有外部调用以及内部服务都要做容错处理</h2>

<p>不管是RPC调用还是对于第三方服务的调用，都不能想当然的认为可用性是100%的。不允许出现服务调用超时和重试，将会对应用程序的稳定性和性能造成不利的影响。</p>

<h2>4. 安全设计上一个系统要遵循最小权限原则</h2>

<p>网络服务随处可见，从而使得黑客可以轻易地利用它进行拒绝服务攻击。所以，设计系统时，需要遵循“最小权限”原则，采用白名单等方式。</p>

<h2>5. 需要提供以下文档</h2>

<ol>
<li>编写单元测试文档并使其拥有良好的代码覆盖率。</li>
<li>高层次的设计图：描述了所有的组件，交互和结构。</li>
<li>详细的设计图：具体到代码层面的设计，以及一些关键逻辑的流程。</li>
<li>系统组成文档：说明系统的所有组成文件、配置文件等。</li>
<li>数据库层面的dml以及ddl文档，尤其是sql查询语句需要经过dba或者核心开发人员的review才能够上线。</li>
</ol>


<p>不仅仅对于传统的开发流程，即使对于敏捷开发，这些文档也是必不可少的，否则在后续的维护、交接上会带来很大的不便。</p>

<h2>6. 做好系统关键功能的监控、错误恢复、备份等</h2>

<p>对于系统一些至关重要的功能模块要做好对其的监控，防止其影响系统的运行,造成不可估算的损失。另外，如果可以，监控到故障后去去试图恢复，恢复失败再发送告警。对于一些很重要的数据文件，还要做到冗余备份，防止发生一些突然故障造成数据丢失。</p>

<h2>7. 数据库设计时设计一些便于追踪历史、整理的列</h2>

<p>比如create_time、update_time可以说明记录的创建和更新时间。create_by、update_by可以说明记录是由谁创建和更新的。</p>

<p>此外，删除记录有时候并非真正删除，这时需要设计表示此记录状态的列，如可以取‘Active’或‘Inactive’的 ‘status’列。</p>

<h2>8. 制定好项目回滚计划</h2>

<p>新的功能上线时，如果发生故障，没有一份回滚计划，那么可能会手忙脚乱而造成线上服务一段时间不可用。有一个良好的回滚计划，可以让你能够有条不紊的执行相关操作，在可控时间内将系统恢复到一个可运行的状态。</p>

<h2>9. 项目上线前要做好量化分析</h2>

<p>对于项目中用到的内存、数据库、文件、缓存等，要做好量化分析。预估出未来一段时间的空间占用，给运维分配机器时一个参考。防止，由于数据量增长过快，导致存储不够。这一点是非常重要的，不然很容易造成线上服务不可用。</p>

<h2>10. 制定好系统的部署计划。</h2>

<p>系统部署的平台是一个至关重要的部分。对于部署平台的描述，不能仅限于一台服务器、两个数据库这个层面，至少需要包括</p>

<ul>
<li>操作系统的特定版本，JVM等。</li>
<li>有多少内存（包括物理内存，JVM堆内存，JVM栈内存和JVM永久代的空间）。</li>
<li>CPU（内核数）。</li>
<li>负载均衡器，需要的节点数、节点类型，比如是Active-Standby型还是Active-Active型。</li>
<li>文件系统要求，例如，你的应用程序可能会收集生成的日志并将其保存很长的周期，之后才进行归档。这样的话，你就需要有足够的硬盘空间。</li>
</ul>


<h2>11. 选择最合适的工具/技术</h2>

<p>很多情况下，开发者会在生产系统中使用一门想要学习的语言或某种工具。通常这不是最好的选择。比如，为已经实际上是关系型的数据使用NoSQL数据库。不管是语言还是工具，都有其适用的场景。不能求新，也不能以“自我”为标准。</p>

<h2>12. 在一些关键技术领域具有充足的知识储备。</h2>

<ul>
<li>设计模式</li>
<li>JVM调优</li>
<li>多线程“并发问题”</li>
<li>事务问题，包括分布式事务</li>
<li>性能问题，包括GC、计算等</li>
<li>缓存</li>
</ul>


<p><strong><em>Ps:此文部分内容来自网上资料。</em></strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java后端工程师技能树]]></title>
    <link href="http://www.rowkey.me/blog/2016/06/17/java-skill-tree/"/>
    <updated>2016-06-17T22:59:44+08:00</updated>
    <id>http://www.rowkey.me/blog/2016/06/17/java-skill-tree</id>
    <content type="html"><![CDATA[<p>此技能树借鉴自<a href="https://github.com/geekcompany/full-stack-tree">https://github.com/geekcompany/full-stack-tree</a>，基本涵盖了一个Java后端工程师应该具备的技能，如有遗漏或者错误，敬请指出。</p>

<p><strong>PS</strong>: 有同学指出tomcat不是web server是servlet容器，这一点图中的确不够严谨，因此更新了此技能树。其实tomcat引入了apr之后也是能够作为web server的(其实目前绝大多servlet容器都同时提供了web服务器的功能，只是性能之类的不好而已)，当然把tomcat直接做为web server的应用还不够广泛。目前，nginx+tomcat还是普遍的七层负载均衡构建集群的方案。此外，这里还牵扯到了servlet容器和应用服务器的一个区别，为什么tomcat只是servlet容器而不叫JavaEE应用服务器呢？这一点大家可以通过此链接了解一下:<a href="http://www.cnblogs.com/maydow/p/4821249.html">http://www.cnblogs.com/maydow/p/4821249.html</a>。当然，对于互联网领域目前的Java后端体系，weblogic、webspher这些应用服务器基本绝迹，因此并没有写在此技能树中。</p>

<!--more-->


<p><a href="http://www.rowkey.me/images/blog_images/java-skill-tree.png"><img src="http://www.rowkey.me/images/blog_images/java-skill-tree.png" alt="java-skill-tree" /></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[谈谈Java内存管理]]></title>
    <link href="http://www.rowkey.me/blog/2016/05/07/javamm/"/>
    <updated>2016-05-07T14:59:44+08:00</updated>
    <id>http://www.rowkey.me/blog/2016/05/07/javamm</id>
    <content type="html"><![CDATA[<h2>目录</h2>

<ul>
<li><a href="#%E4%B8%80.%20%E8%83%8C%E6%99%AF%E7%9F%A5%E8%AF%86">一. 背景知识</a></li>
<li><a href="#%E4%BA%8C.%20Jvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%86%85%E5%AD%98%E7%AE%80%E4%BB%8B">二. Jvm虚拟机内存简介</a></li>
<li><a href="#%E4%B8%89.%20%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86">三. 垃圾收集</a></li>
<li><a href="#%E5%9B%9B.%20Java7%E3%80%818%E5%B8%A6%E6%9D%A5%E7%9A%84%E4%B8%80%E4%BA%9B%E5%8F%98%E5%8C%96">四. Java7、8带来的一些变化</a></li>
</ul>


<p><img src="http://www.rowkey.me/images/blog_images/javamm/java.jpg" alt="java" /></p>

<p>对于一个Java程序员来说，大多数情况下的确是无需对内存的分配、释放做太多考虑，对Jvm也无需有多么深的理解的。但是在写程序的过程中却也往往因为这样而造成了一些不容易察觉到的内存问题，并且在内存问题出现的时候，也不能很快的定位并解决。因此，了解并掌握Java的内存管理是一个合格的Java程序员必需的技能，也只有这样才能写出更好的程序，更好地优化程序的性能。</p>

<!--more-->


<h2><a name='一. 背景知识'></a>一. 背景知识</h2>

<p>根据网络可以找到的资料以及笔者能够打听到的消息，目前国内外著名的几个大型互联网公司的语言选型概括如下：</p>

<ol>
<li>Google: C/C++ Go Python Java JavaScript，不得不提的是Google贡献给java社区的guava包质量非常高，非常值得学习和使用。</li>
<li>Youtube、豆瓣: Python</li>
<li>Fackbook、Yahoo、Flickr、新浪：<strong>php</strong>(优化过的php vm)</li>
<li>网易、阿里、搜狐: Java、PHP、Node.js</li>
<li>Twitter: Ruby->Java,之所以如此就在于与Jvm相比，Ruby的runtime是非常慢的。并且Ruby的应用比起Java还是比较小众的。不过最近twitter有往scala上迁移的趋势。</li>
</ol>


<p>可见，虽然最近这些年很多言论都号称java已死或者不久即死，但是Java的语言应用占有率一直居高不下。与高性能的C/C++相比，Java具有gc机制，并且没有那让人望而生畏的指针，上手门槛相对较低；而与上手成本更低的PHP、Ruby等脚本语言来说，又比这些脚本语言有性能上的优势(这里暂时忽略FB自己开发的HHVM)。</p>

<p>对于Java来说，最终是要依靠字节码运行在jvm上的。目前，常见的jvm有以下几种：</p>

<ul>
<li>Sun HotSpot</li>
<li>BEA Jrockit</li>
<li>IBM J9</li>
<li>Dalvik(Android)</li>
</ul>


<p>其中以HotSpot应用最广泛。目前sun jdk的最新版本已经到了8，但鉴于新版的jdk使用并未普及，因此本文仅仅针对HotSpot虚拟机的jdk6来讲。</p>

<h2><a name='二. Jvm虚拟机内存简介'></a>二. Jvm虚拟机内存简介</h2>

<h3>2.1 Java运行时内存区</h3>

<p>Java的运行时内存组成如下图所示：</p>

<p><img src="http://www.rowkey.me/images/blog_images/javamm/java-runtime-memory.jpg" alt="java-runtime-memory.jpg" /></p>

<p>其中，对于这各个部分有一些是线程私有的，其他则是线程共享的。</p>

<p><strong>线程私有的如下：</strong></p>

<ul>
<li><p>程序计数器</p>

<p>  当前线程所执行的字节码的行号指示器</p></li>
<li><p>Java虚拟机栈</p>

<p>  Java方法执行的内存模型，每个方法被执行时都会创建一个栈帧，存储局部变量表、操作栈、动态链接、方法出口等信息。</p>

<ul>
<li>每个线程都有自己独立的栈空间</li>
<li>线程栈只存基本类型和对象地址</li>
<li>方法中局部变量在线程空间中</li>
</ul>
</li>
<li><p>本地方法栈</p>

<p>  Native方法服务。在HotSpot虚拟机中和Java虚拟机栈合二为一。</p></li>
</ul>


<p><strong>线程共享的如下：</strong></p>

<ul>
<li><p>Java堆</p>

<p>  存放对象实例，几乎所有的对象实例以及其属性都在这里分配内存。</p></li>
<li><p>方法区</p>

<p>  存储已经被虚拟机加载的类信息、常量、静态变量、JIT编译后的代码等数据。</p></li>
<li><p>运行时常量池</p>

<p>  方法区的一部分。用于存放编译期生成的各种字面量和符号引用。</p></li>
<li><p>直接内存</p>

<p>  NIO、Native函数直接分配的堆外内存。DirectBuffer引用也会使用此部分内存。</p></li>
</ul>


<h3>2.2 对象访问</h3>

<p>Java是面向对象的一种编程语言，那么如何通过引用来访问对象呢？一般有两种方式：</p>

<ol>
<li><p>通过句柄访问</p>

<p> <image src="http://www.rowkey.me/images/blog_images/javamm/access_object_handler.png" width="500px"/></p></li>
<li><p>直接指针</p>

<p> <image src="http://www.rowkey.me/images/blog_images/javamm/access_direct.png" width="500px"/></p>

<p> 此种方式也是HotSpot虚拟机采用的方式。</p></li>
</ol>


<h3>2.3 内存溢出</h3>

<p>在JVM申请内存的过程中，会遇到无法申请到足够内存，从而导致内存溢出的情况。一般有以下几种情况：</p>

<ul>
<li>虚拟机栈和本地方法栈溢出

<ul>
<li>StackOverflowError: 线程请求的栈深度大于虚拟机所允许的最大深度(循环递归)</li>
<li>OutOfMemoryError: 虚拟机在扩展栈是无法申请到足够的内存空间，一般可以通过不停地创建线程引起此种情况</li>
</ul>
</li>
<li>Java堆溢出: 当创建大量对象并且对象生命周期都很长的情况下，会引发OutOfMemoryError</li>
<li>运行时常量区溢出：OutOfMemoryError:PermGen space，这里一个典型的例子就是String的intern方法，当大量字符串使用intern时，会触发此内存溢出</li>
<li>方法区溢出：方法区存放Class等元数据信息，如果产生大量的类(使用cglib)，那么就会引发此内存溢出，OutOfMemoryError:PermGen space，在使用Hibernate等框架时会容易引起此种情况。</li>
</ul>


<h2><a name='三. 垃圾收集'></a>三. 垃圾收集</h2>

<h3>3.1 理论基础</h3>

<p>在通常情况下，我们掌握java的内存管理就是为了应对网站/服务访问慢，慢的原因一般有以下几点：</p>

<ul>
<li>内存：垃圾收集占用cpu；放入了太多数据，造成内存泄露(java也是有这种问题的^_^)</li>
<li>线程死锁</li>
<li>I/O速度太慢</li>
<li>依赖的其他服务响应太慢</li>
<li>复杂的业务逻辑或者算法造成响应的缓慢</li>
</ul>


<p>其中，垃圾收集对性能的影响一般有以下几个：</p>

<ul>
<li>内存泄露</li>
<li>程序暂停</li>
<li>程序吞吐量显著下降</li>
<li>响应时间变慢</li>
</ul>


<h4>垃圾收集的一些基本概念</h4>

<ul>
<li>Concurrent Collector:收集的同时可运行其他的工作进程</li>
<li>Parallel Collector: 使用多CPU进行垃圾收集</li>
<li>Stop-the-word(STW):收集时必须暂停其他所有的工作进程</li>
<li>Sticky-reference-count：对于使用“引用计数”（reference count）算法的GC，如果对象的计数器溢出，则起不到标记某个对象是垃圾的作用了，这种错误称为sticky-reference-count problem，通常可以增加计数器的bit数来减少出现这个问题的几率，但是那样会占用更多空间。一般如果GC算法能迅速清理完对象，也不容易出现这个问题。</li>
<li>Mutator：mutate的中文是变异，在GC中即是指一种JVM程序，专门更新对象的状态的，也就是让对象“变异”成为另一种类型，比如变为垃圾。</li>
<li>On-the-fly：用来描述某个GC的类型：on-the-fly reference count garbage collector。此GC不用标记而是通过引用计数来识别垃圾。</li>
<li>Generational gc：这是一种相对于传统的“标记-清理”技术来说，比较先进的gc，特点是把对象分成不同的generation，即分成几代人，有年轻的，有年老的。这类gc主要是利用计算机程序的一个特点，即“越年轻的对象越容易死亡”，也就是存活的越久的对象越有机会存活下去（姜是老的辣）。</li>
</ul>


<h4>吞吐量与响应时间</h4>

<p>牵扯到垃圾收集，还需要搞清楚吞吐量与响应时间的含义</p>

<ul>
<li>吞吐量是对单位时间内完成的工作量的量度。如：每分钟的 Web 服务器请求数量</li>
<li>响应时间是提交请求和返回该请求的响应之间使用的时间。如：访问Web页面花费的时间</li>
</ul>


<p>吞吐量与访问时间的关系很复杂，有时可能以响应时间为代价而得到较高的吞吐量，而有时候又要以吞吐量为代价得到较好的响应时间。而在其他情况下，一个单独的更改可能对两者都有提高。通常，平均响应时间越短，系统吞吐量越大；平均响应时间越长，系统吞吐量越小；
但是，系统吞吐量越大， 未必平均响应时间越短；因为在某些情况（例如，不增加任何硬件配置）吞吐量的增大，有时会把平均响应时间作为牺牲，来换取一段时间处理更多的请求。</p>

<p>针对于Java的垃圾回收来说，不同的垃圾回收器会不同程度地影响这两个指标。例如：并行的垃圾收集器，其保证的是吞吐量，会在一定程度上牺牲响应时间。而并发的收集器，则主要保证的是请求的响应时间。</p>

<h4>GC的流程</h4>

<ul>
<li>找出堆中活着的对象</li>
<li>释放死对象占用的资源</li>
<li>定期调整活对象的位置</li>
</ul>


<h4>GC算法</h4>

<ul>
<li>Mark-Sweep 标记-清除</li>
<li>Mark-Sweep-Compact 标记-整理</li>
<li><p>Copying Collector 复制算法</p></li>
<li><p>Mark-标记</p>

<p> 从&#8221;GC roots&#8221;开始扫描(这里的roots包括线程栈、静态常量等)，给能够沿着roots到达的对象标记为&#8221;live&#8221;,最终所有能够到达的对象都被标记为&#8221;live&#8221;,而无法到达的对象则为&#8221;dead&#8221;。效率和存活对象的数量是线性相关的。</p></li>
<li><p>Sweep-清除</p>

<p> 扫描堆，定位到所有&#8221;dead&#8221;对象，并清理掉。效率和堆的大小是线性相关的。</p></li>
<li><p>Compact-压缩</p>

<p> 对于对象的清除，会产生一些内存碎片，这时候就需要对这些内存进行压缩、整理。包括：relocate(将存货的对象移动到一起，从而释放出连续的可用内存)、remap(收集所有的对象引用指向新的对象地址)。效率和存活对象的数量是线性相关的。</p></li>
<li><p>Copy-复制</p>

<p> 将内存分为&#8221;from&#8221;和&#8221;to&#8221;两个区域，垃圾回收时，将from区域的存活对象整体复制到to区域中。效率和存活对象的数量是线性相关的。</p></li>
</ul>


<p>其中，Copy对比Mark-sweep</p>

<ol>
<li>内存消耗：copy需要两倍的最大live set内存；mark-sweep则只需要一倍。</li>
<li>效率上：copy与live set成线性相关，效率高；mark-sweep则与堆大小线性相关，效率较低。</li>
</ol>


<h4>分代收集</h4>

<p>分代收集是目前比较先进的垃圾回收方案。有以下几个相关理论</p>

<ul>
<li>分代假设：大部分对象的寿命很短，“朝生夕死”，重点放在对年青代对象的收集，而且年青代通常只占整个空间的一小部分。</li>
<li>把年青代里活的很长的对象移动到老年代。</li>
<li>只有当老年代满了才去收集。</li>
<li>收集效率明显比不分代高。</li>
</ul>


<p>HotSpot虚拟机的分代收集，分为一个Eden区、两个Survivor去以及Old Generation/Tenured区，其中Eden以及Survivor共同组成New Generatiton/Young space。通常将对New Generation进行的回收称为Minor GC;对Old Generation进行的回收称为Major GC，但由于Major GC除并发GC外均需对整个堆以及Permanent Generation进行扫描和回收，因此又称为Full GC。</p>

<p><image src="http://www.rowkey.me/images/blog_images/javamm/hotspot-gc.png" width="300px"/></p>

<ul>
<li>Eden区是分配对象的区域。</li>
<li>Survivor是minor/younger gc后存储存活对象的区域。</li>
<li>Tenured区域存储长时间存活的对象。</li>
</ul>


<p>分代收集中典型的垃圾收集算法组合描述如下：</p>

<ul>
<li>年青代通常使用Copy算法收集，会stop the world</li>
<li>老年代收集一般采用Mark-sweep-compact, 有可能会stop the world，也可以是concurrent或者部分concurrent。</li>
</ul>


<p>那么何时进行Minor GC、何时进行Major GC? 一般的过程如下：</p>

<ul>
<li>对象在Eden Space完成内存分配</li>
<li>当Eden Space满了，再创建对象，会因为申请不到空间，触发Minor GC，进行New(Eden + S0 或 Eden S1) Generation进行垃圾回收</li>
<li>Minor GC时，Eden Space不能被回收的对象被放入到空的Survivor（S0或S1，Eden肯定会被清空），另一个Survivor里不能被GC回收的对象也会被放入这个Survivor，始终保证一个Survivor是空的</li>
<li>在Step3时，如果发现Survivor区满了，则这些对象被copy到old区，或者Survivor并没有满，但是有些对象已经足够Old，也被放入Old Space。</li>
<li>当Old Space被放满之后，进行Full GC</li>
</ul>


<p>但这个具体还要看JVM是采用的哪种GC方案。</p>

<p>New Generation的GC有以下三种：</p>

<ul>
<li>Serial</li>
<li>ParallelScavenge</li>
<li>ParNew</li>
</ul>


<p>对于上述三种GC方案均是在Eden Space分配不下时，触发GC。</p>

<p>Old Generation的GC有以下四种：</p>

<ul>
<li>Serial Old</li>
<li>Parallel</li>
<li>CMS</li>
</ul>


<p>对于Serial Old, Parallel Old而言触发机制为</p>

<ul>
<li>Old Generation空间不足</li>
<li>Permanent Generation空间不足</li>
<li>Minor GC时的悲观策略</li>
<li>Minor GC后在Eden上分配内存仍然失败</li>
<li>执行Heap Dump时</li>
<li>外部调用System.gc,可通过-XX:+DisableExplicitGC来禁止,。这里需要注意的是禁用System.gc()会引起使用NIO时的OOM，所以此选项慎重使用。具体可见：<a href="http://hllvm.group.iteye.com/group/topic/27945">http://hllvm.group.iteye.com/group/topic/27945</a>。</li>
</ul>


<p>对于CMS而言触发机制为:</p>

<ul>
<li>当Old Generation空间使用到一定比率时触发，HopSpot V1.6中默认是92%，可通过PrintCMSInitiationStatistics(此参数在V1.5中不能用)来查看这个值到底是多少，通过CMSInitiatingOccupancyFaction来强制指定。默认值是根据如下公式计算出来的:((100 -MinHeapFreeRatio) +(double)(CMSTriggerRatio* MinHeapFreeRatio) / 100.0)/ 100.0，MinHeapFreeRatio默认值为40，CMSTriggerRatio默认值为80。</li>
<li>当Permanent Generation采用CMS收集且空间使用到一定比率触发，Permanent Generation采用CMS收集需设置：-XX:+CMSClassUnloadingEnabled。 Hotspot V1.6中默认为92%，可通过CMSInitiatingPermOccupancyFraction来强制指定。同样，它是根据如下公式计算出来的：((100 -MinHeapFreeRatio) +(double)(CMSTriggerPermRatio* MinHeapFreeRatio) / 100.0)/ 100.0，MinHeapFreeRatio默认值为40，CMSTriggerPermRatio默认值为80。</li>
<li>Hotspot根据成本计算决定是否需要执行CMS GC，可通过-XX:+UseCmsInitiatingOccupancyOnly来去掉这个动态执行的策略。</li>
<li>外部调用System.gc，且设置了ExplicitGCIInvokesConcurrent或者ExplicitGCInvokesConcurrentAndUnloadsClasses。</li>
</ul>


<h3>3.2 HotSpot垃圾收集器</h3>

<p><image src="http://www.rowkey.me/images/blog_images/javamm/hotspot-collector.png" width="300px"/></p>

<p>上图即为HotSpot虚拟机的垃圾收集器组成。</p>

<h4>Serial收集器</h4>

<ul>
<li>-XX:+UserSerialGC参数打开此收集器</li>
<li>Client模式下新生代默认的收集器。</li>
<li>较长的stop the world时间</li>
<li>简单而高效</li>
</ul>


<p>此收集器的一个工作流程如下如所示：</p>

<p>收集前：</p>

<p><image src="http://www.rowkey.me/images/blog_images/javamm/serial_before.png" width="400px"/></p>

<p>收集后：</p>

<p><image src="http://www.rowkey.me/images/blog_images/javamm/serial_after.png" width="400px"/></p>

<h4>ParNew收集器</h4>

<ul>
<li>-XX:+UserParNewGC</li>
<li>+UseConcuMarkSweepGC时默认开启</li>
<li>Serial收集器的多线程版本</li>
<li>默认线程数与CPU数目相同</li>
<li>-XX:ParrallelGCThreads指定线程数目</li>
</ul>


<p>对比Serial收集器如下图所示：</p>

<p><image src="http://www.rowkey.me/images/blog_images/javamm/parnew.png" width="400px"/></p>

<h4>Parallel Scavenge收集器</h4>

<ul>
<li>新生代并行收集器</li>
<li>采用Copy算法</li>
<li>主要关注的是达到可控制的吞吐量，“吞吐量优先”</li>
<li>-XX:MaxGCPauseMillis -XX:GCTimeRAtion两个参数精确控制吞吐量</li>
<li>-XX:UseAdaptiveSizePolicy GC自适应调节策略</li>
<li>Server模式的默认新生代收集器</li>
</ul>


<h4>Serial Old收集器</h4>

<ul>
<li>Serial的老年代版本</li>
<li>Client模式的默认老年代收集器</li>
<li>CMS收集器的后备预案，Concurrent Mode Failure时使用</li>
<li>-XX:+UseSerialGC开启此收集器</li>
</ul>


<h4>Parallel Old收集器</h4>

<ul>
<li>-XX:+UseParallelGC -XX:+UseParallelOldGC启用此收集器</li>
<li>Server模式的默认老年代收集器</li>
<li>Parallel Scavenge的老年代版本，使用多线程和&#8221;mark-sweep&#8221;算法</li>
<li>关注点在吞吐量以及CPU资源敏感的场合使用</li>
<li>一般使用Parallel Scavenge + Parallel Old可以达到最大吞吐量保证</li>
</ul>


<h4>CMS收集器</h4>

<p>并发低停顿收集器</p>

<ul>
<li>-XX:UseConcMarkSweepGC 开启CMS收集器，(默认使用ParNew作为年轻代收集器，SerialOld作为收集失败的垃圾收集器)</li>
<li>以获取最短回收停顿时间为目标的收集器，重视响应速度，希望系统停顿时间最短，会和互联网应用。</li>
</ul>


<p>四个步骤：</p>

<ul>
<li>初始标记 Stop the world: 只标记GC roots能直接关联到的对象，速度很快。</li>
<li>并发标记：进行GC roots tracing，与用户线程并发进行</li>
<li>重新标记 Stop the world：修正并发标记期间因程序继续运行导致变动的标记记录</li>
<li>并发清除</li>
</ul>


<p>对比serial old收集器如下图所示：</p>

<p><image src="http://www.rowkey.me/images/blog_images/javamm/cms.png" width="400px"/></p>

<p>CMS有以下的缺点：</p>

<ul>
<li>CMS是唯一不进行compact的垃圾收集器，当cms释放了垃圾对象占用的内存后，它不会把活动对象移动到老年代的一端</li>
<li>对CPU资源非常敏感。不会导致线程停顿，但会导致程序变慢，总吞吐量降低。CPU核越多越不明显</li>
<li>无法处理浮动垃圾。可能出现“concurrent Mode Failure”失败， 导致另一次full GC ,可以通过调整-XX:CMSInitiatingOccupancyFraction来控制内存占用达到多少时触发gc</li>
<li>大量空间碎片。这个可以通过设置-XX:UseCMSCompacAtFullCollection(是否在full gc时开启compact)以及-XX:CMSFullGCsBeforeCompaction(在进行compact前full gc的次数)</li>
</ul>


<h4>G1收集器</h4>

<p>G1算法在Java6中还是试验性质的，在Java7中正式引入，但还未被广泛运用到生产环境中。它的特点如下：</p>

<ul>
<li>使用标记-清理算法</li>
<li>不会产生碎片</li>
<li>可预测的停顿时间</li>
<li>化整为零：将整个Java堆划分为多个大小相等的独立区域</li>
<li>-XX:+UseG1GC可以打开此垃圾回收器</li>
<li>-XX:MaxGCPauseMillis=200可以设置最大GC停顿时间，当然JVM并不保证一定能够达到，只是尽力。</li>
</ul>


<p><image src="http://www.rowkey.me/images/blog_images/javamm/g1.png" width="500px"/></p>

<h3>3.3 调优经验</h3>

<ul>
<li>需要打开gc日志并读懂gc日志：-XX:PrintHeapAtGC -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamp -Xloggc:$CATALINA_BASE/logs/gc.log</li>
<li>垃圾回收的最佳状态是只有young gc，也就是避免生命周期很长的对象的存在。</li>
<li>从young gc开始，尽量给年青代大点的内存，避免full gc</li>
<li>注意Survivor大小</li>
<li>注意内存墙：4G~5G</li>
</ul>


<h4>GC日志简介</h4>

<p>1403682.561: [GC [PSYoungGen: 1375104K->11376K(1386176K)] 4145665K->2782002K(4182400K), 0.0174410 secs] [Times: user=0.27 sys=0.00, real=0.02 secs]</p>

<ul>
<li>1403682.561：发生的时间点，JVM运行的时间长度，以度为单位，也可以格式化成固定的时间格式(使用-XX:+PrintGCDateStamps)</li>
<li>PSYoungGen：发生了何种类型的GC，此处代表发生了年轻代的GC</li>
<li>1375104K：回收前的大小</li>
<li>11376K：回收后的大小</li>
<li>1386176K：YOUNG代的大小</li>
<li>4145665 K：回收前总的占用大小</li>
<li>2782002K：回收后的占用大小</li>
<li>4182400K：总占用大小</li>
<li>0.0174410：垃圾收集停顿时间</li>
<li>0.27和0.00：代表在用户态(user)和系统状(sys)的CPU运行时间</li>
<li>0.02 secs：代表实际的GC的运行时间</li>
</ul>


<p>注：上面实际GC的运行时间小于用户态和系统态的时间总和，是由于前者仅指CPU的运行时间，包括等待或IO阻塞的时间，而现在的GC是采用多线程收集的，同时机器也是多个CPU，因此，大部分是二者之和要比前面的值大。如果是采用串形化收集器的话，二者时间几乎相差不多。</p>

<h4>老年代使用建议</h4>

<ul>
<li>Parallel GC(-XX:+UseParallel[Old]GC)

<ul>
<li>Parallel GC的minor GC时间是最快的， CMS的young gc要比parallel慢， 因为内存碎片</li>
<li>可以保证最大的吞吐量</li>
</ul>
</li>
<li><strong>确实有必要才改成CMS或G1(for old gen collections)</strong></li>
</ul>


<h4>开发建议</h4>

<ul>
<li>小对象allocate的代价很小，通常10个CPU指令；收集掉新对象也非常廉价；不用担心活的很短的小对象</li>
<li>大对象分配的代价以及初始化的代价很大；不同大小的大对象可能导致java堆碎片，尤其是CMS, ParallelGC 或 G1还好；尽量避免分配大对象</li>
<li>避免改变数据结构大小，如避免改变数组或array backed collections / containers的大小;对象构建（初始化）时最好显式批量定数组大小;改变大小导致不必要的对象分配，可能导致java堆碎片</li>
<li>对象池可能潜在的问题

<ul>
<li>增加了活对象的数量，可能增加GC时间</li>
<li>访问（多线程）对象池需要锁，可能带来可扩展性的问题</li>
<li>小心过于频繁的对象池访问</li>
</ul>
</li>
</ul>


<h4>GC的庞氏骗局</h4>

<p>虽然GC在大多数情况下还是正常的，但有时候JVM也会发生欺骗你的场景， JVM不停的在垃圾回收，可是每次回收完后堆却还是满的，很明显程序内存被使用完了，已经无法正常工作了，但JVM就是不抛出OutOfMemoryError(OOM)这个异常来告诉程序员内部发出了什么，只是不停的做老好人尝试帮我们做垃圾回收，把服务器的资源耗光了。</p>

<p>出现这种现象的一种典型情况就是GC的GCTimeLimit和GCHeapFreeLimit参数设置不合适。GCTimeLimit的默认值是98%，也就是说如果大于等于98%的时间都用花在GC上，则会抛出OutOfMemoryError。GCHeapFreeLimit是回收后可用堆的大小，默认值是2%，也就是说只要有多余2%的内存可用就认为此次gc是成功的。如果GCTimeLimit设置过大或者GCHeapFreeLimit设置过小那么就会造成GC的庞式骗局，不停地进行垃圾回收。</p>

<h2><a name='四. Java7、8带来的一些变化'></a>四. Java7、8带来的一些变化</h2>

<ul>
<li>Java7带来的内存方面的一个很大的改变就是String常量池从Perm区移动到了Heap中。调用String的intern方法时，如果存在堆中的对象，则会直接保存对象的引用，而不会重新创建对象。</li>
<li>Java7正式引入G1垃圾收集器用于替换CMS。</li>
<li>Java8中，取消掉了方法区(永久代)，使用“元空间”替代，元空间只与系统内存相关。</li>
<li>Java 8 update 20所引入的一个很棒的优化就是G1回收器中的字符串去重（String deduplication）。由于字符串(包括它们内部的char[]数组）占用了大多数的堆空间，这项新的优化旨在使得G1回收器能识别出堆中那些重复出现的字符串并将它们指向同一个内部的char[]数组，以避免同一个字符串的多份拷贝，那样堆的使用效率会变得很低。可以使用-XX:+UseStringDeduplication这个JVM参数来试一下这个特性。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[基于内容和用户画像的个性化推荐]]></title>
    <link href="http://www.rowkey.me/blog/2016/04/07/up-recommend/"/>
    <updated>2016-04-07T14:59:44+08:00</updated>
    <id>http://www.rowkey.me/blog/2016/04/07/up-recommend</id>
    <content type="html"><![CDATA[<p>目前比较流行的个性化推荐算法有以下几种：</p>

<ul>
<li>基于内容的推荐：根据内容本身的属性(特征向量)所作的推荐。</li>
<li>基于关联规则的推荐：“啤酒与尿布”的方式，是一种动态的推荐，能够实时对用户的行为作出推荐。是基于物品之间的特征关联性所做的推荐，在某种情况下会退化为物品协同过滤推荐。</li>
<li>协同过滤推荐：与基于关联规则的推荐相比是一种静态方式的推荐，是根据用户已有的历史行为作分析的基础上做的推荐。可分为物品协同过滤、用户协同过滤、基于模型的协同过滤。其中，基于模型的协同又可以分为以下几种类型：基于距离的协同过滤；基于矩阵分解的协同过滤，即Latent Factor Model(SVD)或者ALS；基于图模型协同，即Graph，也叫社会网络图模型。</li>
</ul>


<p>本文所讲述的基于内容和用户画像的个性化推荐属于第一种。对于此种推荐，有两个实体：内容和用户，因此需要有一个联系这两者的东西，即为标签。内容转换为标签即为内容特征化，用户则称为用户特征化。对于此种推荐，主要分为以下几个关键部分：</p>

<ul>
<li>标签库</li>
<li>内容特征化</li>
<li>用户特征化</li>
<li>隐语义推荐</li>
</ul>


<p>综合上面讲述的各个部分即可实现一个基于内容和用户画像的个性化推荐系统。如下图所示：</p>

<p><img src="http://www.rowkey.me/images/blog_images/up-recommend.png" alt="uc_interest" /></p>

<!--more-->


<h3>标签库</h3>

<p>标签是联系用户与物品、内容以及物品、内容之间的纽带，也是反应用户兴趣的重要数据源。标签库的最终用途在于对用户进行行为、属性标记。是将其他实体转换为计算机可以理解的语言关键的一步。</p>

<p>标签库则是对标签进行聚合的系统，包括对标签的管理、更新等。</p>

<p>一般来说，标签是以层级的形式组织的。可以有一级维度、二级维度等。</p>

<p>标签的来源主要有：</p>

<ul>
<li>已有内容的标签</li>
<li>网络抓取流行标签</li>
<li>对运营的内容进行关键词提取</li>
</ul>


<p>对于内容的关键词提取，使用<a href="https://github.com/fxsjy/jieba">结巴分词</a> + <a href="http://www.ruanyifeng.com/blog/2013/03/tf-idf.html">TFIDF</a>即可。此外，也可以使用<a href="http://www.tuicool.com/articles/UZ77Z3">TextRank</a>来提取内容关键词。</p>

<p>这里需要注意的一点是对于关联标签的处理，比如用户的标签是足球，而内容的标签是德甲、英超，那么用户和内容是无法联系在一起的。最简单的方式是人工设置关联标签，此外也可以使用word2vec一类工具对标签做聚类处理，构建主题模型，将德甲、英超聚类到<strong>足球</strong>下面。</p>

<h3>内容特征化</h3>

<p>内容特征化即给内容打标签。目前有两种方式：</p>

<ul>
<li>人工打标签</li>
<li>机器自动打标签</li>
</ul>


<p>针对机器自动打标签，需要采取机器学习的相关算法来实现，即针对一系列给定的标签，给内容选取其中匹配度最高的几个标签。这不同于通常的分类和聚类算法。可以采取使用分词 + <a href="http://www.cnblogs.com/wowarsenal/p/3293586.html">Word2Vec</a>来实现，过程如下：</p>

<ul>
<li>将文本语料进行分词，以空格,tab隔开都可以，使用结巴分词。</li>
<li>使用word2vec训练词的相似度模型。</li>
<li>使用tfidf提取内容的关键词A,B,C。</li>
<li>遍历每一个标签，计算关键词与此标签的相似度之和。</li>
<li>取出TopN相似度最高的标签即为此内容的标签。</li>
</ul>


<p>此外，可以使用文本主题挖掘相关技术，对内容进行特征化。这也分为两种情况:</p>

<ol>
<li>通用情况下，只是为了效果优化的特征提取，那么可以使用非监督学习的主题模型算法。如LSA、PLSI和GaP模型或者LDA模型。</li>
<li>在和业务强相关时，需要在业务特定的标签体系下给内容打上适合的标签。这时候需要使用的是监督学习的主题模型。如sLDA、HSLDA等。</li>
</ol>


<h3>用户特征化</h3>

<p>用户特征化即为用户打标签。通过用户的行为日志和一定的模型算法得到用户的每个标签的权重。</p>

<ul>
<li>用户对内容的行为：点赞、不感兴趣、点击、浏览。对用户的反馈行为如<strong>点赞</strong>赋予权值1，<strong>不感兴趣</strong>赋予-1；对于用户的浏览行为，则可使用<strong>点击/浏览</strong>作为权值。</li>
<li>对内容发生的行为可以认为对此内容所带的标签的行为。</li>
<li>用户的兴趣是时间衰减的，即离当前时间越远的兴趣比重越低。时间衰减函数使用<strong>1/[log(t)+1]</strong>, t为事件发生的时间距离当前时间的大小。</li>
<li>要考虑到热门内容会干预用户的标签，需要对热门内容进行降权。使用<strong>click/pv</strong>作为用户浏览行为权值即可达到此目的。</li>
<li>此外，还需要考虑噪声的干扰，如标题党等。</li>
</ul>


<p>另，在非业务强相关的情况下，还可以考虑使用LSA主题模型等矩阵分解的方式对用户进行标签化。</p>

<h3>隐语义推荐</h3>

<p>有了内容特征和用户特征，可以使用<a href="http://blog.csdn.net/harryhuang1990/article/details/9924377">隐语义模型</a>进行推荐。这里可以使用其简化形式，以达到实时计算的目的。</p>

<p>用户对于某一个内容的兴趣度(可以认为是CTR)：</p>

<p><img src="http://www.rowkey.me/images/blog_images/uc_interest.jpg" alt="uc_interest" /></p>

<p>其中i=1&hellip;N是内容c具有的标签，m(ci)指的内容c和标签i的关联度(可以简单认为是1),n(ui)指的是用户u的标签i的权重值,当用户不具有此标签时n(ui)=0，q&copy;指的是内容c的质量，可以使用点击率(click/pv)表示。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据杂谈]]></title>
    <link href="http://www.rowkey.me/blog/2016/02/23/data-talk/"/>
    <updated>2016-02-23T18:44:34+08:00</updated>
    <id>http://www.rowkey.me/blog/2016/02/23/data-talk</id>
    <content type="html"><![CDATA[<h2>目录</h2>

<ul>
<li><a href="#%E5%89%8D%E8%A8%80">前言</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE">数据</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E7%B3%BB%E7%BB%9F">数据系统</a></li>
<li><a href="#%E5%A4%A7%E6%95%B0%E6%8D%AE">大数据</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1">数据统计</a></li>
<li><a href="#%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90">个性化推荐</a></li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">参考资料</a></li>
</ul>


<h2><a name='前言'></a>前言</h2>

<p>记得几年前，曾经有人预测过未来最流行的三大技术：大数据、高并发、数据挖掘。到现在来看，这三种技术的确也随着这几年互联网的发展变得越发成熟和可靠。掌握这三种技术的人，不管是求职还是创业，都属于香饽饽。一个很深的印象就是当年研究生毕业的时候，专业是数据挖掘、大数据的学生都比较受各种企业的青睐，不管他是不是真的掌握了这些东西。虽然我对大部分高校的相关专业持怀疑态度，但是却也不得不承认，这些专业的确改变了很多东西，也给很多学生镀上了一层金。</p>

<p>自己一直从事的是Java EE中间件、基础架构等方面的研发工作，对数据这一块只是略知皮毛，在前东家的时候我也没有机会接触数据平台。但由于现公司业务的原因，却不得不去触碰这一块，到目前为止也就仅仅半年时间（其间穿插各种协调、管理的杂事）。因此，数据相关的东西对我来说完全是一个新的领域，算是离开了自己的舒适区。不过，逃离舒适区这个想想也挺兴奋的。</p>

<!--more-->


<h2><a name='数据'></a>数据</h2>

<h3>什么是数据？</h3>

<p>最近有一本很火的书叫《精益数据分析》，其核心的一个观点就是：需要用数据驱动产品和公司的发展，而不能靠直觉或者拍脑袋。可见，数据是多么的重要。在一个产品的生命周期中，会产生很多数据：用户信息、用户行为信息、ugc数据等等。这些数据表现形式可以为文字、图片、日志、视频、音频等等。</p>

<p>从技术角度来讲，数据一般分为结构化数据、半结构化数据和非结构化数据。</p>

<ul>
<li>结构化数据：指的是行数据库可以存储的，数据具有相同的字段，以及相同的存储大小,可以用二维表的逻辑结构来表达实现。</li>
<li>半结构化数据：半结构化数据，指的整体上是结构化数据形式，但字段数目不定，数据结构和内容混杂在一起。</li>
<li>非结构化数据：不方便用二维表描述的数据，如各种文档、图片、音/视频等。</li>
</ul>


<h3>能用来干什么?-数据挖掘</h3>

<p>说到数据的作用，不得不提数据分析师这个职位。此职位一般来说倾向的是数学相关专业人士，使用数据来指导产品、运营、市场等工作，是公司中使用数据最多的人。在公司中，市场运营销售这几个部门也都是和数据关系很密切的。市场需要参考数据分析哪一个渠道推广效果更好，运营部门需要根据数据分析什么内容更能提高产品的活跃度，销售部门则需要数据反映公司的收入情况。当然，除了这些，数据挖掘就是另一个很重要的使用数据的方面了，可以使用数据对用户进行行为分析，从而挖掘用户的兴趣，最终达到精准推荐、精准营销的目的。</p>

<p>概括来看，数据的作用就是数据挖掘，就是试图从海量数据中找出有用的知识，也可以称为“知识发现”。数据挖掘的支撑技术主要包含统计学以及机器学习两方面。从这个角度来看，数据主要有以下两点作用：</p>

<ul>
<li>数据统计：通过对数据的统计计算出一些和产品、用户相关的指标，从而指导产品、市场、运营、销售工作。</li>
<li>机器学习：使用相关技术让机器通过已有的数据学习到新的有用的知识。比如：从已有的用户行为数据分析得到用户的兴趣、爱好等信息，从而进一步实现用户个性化推荐。个性化推荐也是机器学习目前使用数据最为广泛的一点。</li>
</ul>


<h3>数据库&amp;&amp;数据仓库</h3>

<p>有了数据，就需要有存放数据的地方。数据库和数据仓库即存放数据库的两种形式。两者在本质上没有区别，都是为了存储数据。</p>

<ul>
<li><p>数据库：面向业务设计，一般针对的是在线业务，存储的是在线业务数据。如：Oracle、DB2、MySQL、Sybase、MS SQL Server等。可以分为：关系型数据库和NoSql数据库，其中后者又可分为KV数据库、文档型数据库、列数据库。</p></li>
<li><p>数据仓库：是数据库概念的升级，面向分析，存储的是历史数据。从数据量来说，数据仓库要比数据库更庞大得多。主要用于数据挖掘和数据分析，代表软件为Hive。</p></li>
</ul>


<p>ETL: 数据仓库很多时候是需要从其他地方传输数据到数据仓库，这个过程就是ETL：extract-抽取、transform-转换、load-加载。</p>

<h3>数据的生命周期</h3>

<p>无论是历史数据还是线上数据，都是有生命周期的。比如，对于一个产品的用户活跃度统计业务，最近半年的数据是热点数据，访问较频繁；而随着时间的推移，慢慢的这些数据不再被频繁关注，变为了一般数据；再随着时间的推移，总有一天这些数据不再会被关注就成为了冷数据。</p>

<p>热点数据→一般数据→冷数据，这就是数据的一个生命周期，对于不同的生命周期，所需要的技术选型也应该不一样。</p>

<h2><a name='数据系统'></a>数据系统</h2>

<p>不管是数据统计还是数据挖掘，构建一个数据系统都是做好这些的前提。一般来说，构建一个完备的数据系统有以下几点：</p>

<ol>
<li><p>数据采集</p>

<p> 无论是移动端还是web上，要做好数据采集集最重要的一点就是埋点。也就是要在你需要采集数据的地方做一个标记，向服务端发起一个日志请求。当然，对于服务端能够通过业务逻辑获取的内容，原则上不要打点。比如，统计某一篇新闻的阅读数目、点赞数，这些行为其实在用户打开此新闻、点赞时已经发起了服务端请求，不需要再埋一个点；此外，统计用户数目这种，在用户数据库中就可以计算出来，也不需要埋点。埋点主要针对的是通过产品的业务逻辑无法获取到的一些数据，如一个站点中某一个模块的pv、uv等。</p>

<p> 埋点后向服务端发起日志请求，这些请求在用户量规模并不很大的架构设计中直接实时计算数据入库即可，但是在用户请求量很大的情况下，这种设计是有问题的，会增加业务请求的压力，从而影响线上服务，因此好的设计应该是数据请求只形成一条日志（一般通过nginx日志实现）。因此，这里很关键的一点就是如何将这些日志收集起来进行处理。目前常用的技术有flume、Scribe、Chukwa等。其中，flume是目前比较成熟且应用比较广泛的方案。</p>

<p> 由于从数据源到来的数据并不一定是我们处理需要的数据或者数据格式，因此这里还有数据的清洗过程，包括分析，验证，清洗，转换，去重，</p></li>
<li><p>数据队列</p>

<p> 数据采集之后需要通过数据队列传输，这里的队列主要起的是缓冲作用以及其他非采集数据源的输入(比如某一业务逻辑产生了一条统计报文，可以直接写入队列中)，可以采取本地队列或者分布式队列。目前，比较成熟的队列有kafka、rabbitMQ等。其中，在数据统计领域kafka是应用比较广泛的。</p></li>
<li><p>数据处理</p>

<p> 对于采集到的数据，很多是需要计算才能得到需要的统计结果的。这时候就牵扯到了计算模型。这里分为离线计算和实时计算两种模型。离线计算针对实时来讲，就是非实时的，可以定时调度进行计算的，一般来说是耗时比较长，对结果需求没那么实时的业务场景，适合非线上业务；实时计算则是需要在数据一到达就开始进行计算、处理的，适合对实时性要求高的一些业务场景，比如广告的实时结算等。</p></li>
<li><p>数据存储</p>

<p> 服务端在数据统计中一个关键的功能是对采集到的内容进行存储。对于中小规模的数据，使用mysql等传统数据库即可应对，大一点规模采用分表、分库也能应对。再大一点的那就只能祭出大数据数据库了。此外，数据的存储结构也需要慎重考虑，尤其是在应对多维度查询的时候，不合理的数据结构设计会导致低下的查询效率和冗余的存储空间。</p></li>
<li><p>数据可视化</p>

<p> 数据存储的下一步是要把数据展示出来，也就是数据可视化。通常情况下，导出excel表格是一种形式，此外，web端/移动端甚至pc端也需要展示数据的话，就引出了数据可视化技术，尤其是在大数据量情况下如何更加高效快速地展示数据。</p></li>
</ol>


<p>数据采集+数据队列+数据处理+数据存储+数据可视化即组成了一个完整的数据系统。而从本质上来看，数据系统=数据+查询，万变不离其宗。</p>

<p>对于一般规模的产品，数据其实远远没有达到需要使用大数据技术的地步。使用传统的收集数据→定时调度程序计算，存储到mysql中即可解决。如果有大的并发请求，那么使用数据队列做缓冲。当数据规模大到一定规模时，例如mysql数据库在分表分库的情况下，单表数据量还是达到了千万的规模、单机存储依然不够或者单机计算已经慢到无法容忍。应对这种情况，就需要分布式技术出场了。</p>

<p>说到这里，借用《计算广告》一书中所讲，对于数据分为三种：</p>

<ul>
<li>小规模数据：此种数据可以通过采样部分数据即可反映出数据的特征。这时候，根本无需什么大数据技术，单机规模的传统数据系统架构即可应对这种场景。</li>
<li>中等规模数据：小规模数据无法反应数据特征，当数据规模达到一定规模时，再增大特征趋向于平稳，那么此时也无需大数据技术的出场。</li>
<li>大规模数据：不能通过采样来反应数据特征，必须全量采集数据才能获取到数据特征。此时，就需要大数据技术来解决问题。</li>
</ul>


<p>其中，大规模数据就不是一般架构可以解决的了的了。</p>

<h2><a name='大数据'></a>大数据</h2>

<p>麦肯锡的《大数据：创新、竞争和生产力的下一个前沿领域》中对大数据的定义：</p>

<pre>
大数据指的是规模超过现有数据库工具获取、存储、管理和分析能力的数据集，并同时强调并不是超过某个特定数量级的数据集才是大数据。
</pre>


<p></p>

<p>大数据系统通常被认为具有数据的五个主要特征，通常称为数据的5Vs。分别是大规模，多样性，高效性、准确性和价值性。</p>

<h3>相关技术</h3>

<p>大数据是一个很宽泛的概念。当单机无法处理数据时，就有了大数据。而应对各种不同的业务场景，诞生了很多不同的软件。完成一个功能完备的系统需要多个软件的组合。</p>

<ol>
<li><p>文件/数据存储</p>

<p> 传统的文件存储都是单机的，不能横跨不同的机器，一般会使用raid做安全冗余保障。但是还是无法从根本上解决问题。HDFS（Hadoop Distributed FileSystem）则是为了应对这种业务场景产生的，其基本原理来自于google的gfs，让大量的数据可以横跨成千上百万台机器。但是对用户来说，看到的文件和单机没任何区别，已经屏蔽掉了底层细节。</p>

<p> 除了文件存储，还有数据的存储，即数据库。传统的mysql等数据库，在存储结构化、小规模数据的时候可以妥妥应对。但当需要存储半结构化或者非结构化数据，或者用分表、分库来解决存储性能、空间问题带来了复杂的管理、join时，就需要一种更好的数据库的出现。大数据领域的Hbase就是为了这种场景产生的，其原理是google的BigTable。当然，hbase底层还是依赖于hdfs，是一个针对半结构化、非结构化、稀疏的数据的数据库。</p>

<p> 此外，hbase和hdfs相比起mysql这种毫秒级数据库，其响应速度是很慢的。如果线上业务场景需要使用这些数据，那么这时候就需要更好的数据库的出现。elasticserach就是其中的佼佼者，当然，使用这种基于索引、高效的查询数据库，并不建议存储全量数据(除非你钱有的是)。一般情况下，存储热点数据即可。</p></li>
<li><p>离线数据处理</p>

<p> 大数据的处理是非常关键的一个环节。当单机的处理程序无法在期望的时间内处理完数据时，就需要考虑使用分布式技术了。于是就出现了MapReduce、Tez、Spark这些技术。MapReduce是第一代计算引擎，Tez和Spark是第二代。MapReduce的设计，采用了很简化的计算模型，只有Map和Reduce两个计算过程（中间用Shuffle串联），用这个模型，已经可以处理大数据领域很大一部分问题了。但是，MR模型很简单，但也很笨重，有不少缺点，比如：编程模型非常复杂；计算过程磁盘IO过多。于是催生出了第二代数据处理技术，Tez、Spark这些鉴于MR模型的缺点，引入了内存cache之类新的feature，让Map和Reduce之间的界限更模糊，数据交换更灵活，更少的磁盘读写，以便更方便地描述复杂算法，取得更高的吞吐量。</p>

<p> 如上面所说，编写MR的编程复杂度非常高，于是就产生了Hive、Pig，在MR上面又抽象了一层更高级的语法出来，大大简化了MR的编程复杂度。其中以Hive为代表是Sql on xx的一个典型应用。之所以使用sql，一方面是容易编写、容易维护；另一方面SQL可以让没有编程技能的诸如数据分析师都可以不依赖工程师就可以使用数据。但由于一开始的hive还是基于MR之上的，因此，其运算速度还是受到不少人的诟病。于是Hive on Tez / Spark和SparkSQL也出现了。它们都旨在用新一代通用计算引擎Tez或者Spark来跑SQL，这样就避免了基于MR带来的运算瓶颈。</p>

<p> 对于程序的离线数据处理，hive一般情况下都能够满足需求。但是对于数据分析师的数据分析需求来说，这速度就真的有点龟速了。因此为了应对数据分析的需求，Impala、Presto、Drill这些交互式sql引擎应运而生。这些系统的唯一目标就是快，能够让用户更快速地处理SQL任务，因此牺牲了通用性稳定性等特性。</p>

<p> 一个典型的数据仓库系统可以满足中低速数据处理的需求：底层HDFS，之上是MR、Tez、Spark,再上面则是Hive、Pig；此外，直接跑在HDFS上的Presto、Impala等也是另一种方案。</p>

<p> 由于是离线计算，因此是需要一个任务调度工具来定时调度计算过程的。比较流行的一个任务调度工具是azkaban，是一个基于工作流的调度软件，在一定程度上能够满足目前的离线调度需求。</p></li>
<li><p>实时计算</p>

<p> 上面说的都是数据仓库以及离线处理需求，也是低速数据处理需求。对于高速的数据处理，则需要引出实时计算的概念，也叫流式计算。目前，storm是比较成熟和流行的流式计算技术，spark streaming则是另外一种基于批量计算的流式计算技术。所谓流式计算，就是指数据过来的时候立刻进行处理，基本无延迟，但是不够灵活，计算过的数据也不能回放，因此也无法替代上面说的数据仓库和离线计算技术。</p></li>
<li><p>资源调度</p>

<p> 综上的所有东西都在同一个集群上运行，需要达到一个有序工作的状况。因此，需要一个资源调度系统来调度这些工作，MR2.0带来的yarn就是负责此工作的一个框架。目前，docker on yarn，storm on yarn等on yarn技术的出现都得益于此框架，大大提高了大数据集群的资源使用率。此外，mesos也是另一种资源调度框架。</p></li>
<li><p>协调服务</p>

<p> 一个分布式系统能够有条不紊的运行离不开协调服务的功劳。不管是hadoop还是storm、kakfa等，都是需要通过zookeeper进行协调的。zookeeper在分布式服务中扮演的角色就类似其字面意思-动物园管理员，而大数据的各个组件就类似动物园中的动物们。</p></li>
<li><p>集群监控</p>

<p> 集群的稳定性对于一个数据系统是至关重要的。因此，集群监控技术也是需要重点考虑的一点。目前，ganglia是对hadoop进行监控一个较好的工具。除了hadoop之外，ganglia也可以对kafka、zookeeper、storm等集群进行监控。当然，只要支持jmx，任何集群都是可以通过ganglia进行监控的。</p></li>
<li><p>数据可视化</p>

<p> 最近几年，数据可视化是一个很火的概念。尤其是大数据的可视化，考虑到高效、速度以及体验等等问题，并非是个很简单的事情。目前，百度开源的echarts是比较好的一个可视化前端解决方案，在大数据可视化方面支持的也比较好。</p></li>
</ol>


<p>《大数据：可扩展实时系统的原理和最佳实践》一书的作者将big data相关的开源项目做了以下分类：</p>

<ol>
<li>批量计算系统：延时较高、吞吐量大，如Hadoop。</li>
<li>序列化框架：为对象和字段提供一种模式定义语言，实现传输通信以及不同语言环境之间的转化。如Thrift, Protocol Buffers, 和Avro。</li>
<li>支持任意存取的NoSQL数据库：牺牲了SQL强大的表现力优势，根据应用场景不同仅支持部分操作。按照CAP理论来说，就是牺牲C（一致性）或A（可用性）来实现AP或CP。如Cassandra, HBase, MongoDB,Voldemort, Riak, CouchDB等。</li>
<li>消息/排队系统：保证进程之间以容错和异步的方式传递消息，在实时处理系统中非常重要。如Kestrel。</li>
<li>实时计算系统：高吞吐、低延时的流处理系统。如Storm。</li>
</ol>


<h3>一般架构</h3>

<p>下图为一个典型的大数据系统架构：</p>

<p><img src="http://www.rowkey.me/images/blog_images/data-arch.png" alt="data-arch" /></p>

<p>这里还需要提到的是Lambda架构这个概念。Lambda架构是由Storm的作者Nathan Marz提出的一个实时大数据处理框架。目标是设计出一个能满足实时大数据系统关键特性的架构，包括有：高容错、低延时和可扩展等。Lambda架构整合离线计算和实时计算，融合不可变性（Immunability），读写分离和复杂性隔离等一系列架构原则，可集成Hadoop，Kafka，Storm，Spark，Hbase等各类大数据组件。</p>

<p><img src="http://www.rowkey.me/images/blog_images/lambda-arch.png" alt="lambda-arch" /></p>

<p>Lambda架构是由三层组成：批处理层、服务层和速度层，总体可由query = function(alldata)这个公式来表示。</p>

<ul>
<li>批处理层：Hadoop是理想的批处理层工具。特点是延时较高、高吞吐量，并且是append-only（没有delete和update的概念）的。包括对全部数据集的预计算。</li>
<li>服务层：用于加载和显示数据库中的批处理视图，以便用户能够查询。可以使用Impala作为这一层的工具（使用Hive元数据指向HDFS中的一个表）。</li>
<li>速度层：主要处理新数据和服务层更新造成的高延迟补偿，利用流处理系统如 (Storm, Spark)计算实时视图(HBase)。这些视图有效期一直到它们已经能通过批处理和服务层获得时为止。</li>
</ul>


<p>为了获得一个完整结果，批处理和实时视图都必须被同时查询和融合(实时代表新数据)。</p>

<p>当然，架构的引入是不能照本宣科的，还是需要根据实际情况进行调整，以更好地适应业务场景。</p>

<h2><a name='数据统计'></a>数据统计</h2>

<p>数据统计是数据首当其冲的一个作用。关于数据统计，有以下几个关键点：</p>

<ol>
<li>数据统计是业务导向的，需要和数据分析师、运营、市场等需求方做好充分的沟通，且很关键的一点要区分清楚哪些是真正的需求，哪些仅仅是临时需求，对于前者需要以对待产品的态度去对待，后者则一过性产生结果即可。</li>
<li>数据统计一般来说都是pv、uv这些累加指标。使用数据库自带的累加器即可，如hbase/redis的incr。</li>
<li>数据统计在牵扯到用户、IP时，有些业务是需要去重的。去重的方案有bitmap、bloomfilter等，其中，redis的hyperloglog在容许一定误差的情况下使用比较广泛。</li>
<li>用户统计中的用户质量模型是比较复杂的一个地方。这个地方需要一定的建模，才能做到更好的判断一个用户的质量。通常，把一个新增用户一周内以及一周后的活跃情况作为这个用户质量的判别标准。</li>
</ol>


<h2><a name='个性化推荐'></a>个性化推荐</h2>

<p>由于个性化推荐是“机器学习”的典型应用，因此这里首先要讲一下“机器学习”。</p>

<p>机器学习是为了让机器具有人的学习能力，目的是建模隐藏的数据结构，然后做识别、预测、分类等。大多数情况下，这相当于将一组数据传递给算法，并由算法判断出与这些数据的属性相关的信息，借助这些信息可以预测出未来有可能出现的其他数据。对于机器学习广泛的一个定义是“利用经验来改善计算机系统自身的性能”，而计算机中的经验都是以数据的形式存在的。机器学习的一个典型过程就是机器利用它所认定的出现于数据中的重要特征对数据进行“训练”，并借此得到一个模型。</p>

<p>此外，与机器学习相关的还有几个名词会被混淆或者概念不清。</p>

<ul>
<li>集体智慧：简称集智，它是一种共享的或群体的智能。百度百科、维基百科、百度知道、猪八戒网等都是目前使用集体智慧的一种形式；数据挖掘、机器学习同样需要大量群体的数据才能做出计算，是使用集体智慧的另一种形式。</li>
<li>数据挖掘：数据挖掘就是试图从海量数据中找出有用的信息。数据挖掘支撑技术包含了机器学习、数据库、统计学等。其中，数据库提供数据管理技术，机器学习和统计学提供了数据分析技术。但是由于机器学习并不以大数据作为处理对象，因此数据挖掘要对算法进行改造，使得算法性能和空间占用达到实用的地步。</li>
<li>模式识别：模式识别是一种目的。传统的模式识别的方法一般分为两种：统计方法和句法方法。句法分析一般是不可学习的，而统计分析则是发展了不少机器学习的方法。因此机器学习给模式识别提供了数据分析技术。当然，也就是因为几乎所有的非随机数据都会包含这样或者那样的“模式(pattern)”，才使得机器学习的预测是可能的。</li>
</ul>


<p>总之，机器学习也是使用数据的一个很关键的领域，典型应用有个性化推荐、CTR预估、模式识别等。牵扯到的算法、技术非常多。如此部分开头所说，其中的个性化推荐是应用最广泛的领域，用到了很多机器学习相关技术。</p>

<p>从本质上看，个性化推荐和大家接触很普遍的搜索引擎是一样的，同样是为了解决信息过载的问题。搜索引擎某种意义上也是一个个性化推荐系统，其输入特征是从搜索关键字可以直接得到的。而个性化推荐中，输入特征则是需要使用机器学习相关技术才能得到。</p>

<p>个性化推荐系统一般由日志系统、推荐算法、内容展示UI三部分组成。</p>

<ul>
<li>日志系统：这是推荐系统的输入源，是一个推荐系统所有信息的源头。</li>
<li>推荐算法：这是推荐系统的核心，根据输入数据得出最终的推荐结果的具体过程就在这里。</li>
<li>内容展示UI：对于推荐结果如何展示，也是一个值得权衡的地方。以更好地满足推荐系统的目标，并能更好的收集用户的行为信息等。</li>
</ul>


<p>其中，个性化推荐中最为核心的推荐算法，目前比较流行的有以下几种：</p>

<ul>
<li>基于内容的推荐：根据内容本身的属性(特征向量)所作的推荐。</li>
<li>基于关联规则的推荐：“啤酒与尿布”的方式，是一种动态的推荐，能够实时对用户的行为作出推荐。</li>
<li>协同过滤推荐：与基于关联规则的推荐相比是一种静态方式的推荐，是根据用户已有的历史行为作分析的基础上做的推荐。可分为物品协同过滤、用户协同过滤、基于模型的协同过滤。其中，基于模型的协同又可以分为以下几种类型：基于距离的协同过滤；基于矩阵分解的协同过滤，即Latent Factor Model(SVD)；基于图模型协同，即Graph，也叫社会网络图模型。</li>
</ul>


<p>个性化推荐系统的典型架构如下图所示：</p>

<p><img src="http://www.rowkey.me/images/blog_images/recommend-sys/recommend-sys-arch.png" alt="recommend-sys" /></p>

<p>在线业务系统的日志接入数据高速公路，再由数据高速公路迅速运转到离线数据处理平台和在线流计算平台；离线数据处理平台周期性地以批处理方式加工过去一段时间的数据，得到人群标签和其他模型参数，存放在高速缓存中，供在线业务系统使用，与此同时，在线流计算平台实时对线上的日志数据做处理，对离线计算出的数据进行补充、修正等；在线业务系统综合离线特征和在线特征使用一定的逻辑得到输出供业务使用，产生的日志流入数据高速公路。</p>

<p>基于此框架，个性化推荐系统的典型流程如下：</p>

<p><img src="http://www.rowkey.me/images/blog_images/recommend-sys/recommend-sys.png" alt="recommend-sys" /></p>

<p>其他更为详细的，个性化推荐牵扯到的算法、细节还有很多，留待后续推荐系统相关文章中再谈。</p>

<h2><a name='总结'></a>总结</h2>

<p>无论是互联网还是其他领域的产品，数据的作用正变得越来越重要。综合来看，数据统计和机器学习/个性化推荐是目前最关键的使用数据的领域。基于具体的需求，搭建合适的数据系统是解决问题的关键。其中，大数据是在应对大规模数据的情况下合适的技术选型架构。</p>

<h2><a name='参考资料'></a>参考资料</h2>

<ul>
<li><a href="https://book.douban.com/subject/26596778/">《计算广告》</a></li>
<li><a href="https://book.douban.com/subject/10769749/">《推荐系统实践》</a></li>
<li><a href="https://www.zhihu.com/question/27974418/answer/38965760">知乎@Xiaoyu Ma的有关回答</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2016年的几点规划]]></title>
    <link href="http://www.rowkey.me/blog/2016/02/18/2016plan/"/>
    <updated>2016-02-18T20:31:11+08:00</updated>
    <id>http://www.rowkey.me/blog/2016/02/18/2016plan</id>
    <content type="html"><![CDATA[<p>明天就要开始新的一年正式的上班了，回想一下过去的2015年，对于自己来说，虽然有不少的收获和成长，但还是令自己比较不满意的。由于某些原因，自己的学习进度以及工作情况都受到了很大的影响，并没有达到年初的期望。不过，至少没有浑浑噩噩的一年又一年，也算不错了。^_^</p>

<p><strong>工作学习方面：</strong></p>

<ol>
<li><p>大数据</p>

<p> 公司业务的增长让以前的架构达到了瓶颈。大数据技术的引入对于我自己来说算是离开了舒适区。从hadoop、flume、kafka到storm等等都是一个崭新的领域。虽然从本质上来看，大数据和普通的程序是没啥区别的。但是牵扯到分布式，各种需要考虑的东西也就多了起来，也就引出了一个个强大的软件。15年基本上完成了公司的lambda架构，16年需要做的是完善、优化已有的，而需要考虑引入的则包括elasticsearch、spark等大数据技术。</p></li>
<li><p>数据挖掘</p>

<p> 大数据是服务于数据统计的，而数据统计的最终目的一方面是指导市场运营的工作，更重要的一点则是服务于数据挖掘。目前接触的主要是怎样构建用户画像，从而形成一个良好的推荐机制，为用户推荐更多感兴趣的运营内容。15年，完成了用户画像以及初版的推荐机制；16年，需要做的是进一步验证已有系统的效果，考虑引入更好、更成熟的方案，此外在文本内容打标签、分类等方面也需要实现成熟的机器学习方案。</p></li>
<li><p>基础平台</p>

<p> 借鉴已有开源框架，实现了公司的dao框架、redis操作框架、java ee应用性能检测框架、分布式调度框架等。16年需要继续升级基础平台。</p>

<p> 值得一提的是，公司代码版本管理使用的gitbucket，自己在此之上做了不少二次开发，有些提交给了原项目，有些则是仅仅为了应对公司的需求。鉴于此，也接触到了scala的开发，不得不说，scala的学习曲线确实很陡，16年争取要能掌握并熟练运用此语言。</p></li>
<li><p>Github</p>

<p> 在github上写代码，一方面可以提高自己的编码水平，毕竟质量太差的代码，你也怕拿出来丢人；另一方面，github上那么多优秀的项目，只做拿来党是很可耻的，一些好的东西，分享出来帮助更多的同行给自己带来的成就感反过来也能督促自己技术的提升。15年自己开发或者基于原项目二次开发了一些star较多的项目。16年要坚持在github继续贡献更多好的代码。</p></li>
<li><p>技术分享</p>

<p> 在去年的研发招聘过程中，尤其是校招，感受到了目前后端工程师教育的匮乏。对于一个后端工程师的技术体系，先不说学生，不少工作很长时间的人都没有一个清晰的认识。于是自己萌生了写一本后端工程师技术体系书籍的想法，希望能够给选择后端这个方向的人一些指导。到目前为止也写了一些，希望16年至少能出一个初稿。</p>

<p> 此外，自己在开发者头条的<a href="http://toutiao.io/subjects/4944">《后端技术杂谈》</a>专栏，会继续分享自己的技术感悟和总结。一方面，增人玫瑰，手有余香；更重要的一点还是能够督促自己多总结，多思考。</p></li>
</ol>


<p><strong>工作学习之外：</strong></p>

<p>今年最大的一点感受：不管其他如何，健康才是一个人最最重要的东西。尤其是对于天天坐在电脑面前的程序员们来说，保持健康就是保证最大的竞争力。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[也谈IO模型]]></title>
    <link href="http://www.rowkey.me/blog/2016/01/18/io-model/"/>
    <updated>2016-01-18T15:41:31+08:00</updated>
    <id>http://www.rowkey.me/blog/2016/01/18/io-model</id>
    <content type="html"><![CDATA[<h2>目录</h2>

<ul>
<li><a href="#%E5%89%8D%E8%A8%80">前言</a></li>
<li><a href="#IO%E6%A8%A1%E5%9E%8B">IO模型</a></li>
<li><a href="#%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B">网络编程模型</a></li>
<li><a href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">参考资料</a></li>
</ul>


<h2><a name='前言'></a>前言</h2>

<p>说到IO模型，都会牵扯到同步、异步、阻塞、非阻塞这几个词。从词的表面上看，很多人都觉得很容易理解。但是细细一想，却总会发现有点摸不着头脑。自己也曾被这几个词弄的迷迷糊糊的，每次看相关资料弄明白了，然后很快又给搞混了。经历过这么几次之后，发现这东西必须得有所总结提炼才不至于再次混为一谈。尤其是最近看到好几篇讲这个的文章，很多都有谬误，很容易把本来就搞不清楚的人弄的更加迷糊。</p>

<p>最适合IO模型的例子应该是咱们平常生活中的去餐馆吃饭这个场景，下文就结合这个来讲解一下经典的几个IO模型。在此之前，先需要说明以下几点：</p>

<ul>
<li>IO有内存IO、网络IO和磁盘IO三种，通常我们说的IO指的是后两者。</li>
<li>阻塞和非阻塞，是函数/方法的实现方式，即在数据就绪之前是立刻返回还是等待。</li>
<li>以文件IO为例,一个IO读过程是文件数据从磁盘→内核缓冲区→用户内存的过程。同步与异步的区别主要在于数据从内核缓冲区→用户内存这个过程需不需要用户进程等待。(网络IO把磁盘换做网卡即可)</li>
</ul>


<!--more-->


<h2><a name='IO模型'></a>IO模型</h2>

<h3>同步阻塞</h3>

<p>去餐馆吃饭，点一个自己最爱吃的盖浇饭，然后在原地等着一直到盖浇饭做好，自己端到餐桌就餐。这就是典型的同步阻塞。当厨师给你做饭的时候，你需要一直在那里等着。</p>

<p>网络编程中，读取客户端的数据需要调用recvfrom。在默认情况下，这个调用会一直阻塞直到数据接收完毕，就是一个同步阻塞的IO方式。这也是最简单的IO模型，在通常fd较少、就绪很快的情况下使用是没有问题的。</p>

<p><img src="http://www.rowkey.me/images/blog_images/io/bio.png" alt="bio" /></p>

<h3>同步非阻塞</h3>

<p>接着上面的例子，你每次点完饭就在那里等着，突然有一天你发现自己真傻。于是，你点完之后，就回桌子那里坐着，然后估计差不多了，就问老板饭好了没，如果好了就去端，没好的话就等一会再去问，依次循环直到饭做好。这就是同步非阻塞。</p>

<p>这种方式在编程中对socket设置O_NONBLOCK即可。但此方式仅仅针对网络IO有效，对磁盘IO并没有作用。因为本地文件IO就没有被认为是阻塞，我们所说的网络IO的阻塞是因为网路IO有无限阻塞的可能，而本地文件除非是被锁住，否则是不可能无限阻塞的，因此只有锁这种情况下，O_NONBLOCK才会有作用。而且，磁盘IO时要么数据在内核缓冲区中直接可以返回，要么需要调用物理设备去读取，这时候进程的其他工作都需要等待。因此，后续的IO复用和信号驱动IO对文件IO也是没有意义的。</p>

<p><img src="http://www.rowkey.me/images/blog_images/io/nio.png" alt="bio" /></p>

<p>此外，需要说明的一点是nginx和node中对于本地文件的IO是用线程的方式模拟非阻塞的效果的，而对于静态文件的io，使用zero copy(例如sendfile)的效率是非常高的。</p>

<h3>IO复用</h3>

<p>接着上面的列子，你点一份饭然后循环的去问好没好显然有点得不偿失，还不如就等在那里直到准备好，但是当你点了好几样饭菜的时候，你每次都去问一下所有饭菜的状态(未做好/已做好)肯定比你每次阻塞在那里等着好多了。当然，你问的时候是需要阻塞的，一直到有准备好的饭菜或者你等的不耐烦(超时)。这就引出了IO复用，也叫多路IO就绪通知。这是一种进程预先告知内核的能力，让内核发现进程指定的一个或多个IO条件就绪了，就通知进程。使得一个进程能在一连串的事件上等待。</p>

<p><img src="http://www.rowkey.me/images/blog_images/io/mulio.png" alt="bio" /></p>

<p>IO复用的实现方式目前主要有select、poll和epoll。</p>

<p>select和poll的原理基本相同：</p>

<ul>
<li>注册待侦听的fd(这里的fd创建时最好使用非阻塞)</li>
<li>每次调用都去检查这些fd的状态，当有一个或者多个fd就绪的时候返回</li>
<li>返回结果中包括已就绪和未就绪的fd</li>
</ul>


<p>相比select，poll解决了单个进程能够打开的文件描述符数量有限制这个问题：select受限于FD_SIZE的限制，如果修改则需要修改这个宏重新编译内核；而poll通过一个pollfd数组向内核传递需要关注的事件，避开了文件描述符数量限制。</p>

<p>此外，select和poll共同具有的一个很大的缺点就是包含大量fd的数组被整体复制于用户态和内核态地址空间之间，开销会随着fd数量增多而线性增大。</p>

<p>select和poll就类似于上面说的就餐方式。但当你每次都去询问时，老板会把所有你点的饭菜都轮询一遍再告诉你情况，当大量饭菜很长时间都不能准备好的情况下是很低效的。于是，老板有些不耐烦了，就让厨师每做好一个菜就通知他。这样每次你再去问的时候，他会直接把已经准备好的菜告诉你，你再去端。这就是事件驱动IO就绪通知的方式-<strong>epoll</strong>。</p>

<p>epoll的出现，解决了select、poll的缺点：</p>

<ul>
<li>基于事件驱动的方式，避免了每次都要把所有fd都扫描一遍。</li>
<li>epoll_wait只返回就绪的fd。</li>
<li>epoll使用nmap内存映射技术避免了内存复制的开销。</li>
<li>epoll的fd数量上限是操作系统的最大文件句柄数目,这个数目一般和内存有关，通常远大于1024。</li>
</ul>


<p>目前，epoll是Linux2.6下最高效的IO复用方式，也是Nginx、Node的IO实现方式。而在freeBSD下，kqueue是另一种类似于epoll的IO复用方式。</p>

<p>此外，对于IO复用还有一个水平触发和边缘触发的概念：</p>

<ul>
<li>水平触发：当就绪的fd未被用户进程处理后，下一次查询依旧会返回，这是select和poll的触发方式。</li>
<li>边缘触发：无论就绪的fd是否被处理，下一次不再返回。理论上性能更高，但是实现相当复杂，并且任何意外的丢失事件都会造成请求处理错误。epoll默认使用水平触发，通过相应选项可以使用边缘触发。</li>
</ul>


<h3>信号驱动</h3>

<p>上文的就餐方式还是需要你每次都去问一下饭菜状况。于是，你再次不耐烦了，就跟老板说，哪个饭菜好了就通知我一声吧。然后就自己坐在桌子那里干自己的事情。更甚者，你可以把手机号留给老板，自己出门，等饭菜好了直接发条短信给你。这就类似信号驱动的IO模型。</p>

<p><img src="http://www.rowkey.me/images/blog_images/io/sigio.png" alt="bio" /></p>

<p>流程如下：</p>

<ul>
<li>开启套接字信号驱动IO功能</li>
<li>系统调用sigaction执行信号处理函数（非阻塞，立刻返回）</li>
<li>数据就绪，生成sigio信号，通过信号回调通知应用来读取数据。</li>
</ul>


<h3>异步非阻塞</h3>

<p>之前的就餐方式，到最后总是需要你自己去把饭菜端到餐桌。这下你也不耐烦了，于是就告诉老板，能不能饭好了直接端到你的面前或者送到你的家里(外卖)。这就是异步非阻塞IO了。</p>

<p><img src="http://www.rowkey.me/images/blog_images/io/aio.png" alt="bio" /></p>

<p>对比信号驱动IO，异步IO的主要区别在于：信号驱动由内核告诉我们何时可以开始一个IO操作(数据在内核缓冲区中)，而异步IO则由内核通知IO操作何时已经完成(数据已经在用户空间中)。</p>

<p>异步IO又叫做事件驱动IO，在Unix中，POSIX1003.1标准为异步方式访问文件定义了一套库函数，定义了AIO的一系列接口。使用aio_read或者aio_write发起异步IO操作。使用aio_error检查正在运行的IO操作的状态。</p>

<h2><a name='网络编程模型'></a>网络编程模型</h2>

<p>上文讲述了UNIX环境的五种IO模型。基于这五种模型，在Java中，随着NIO和NIO2.0(AIO)的引入，一般具有以下几种网络编程模型：</p>

<ul>
<li>BIO</li>
<li>NIO</li>
<li>AIO</li>
</ul>


<h3>BIO</h3>

<p>BIO是一个典型的网络编程模型，是通常我们实现一个服务端程序的过程，步骤如下：</p>

<ul>
<li>主线程accept请求阻塞</li>
<li>请求到达，创建新的线程来处理这个套接字，完成对客户端的响应。</li>
<li>主线程继续accept下一个请求</li>
</ul>


<p>这种模型有一个很大的问题是：当客户端连接增多时，服务端创建的线程也会暴涨，系统性能会急剧下降。因此，在此模型的基础上，类似于
tomcat的bio connector，采用的是线程池来避免对于每一个客户端都创建一个线程。有些地方把这种方式叫做伪异步IO(把请求抛到线程池中异步等待处理)。</p>

<h3>NIO</h3>

<p>JDK1.4开始引入了NIO类库，这里的NIO指的是Non-blcok IO，主要是使用Selector多路复用器来实现。Selector在Linux等主流操作系统上是通过epoll实现的。</p>

<p>NIO的实现流程，类似于select：</p>

<ul>
<li>创建ServerSocketChannel监听客户端连接并绑定监听端口，设置为非阻塞模式。</li>
<li>创建Reactor线程，创建多路复用器(Selector)并启动线程。</li>
<li>将ServerSocketChannel注册到Reactor线程的Selector上。监听accept事件。</li>
<li>Selector在线程run方法中无线循环轮询准备就绪的Key。</li>
<li>Selector监听到新的客户端接入，处理新的请求，完成tcp三次握手，建立物理连接。</li>
<li>将新的客户端连接注册到Selector上，监听读操作。读取客户端发送的网络消息。</li>
<li>客户端发送的数据就绪则读取客户端请求，进行处理。</li>
</ul>


<p>相比BIO，NIO的编程非常复杂。</p>

<h3>AIO</h3>

<p>JDK1.7引入NIO2.0，提供了异步文件通道和异步套接字通道的实现，是真正的异步非阻塞IO, 对应于Unix中的异步IO。</p>

<ul>
<li>创建AsynchronousServerSocketChannel，绑定监听端口</li>
<li>调用AsynchronousServerSocketChannel的accpet方法，传入自己实现的CompletionHandler。包括上一步，都是非阻塞的</li>
<li>连接传入，回调CompletionHandler的completed方法，在里面，调用AsynchronousSocketChannel的read方法，传入负责处理数据的CompletionHandler。</li>
<li>数据就绪，触发负责处理数据的CompletionHandler的completed方法。继续做下一步处理即可。</li>
<li>写入操作类似，也需要传入CompletionHandler。</li>
</ul>


<p>其编程模型相比NIO有了不少的简化。</p>

<h3>对比</h3>

<table>
<thead>
<tr>
<th>.  </th>
<th> 同步阻塞IO </th>
<th> 伪异步IO </th>
<th> NIO </th>
<th> AIO</th>
</tr>
</thead>
<tbody>
<tr>
<td>客户端数目 ：IO线程  </td>
<td> 1 : 1</td>
<td> m : n</td>
<td> m : 1 </td>
<td> m : 0</td>
</tr>
<tr>
<td>IO模型 </td>
<td> 同步阻塞IO </td>
<td> 同步阻塞IO </td>
<td> 同步非阻塞IO</td>
<td> 异步非阻塞IO</td>
</tr>
<tr>
<td>吞吐量 </td>
<td> 低</td>
<td>中</td>
<td>高</td>
<td>高</td>
</tr>
<tr>
<td>编程复杂度 </td>
<td> 简单</td>
<td>简单</td>
<td>非常复杂</td>
<td>复杂</td>
</tr>
</tbody>
</table>


<h2><a name='参考资料'></a>参考资料</h2>

<ul>
<li><a href="http://book.douban.com/subject/3924175/">构建高性能Web站点</a></li>
<li><a href="http://book.douban.com/subject/26373138/">Netty权威指南</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Git操作指南]]></title>
    <link href="http://www.rowkey.me/blog/2016/01/10/git-usage/"/>
    <updated>2016-01-10T10:15:30+08:00</updated>
    <id>http://www.rowkey.me/blog/2016/01/10/git-usage</id>
    <content type="html"><![CDATA[<p><strong><em>ps:本指南会持续更新</em></strong></p>

<p>其实一般情况下，只需要掌握git的几个常用命令即可，但是在使用的过程中难免会遇到各种复杂的需求，这时候经常需要搜索，非常麻烦，故总结了一下自己平常会用到的git操作。</p>

<p><img src="http://www.rowkey.me/images/blog_images/git-process.png" alt="git-process" /></p>

<p>上图所示，使用git的流程一般如此，通常使用图中的六个命令即可。</p>

<!--more-->


<h2>目录</h2>

<ul>
<li><a href="#config">配置</a></li>
<li><a href="#repo">取得项目的 Git 仓库</a></li>
<li><a href="#commit">记录每次更新到仓库</a></li>
<li><a href="#remote">远程仓库的使用</a></li>
<li><a href="#branch">分支的使用</a></li>
<li><a href="#tag">标签的使用</a></li>
<li><a href="#log">日志</a></li>
<li><a href="#revert">撤销</a></li>
<li><a href="#cherrypick">选择某些commits操作</a></li>
<li><a href="#conflict">解决冲突</a></li>
<li><a href="#submodule">Submodule</a></li>
<li><a href="#subtree">Subtree</a></li>
<li><a href="#other">其他</a></li>
</ul>


<h2><a name="config"></a>配置</h2>

<ol>
<li><p>下面的命令将修改/home/[username]/.gitconfig文件，也就是说下面的配置只对每一个ssh的用户可见，所以每个人都需要做。</p>

<ul>
<li><p>提交代码的log里面会显示提交者的信息</p>

<pre><code>  git config --global user.name [username]
  git config --global user.email [email]
</code></pre></li>
<li><p>在git命令中开启颜色显示</p>

<pre><code>  git config --global color.ui true
</code></pre></li>
<li><p>区分文件名大小写</p>

<pre><code>  编辑.git/config -&gt; core.ignorecase = false或者使用git mv oldFileName newFileName
</code></pre></li>
<li><p>兼容不同平台的换行符</p>

<p>  For Windows:</p>

<pre><code>  git config --global core.autocrlf true
</code></pre>

<p>  For Mac:</p>

<pre><code>  git config --global core.autocrlf input
</code></pre>

<p>  同时在ADD之前使用以下命令不再收到关于换行符的提示:</p>

<pre><code>  git config --global core.safecrlf false
</code></pre></li>
<li><p>如果使用HTTP clone遇到提交大小限制，请使用以下命令提高限值</p>

<pre><code>  git config --global http.postBuffer 524288000(bytes)
</code></pre></li>
<li><p>此外，也可以使用以下命令做相应修改</p>

<pre><code>  git config -e --global
</code></pre></li>
</ul>
</li>
<li><p>下面的命令将修改/etc/gitconfig文件，这是全局配置，所以admin来做一次就可以了。</p>

<p> 配置一些git的常用命令alias</p>

<pre><code> sudo git config --system alias.st status     #git st
 sudo git config --system alias.ci commit   #git ci
 sudo git config --system alias.co checkout  #git co
 sudo git config --system alias.br  branch  #git br
</code></pre></li>
<li><p>也可以进入工作根目录，运行git config -e，这样就只会修改工作区的.git/config文件，但是暂时还用不着.</p>

<p> git config文件的override顺序是3>1>2.</p></li>
<li><p>显示配置列表</p>

<pre><code> git config --list
</code></pre></li>
<li><p>配置密钥</p>

<pre><code> ssh-keygen -t rsa -C superhj1987@126.com #生成密钥
 ssh -T git@github.com #测试是否成功
 exec ssh-agent bash
 ssh-add ~/.ssh/private-key-name # 添加私钥到ssh-agent中
 ssh-add -l  #查看到当前计算机中存储的密钥
 ssh-add -d pub_key # 将私钥从ssh-agent删除
</code></pre></li>
</ol>


<h2><a name="repo"></a>取得项目的 Git 仓库</h2>

<p>有两种取得 Git 项目仓库的方法。第一种是在现存的目录下，通过导入所有文件来创建新的 Git 仓库。第二种是从已有的 Git 仓库克隆出一个新的镜像仓库来。</p>

<ol>
<li><p>在工作目录中初始化新仓库</p>

<p> 要对现有的某个项目开始用 Git 管理，只需到此项目所在的目录，执行：</p>

<pre><code> git init 在当前目录新建一个Git代码库
 git init [projectName] 新建一个目录并初始化为Git代码库
</code></pre></li>
<li><p>从现有仓库克隆</p>

<pre><code> git clone git://github.com/superhj1987/test.git
</code></pre>

<p> 这会在当前目录下创建一个名为“test”的目录，其中包含一个 .git 的目录，用于保存下载下来的所有版本记录，然后从中取出最新版本的文件拷贝。</p></li>
</ol>


<h2><a name="commit"></a>记录每次更新到仓库</h2>

<ol>
<li><p>检查当前文件状态</p>

<pre><code> git status
</code></pre></li>
<li><p>跟踪新文件、暂存已修改文件</p>

<p> 使用命令<strong>git add [dirName] [fileName1] [fileName2]</strong>(支持正则)开始跟踪一个新文件/文件夹(包括子文件夹)。实际上只是add file into staged area，并没有提交文件。</p>

<p> 此外：</p>

<pre><code> git add . 添加当前目录的所有文件到暂存区
 git add --a 添加所有文件和目录到暂存区(自己最常用的)
</code></pre></li>
<li><p>忽略未纳入版本管理的某些文件/文件夹</p>

<p> 一般我们总会有些文件无需纳入 Git的管理，也不希望它们总出现在未跟踪文件列表。通常都是些自动生成的文件，比如日志文件，或者编译过程中创建的临时文件等。我们可以创建一个名为 .gitignore的文件，列出要忽略的文件模式。</p>

<p> 文件.gitignore 的格式规范如下：</p>

<ul>
<li>所有空行或者以注释符号 ＃ 开头的行都会被 Git 忽略。</li>
<li>可以使用标准的 glob 模式匹配。 * 匹配模式最后跟反斜杠（/）说明要忽略的是目录。 * 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。</li>
<li>所谓的 glob 模式是指 shell 所使用的简化了的正则表达式。星号（*）匹配零个或多个任意字符；[abc] 匹配任何一个列在方括号中的字符（这个例子要么匹配一个 a，要么匹配一个 b，要么匹配一个 c）；问号（?）只匹配一个任意字符；如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配（比如 [0-9] 表示匹配所有 0 到 9 的数字）。</li>
</ul>


<p> 此外，忽略未纳入版本管理的文件或文件夹的方式还有：</p>

<ul>
<li>可以为自己配置一个全局的ignore文件，位于任何版本库之外：$ git config &ndash;global core.excludesfile ~/.gitignoreglobal</li>
<li>.git/info/exclude文件里设置你自己本地需要排除的文件,不会影响到其他人,也不会提交到版本库中去</li>
</ul>
</li>
<li><p>忽略已经在版本库里的文件/文件夹</p>

<ul>
<li><p>告诉git忽略对已经纳入版本管理的文件a的修改,git会一直忽略此文件直到重新告诉git可以再次跟踪此文件:</p>

<pre><code>  git update-index --assume-unchanged a
</code></pre></li>
<li><p>告诉git恢复跟踪a</p>

<pre><code>  git update-index -—no-assume-unchanged a
</code></pre></li>
<li><p>查看当前被忽略的、已经纳入版本库管理的文件</p>

<pre><code>  git ls-files -v | grep -e "^[hsmrck]"
</code></pre></li>
</ul>
</li>
<li><p>查看已暂存和未暂存的更新、提交之间的差异</p>

<p> git status 的显示比较简单，仅仅是列出了修改过的文件，如果要查看具体修改了什么地方，可以用 git diff 命令。</p>

<ul>
<li>git diff #查看尚未暂存的文件更新了哪些部分</li>
<li>git diff &ndash;cached [file] #看已经暂存起来的文件和上次提交时的快照之间的差异</li>
<li>git diff [branch1] [branch2] #显示两次提交之间的差异</li>
</ul>
</li>
<li><p>提交更新</p>

<p> 每次准备提交前，先用git status看下，是不是都已暂存起来了，然后再运行提交命令git commit提交更新</p>

<ul>
<li>git commit [file1] [file2] 提交会提示输入本次提交说明</li>
<li>git commit -m [messag] 直接附带提交说明</li>
<li>git commit &ndash;amend#修改最后一次提交</li>
<li>git commit -v 提交时显示所有diff信息</li>
<li>git commit &ndash;amend -m [message] 使用一次新的commit，替代上一次提交,如果代码没有任何新变化，则用来改写上一次commit的提交信息</li>
<li>git commit &ndash;amend [file1] [file2] &hellip; 重做上一次commit，并包括指定文件的新变化</li>
</ul>
</li>
<li><p>跳过使用暂存区域</p>

<p> git commit -a 跳过git add步骤直接commit</p></li>
<li><p>移除文件
 要从 Git 中移除某个文件（包括暂存区域和工作目录），就必须要从已跟踪文件清单中移除（确切地说，是从暂存区域移除），然后提交。
可以用 git rm 命令完成此项工作，并连带从工作目录中删除指定的文件，这样以后就不会出现在未跟踪文件清单中了。</p>

<pre><code>  git rm [file1] [file2]
</code></pre>

<p>  最后提交的时候，该文件就不再纳入版本管理了。
  如果删除之前修改过并且已经放到暂存区域的话，则必须要用强制删除选项 -f（即 force的首字母），以防误删除文件后丢失修改的内容。</p>

<p>  另外一种情况是，我们想把文件从 Git 仓库中删除（亦即从暂存区域移除），但仍然希望保留在当前工作目录中。换句话说，仅是从跟踪清单中删除。比如一些 编译文件，不小心纳入仓库后，要移除跟踪但不删除文件，以便稍后在 .gitignore 文件中补上，用 &ndash;cached 选项即可：</p>

<pre><code>  git rm --cached [file]
</code></pre>

<p>  后面可以列出文件或者目录的名字，也可以使用 glob 模式。比方说：</p>

<pre><code>  git rm log/\*.log
</code></pre>

<p>  注意到星号 * 之前的反斜杠 \，因为 Git 有它自己的文件模式扩展匹配方式，所以我们不用 shell 来帮忙展开（译注：实际上不加反斜杠也可以运行，只不过按照 shell扩展的话，仅仅删除指定目录下的文件而不会递归匹配。上面的例子本来就指定了目录，所以效果等同，但下面的例子就会用递归方式匹配，所以必须加反斜杠。此命令删除所有 log/ 目录下扩展名为 .log 的文件。类似的比如：</p>

<pre><code>  git rm \*~
</code></pre>

<p>  会递归删除当前目录及其子目录中所有 ~ 结尾的文件。</p></li>
<li><p>移动文件</p>

<p> 要在 Git 中对文件改名，可以运行如下命令</p>

<pre><code> git mv file_from file_to
</code></pre>

<p> 其实，运行 git mv 就相当于运行了下面三条命令：</p>

<pre><code> $ mv README.txt README
 $ git rm README.txt
 $ git add README
</code></pre></li>
<li><p>回滚文件</p>

<pre><code>git branch backup // 先备份到一个新分支
git log // 找到要回滚的版本
git reset --hard 版本号 // 回滚
</code></pre></li>
</ol>


<h2><a name="remote"></a>远程仓库的使用</h2>

<p>远程仓库是指托管在网络上的项目仓库，可能会有好多个，其中有些你只能读，另外有些可以写。</p>

<ol>
<li><p>查看当前的远程库</p>

<p> 要查看当前配置有哪些远程仓库，可以用 git remote 命令，它会列出每个远程库的简短名字。在克隆完某个项目后，至少可以看到一个名为 origin 的远程库，也可以加上 -v 选项git remote -v（译注：此为 &ndash;verbose 的简写，取首字母），显示对应的克隆地址。</p></li>
<li><p>添加远程仓库</p>

<p> 要添加一个新的远程仓库，可以指定一个简单的名字，以便将来引用，运行 git remote add [shortname] [url]</p></li>
<li><p>从远程同步信息</p>

<pre><code> git fetch [remote] #下载仓库的所有变动
 git pull [remote] [branch] #取回远程仓库的变化冰河本地分支合并
</code></pre></li>
<li><p>推送数据到远程仓库</p>

<p> 项目进行到一个阶段，要同别人分享目前的成果，可以将本地仓库中的数据推送到远程仓库。实现这个任务的命令很简单：</p>

<pre><code> git push [remote-name] [branch-name]。
</code></pre>

<p> 如果要把本地的 master 分支推送到 origin 服务器上（再次说明下，克隆操作会自动使用默认的 master 和 origin 名字），可以运行下面的命令：</p>

<pre><code> git push origin master

 git push -u origin master //push同时设置默认跟踪分支
</code></pre>

<p> 只有在所克隆的服务器上有写权限，或者同一时刻没有其他人在推数据，这条命令才会如期完成任务。如果在你推数据前，已经有其他人推送了若干更新，那你的推送操作就会被驳回。你必须先把他们的更新merge到本地才能继续。</p>

<p> 此外，当你本地的版本落后于远程仓库，但是你想要用旧版本覆盖远程版本的话，使用</p>

<pre><code> git push --force origin master
</code></pre>

<p> 推送所有分支到远程仓库：</p>

<pre><code> git push [remote] --all
</code></pre></li>
<li><p>查看远程仓库信息</p>

<p> 我们可以通过命令 git remote show [remote-name]查看某个远程仓库的详细信息，比如要看所克隆的 origin 仓库，可以运行：</p></li>
<li><p>远程仓库的删除和重命名</p>

<p> 在新版 Git 中可以用 git remote rename 命令修改某个远程仓库在本地的简短名称。使用git remote rm 命令删除远程仓库。</p></li>
<li><p>检出远程仓库的某一分支</p>

<pre><code> git checkout -b &lt;local.branch&gt; &lt;remote.branch&gt;
 git checkout -t &lt;x
</code></pre></li>
</ol>


<h2><a name="branch"></a>分支的使用</h2>

<p>分支是在开发中经常使用的一个功能。</p>

<pre><code>git branch 列出本地分支
git branch -r 列出远端分支
git branch -a 列出所有本地分支和远程分支
git branch -v#查看各个分支最后一个提交对象的信息
git branch --merge#查看已经合并到当前分支的分支
git branch --no-merge#查看为合并到当前分支的分支

git branch [branch-name] 新建分支,但仍然停留在当前分支
git branch [branch] [commit] 新建一个分支，指向指定commit
git checkout [branch-name] 切换到分支
git checkout -b [branch-name] 新建+切换到该分支
git checkout -b [branch1] [branch2] 基于branch2新建branch1分支，并切换

git branch -d [branch-name] 删除分支
git branch -D [branch-name] 强制删除分支

git merge [branch-name] 将分支合并到当前分支
git rebase [branch-name] 将banch-name分支上超前的提交，变基到当前分支

git branch --set-upstream [branch] [remote-branch] 建立现有分支和指定远程分支的追踪关系

# 删除远程分支
git push origin --delete [branch-name]
git push origin :[branch-name]
git branch -dr [remote/branch-name]
</code></pre>

<h2><a name="tag"></a>标签的使用</h2>

<p>当你完成一个版本的开发，需要做发布的时候，会需要给此次版本打一个表标签：</p>

<pre><code>git tag #列出现有标签

git tag [tag] #新建标签
git tag [tag] #新建一个tag在当前commit
git tag [tag] [commit] #新建一个tag在指定commit
git tag -a [tag] -m 'tag cooment' #新建带注释标签
git checkout -b [branch] [tag] #新建一个分支，指向某个tag

git show [tag] #查看tag信息

git checkout [tagn] #切换到标签

git push [remote] [tag] #推送分支到源上
git push [remote] --tags #一次性推送所有分支

git tag -d [tag] #删除标签
git push origin :refs/tags/v0.1 #删除远程标签
</code></pre>

<h2><a name="log"></a>日志</h2>

<p>有时候需要查看版本的日志记录，以确定、跟踪代码的变化等</p>

<pre><code>git log #显示当前分支的版本历史
git log --stat #显示commit历史，以及每次commit发生变更的文件，每次提交的文件增删数量

#显示某个文件的版本历史，包括文件改名
git log --follow [file] 
git whatchanged [file]

git blame [file] #显示指定文件由谁何时修改过

git log -p [file] #显示指定文件相关的每一次diff

git show [commit] #显示每次提交的元数据和内容变化
git show --name-only [commit] #显示某次提交发生变化的文件
git show [commit]:[filename] #显示某次提交某个文件的内容

git reflog #显示当前分支的最近几次提交


##下面是git log的高级用法##

git log --oneline #把每一个提交压缩到一行

git log --decorate #显示指向这个提交的所有引用（比如说分支、标签等）
git shortlog #把每个提交按作者分类，显示提交信息的第一行。这样可以容易地看到谁做了什么
git log --graph #绘制一个ASCII图像来展示提交历史的分支结构
git log -&lt;n&gt; #限制显示的提交数量

# 按照现实日期过滤显示结果，日期可以使用多种格式，如2015-1-1, yesterday
git log --after="&lt;date&gt;" #在日期之后
git log --before="&lt;date&gt;" #在日期之前

git log --author="&lt;author&gt;" #按照作者(作者的邮箱地址也算作是作者的名字)

git log --no-merges #排除外来的和并提交
git log --merges #只显示外来合并提交

git log master..feature #从master分支fork到feature分支后发生的变化

git log -- xxx.java #--告诉后面是文件名不是分支名

git log --grep="xxx" #按提交信息来过滤提交
git log &lt;last release&gt; HEAD --grep feature #仅仅显示本次发布新增加的功能。

git log -S "xxx"(-G"&lt;regex&gt;") #根据内容(源代码)来过滤提交
git log --pretty=format:"&lt;string&gt;" #自定义输出格式，占位符：%cn-作者名字 %h-缩略标识 %cd-提价日期
git log &lt;last tag&gt; HEAD --pretty=format:%s #显示上次发布后的变动，每个commit占据一行
</code></pre>

<h2><a name="revert"></a>撤销</h2>

<p>在提交了错误的修改或者想撤销文件的变动时，需要以下命令：</p>

<pre><code>git checkout [file] #恢复暂存区的指定文件到工作区
git checkout [commit] [file] #恢复某个commit的指定文件到工作区
git checkout . #回复上一个commit的所有文件到工作区

git reset --hard #重置暂存区和工作区到上一次commit
git reset [commit] [file] #重置当前分支到commit，重置暂存区，但工作区不变
git reset —soft #只回退commit,此时可以直接git commi
git reset --hard [commit] #重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致
git reset --keep [commit] #重置当前HEAD为指定commit,但保持暂存区和工作区不变

#新建一个commit撤销指定commit,后者的所有变化都将被前者抵消，并且应用到当前分支
git revert [commit] 
#回退所有内容到上一个版本
git　reset　HEAD^
#回退文件的版本到上一个版本
git　reset　HEAD^　[file]
#向前回退到第3个版本
git　reset　--soft　HEAD~3   

# 清空未进入暂存区的改动
git clean -f -d
</code></pre>

<h2><a name="cherrypick"></a>选择某些commit操作</h2>

<p>git cherry-pick可以选择某一个分支中的一个或几个commit(s)来进行操作。例如，假设我们有个稳定版本的分支，叫v2.0，另外还有个开发版本的分支v3.0，我们不能直接把两个分支合并，这样会导致稳定版本混乱，但是又想增加一个v3.0中的功能到v2.0中，这里就可以使用cherry-pick了。</p>

<pre><code>git cherry-pick &lt;commit id&gt;
</code></pre>

<h2><a name="conflict"></a>解决冲突</h2>

<p>在rebase或者merge时，有时候会产生conflicts，如果无法auto merge，那么一般有两种处理方式：</p>

<ul>
<li>手动修改冲突的文件：修改完成后，使用git add,git commit或者git rebase &ndash;continue等后续操作即可。</li>
<li><p>使用任一方的文件最为最新文件</p>

<pre><code>  git checkout --ours xx
  git checkout --theirs xx
</code></pre></li>
</ul>


<h2><a name="submodule"></a>Submodule</h2>

<p>当你的工程的部分文件是另一个git库时，可以使用submodule（现在subtree已经替代了submodule）。</p>

<ol>
<li><p>添加</p>

<p> 为当前工程添加submodule，命令如下：</p>

<pre><code> git submodule add 仓库地址 路径
</code></pre></li>
<li><p>删除</p>

<p> submodule的删除稍微麻烦点：首先，要在“.gitmodules”文件中删除相应配置信息。然后，执行“git rm –cached ”命令将子模块所在的文件从git中删除。</p></li>
<li><p>下载的工程带有submodule</p>

<p> 当使用git clone下来的工程中带有submodule时，初始的时候，submodule的内容并不会自动下载下来的，此时，只需执行如下命令：</p>

<pre><code> git submodule update --init --recursive
</code></pre></li>
</ol>


<h2><a name="subtree"></a>Subtree</h2>

<ol>
<li><p>第一次添加子目录，建立与git项目的关联</p>

<pre><code> git remote add -f &lt;子仓库名&gt; &lt;子仓库地址&gt; #其中-f意思是在添加远程仓库之后，立即执行fetch。
 git subtree add --prefix=&lt;子目录名&gt; &lt;子仓库名&gt; &lt;分支&gt; --squash #–squash意思是把subtree的改动合并成一次commit，这样就不用拉取子项目完整的历史记录。–prefix之后的=等号也可以用空格。
</code></pre></li>
<li><p>从远程仓库更新子目录</p>

<pre><code> git fetch &lt;远程仓库名&gt; &lt;分支&gt;
 git subtree pull --prefix=&lt;子目录名&gt; &lt;远程分支&gt; &lt;分支&gt; --squash
</code></pre></li>
<li><p>从子目录push到远程仓库（确认你有写权限）</p>

<pre><code> git subtree push --prefix=&lt;子目录名&gt; &lt;远程分支名&gt; 分支
</code></pre></li>
</ol>


<h2><a name="other"></a>其他</h2>

<pre><code>git help #获取命令的帮助信息
git archive #生成一个可供发布的压缩包
git rev-list --max-count=1 HEAD #查看当前分支的最新rev
git filter-branch -f --env-filter "GIT_AUTHOR_NAME='Newname'; GIT_AUTHOR_EMAIL='newemail'; GIT_COMMITTER_NAME='Newname'; GIT_COMMITTER_EMAIL='newemail';" HEAD #可以修改历史记录中的作者名字和邮箱
git filter-branch --index-filter 'git rm --cached --ignore-unmatch *.sql' # 删除sql文件的历史记录
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[研发招聘之殇]]></title>
    <link href="http://www.rowkey.me/blog/2015/12/31/dev-job-talk/"/>
    <updated>2015-12-31T22:01:02+08:00</updated>
    <id>http://www.rowkey.me/blog/2015/12/31/dev-job-talk</id>
    <content type="html"><![CDATA[<p><strong><em>ps: 本文完成于2015年12月31号</em></strong></p>

<p>对于一个公司来说，要想健康长久的发展，招聘是一个永久的话题。而对于一个互联网公司，尤其是以产品为主的公司来说，研发是招聘中的关键职位，高质量的研发人才也是所有企业都急缺的。一直持有一个观点：招一个优秀的人给他两倍的薪资带来的效果远远大于招两个普通的人。也一直秉着这个观点来招聘。</p>

<!--more-->


<p>今年十月份去西安、武汉两地进行校招，发现了目前很多学生存在的问题(其实之前在前东家参与校招的时候也发现了)：</p>

<ol>
<li><p>技术脱离业界前沿。现在高校里开的课以及实验室用的技术基本都脱离业界，面试了很多学生，他们的技能还千篇一律都是ssh系列。而这种技术选项，目前也就在传统it行业流行，互联网公司早就摒弃了这一套。此外，分布式缓存、消息队列等技术更是鲜少有人涉及。更为遗憾的是，对于服务端工程师、前端工程师、客户工程师等职位应该具有的技术栈，在高校里也缺乏相应的课程和相关的人来指导(大公司一般都设置有入职培训针对这一点)。</p></li>
<li><p>基础素质欠缺。对于应届生来说，基础素质是最关键的一点，项目经验是加分项，但不是必需和最关键的。很多学生被问起hashmap的实现原理以及怎样解决冲突时就不知所云(明明就是数据结构课讲过的)，被问到tcp/ip、操作系统时更是乱七八糟。也有些学生做过很多项目，自认为经验丰富，但当被问起使用的技术比如spring mvc的原理、mysql的索引机制时，没有任何思路。其实项目经验、工具这些东西决定了你的下限，你的基础知识和素质才决定了你的上限。互联网行业需要的是学习能力强、基本功扎实的优秀工程师，而非很多传统IT企业需要的螺丝钉。</p></li>
<li><p>没有畏惧心。这一点可能因人而异，毕竟有些人的性格就是桀骜不驯。但是在研发这个圈子里，大神太多，是大神但比你还努力的人也太多。技术也太广，任何一门技术都很难精通。做为一个研发工程师，你必须对所有人、所有技术都有一颗为畏惧心。每个人都有亮点值得你学习，每一种技术都需要你下大功夫才能精通。记得有些学生在基础知识被问得绊绊磕磕的时候，急得号称自己精通java，但试着去问了，却发现很多知识都似是而非。说到这里，最近公司的几个实习生让我体会挺深刻的，有些学生的确会对技术有畏惧感，很谦虚，抱着一种学习的态度；但是也有很多学生自视甚高，甚至有人待了一周(很多东西都没接触到)就离职，觉得我们这技术水平入不了他的法眼，总是拿一个我听都没听过的公司跟我说这个公司很厉害的，早就给offer了等等。不敢说我们公司的技术业界领先，至少我们做出了拥有几亿用户的产品。人，还是应该有一颗畏惧心，不论对人还是对事。</p></li>
<li><p>知其然不知其所以然。这一点的反面(知其然更知其所以然)对于研发人员其实是最关键的素质。能做出东西来只能证明你上手能力强，但并不代表你学习能力强。学习能力强，是指的能够快速吸纳理解新的知识，融汇贯通。比如，就拿最简单的spring ioc来说，用过的人基本都知道大概是个什么事情，但是抛开spring，让你自己去实现ioc，很多人估计就不知所措了。</p></li>
</ol>


<p>校招毕竟只是研发招聘的补充，最关键的还是社招。但是面试了很多有经验的工程师之后，却也发现了很多问题。除了上面校招提到的一些，最令我印象深刻的就是薪资。本科毕业一两年的，之前在一些不知名的公司工作过的人，动辄就漫天要价。好吧，我觉得可能是真的很优秀，那倒也匹配的上。结果，面试过n次这种人之后，我发现帝都的薪资真的不能拿常人的眼光来看，也算是给我这种来自杭州的人开了眼界。毕竟帝都这地方互联网企业一大把，舍得给钱的、面向vc的创业公司也一大把。你觉得不值，还有一大批公司觉得值。这种现象，在今年上半年达到了顶峰。不能说正确与否，只能说市场如此，带来的效应就这样。</p>

<p>关于招聘，是一门学问。自己非专业的，所以很多东西肯定看的不够清楚。但对于研发招聘，自己经历过很多次被面试，也面试过很多人。有自己觉得好的面试形式，也有自己很嗤之以鼻的。</p>

<ol>
<li>N轮算法题目面试。这种形式是被微软和谷歌所推崇的，不一定好，但是至少客观，不会掺杂面试官的主管因素，而且据说后续的结果证明了算法好的人在工作上的成绩好的概率非常大。自己曾有幸经历过一次，由于自己对算法不感兴趣，也一直没刷过题目，所以结局很惨烈。不过，自己却也信服口服。应对这种面试，能做的就是做大量的题目(至少《算法导论》上的算法都要搞明白)，总结方法，锻炼自己的思维。当然参加一下acm比赛也不妨为一种好方法。剩下的，就看你的天赋和运气了。</li>
<li>掺杂计算机基础知识、算法以及项目经验的面试。这种形式是国内大部分公司采取的。优点是能从多方面考察面试者的技术水平；缺点就是容易被面试官的主观因素所影响，尤其是很多水平很差的面试官，或者是面试官和被面试者方向不对路。</li>
<li>软件设计。这个不同于算法题目，一般是面向某一场景的软件设计题目。每个人提交代码，然后根据代码的效率、模式设计等判定结果。现场面试的时候，面试官当场提出问题、需求，现场进行优化编程。这个面试方式，我见过某土豪日企(应届生起薪30W+)采用过。这个我暂时说不上是好是坏，应该是针对特定企业的工业场景的一种面试方式。</li>
<li>现场结对编程/ppt讲解。记得之前看过一篇文章讲世界上研发面试最难的公司是Thoughtworks。面试官和你结对编程，然后再进行圆桌会议等等一系列复杂的流程。不过，在国内的thougtworks也许是为了迎合中国国情吧，倒是没见过这么招聘。记得校招的时候，初试出一道软件设计题目，你解决好后提交代码，现场面试的时候，就针对这个问题进行ppt演示，面试官当场提出问题，看你的应对。</li>
<li>只看学历、学校。这种面试方式，我知道的一次貌似只有hw校招。当时听同学说，面试官声称不关心你会不会或者专业对不对口，只要学校符合，其他的都能培养出来。当然，我相信，这只是某种形势下hw的招聘策略，毕竟hw里面招的牛人还是大有人在的。尤其是2000年左右，进hw那可是人人羡慕的。</li>
<li>群面。这种方式多见于非研发职位。不敢说在研发面试中出现好不好，但至少对我来说，如果有公司这么干，我肯定去都不去。码农们都不擅长和人打交道的好不。。。虽然，这不一定算是件好事。不过，研发总归还是要看技术的么。</li>
</ol>


<p>目前，我们公司采取的是国内最流行的第二种，基础知识考查这个人的基本素质，也就是看看能否胜任当前工作；项目经验看看这人做过的东西有没有消化、深入理解，看这人的学习主动性、研究问题的深度、对技术的热情如何；开放性问题看看这人是否足够聪明。坚决杜绝问RTFM的问题，也坚决不出什么脑筋急转弯。也会采取一些措施，比如两轮平行面试，来避免掺杂主观因素。</p>

<p>那么怎么定义一个优秀的研发工程师呢？我们希望招到的研发人员或者说我们觉得优秀的研发人员，抛开具体技术来说，共性应该是这样的：</p>

<ol>
<li><p>聪明、思维灵活。研发最重要的一点就是要聪明，这个观点貌似雷军也说过。很难想象一个不聪明的人是如何解决复杂的工程问题的。而什么叫聪明呢？我们现在在面试的时候，会随机从现实的项目中出一道曾遇到过的问题。面试的很多人都会说没遇到过、不知道。其实最终的答案并没有对错，没遇到过这个问题也是我们最希望的，如果你能给出解决方案或者思路才说明你有足够解决现实问题的能力。</p></li>
<li><p>对技术有热情。只有对技术有热情，才不会把工作仅仅当做工作，还会当做乐趣。这样才会对接触过的技术能深入研究下去，快速学习，快速成长起来。相比起聪明，这一点也是至关重要的。如果仅仅是聪明，而对技术不具有热情，那么很多人会浅尝辄止，不求甚解，最后聪明反被聪明误。而没有那么聪明的人如果足够有热情，找到合适的方法，努力学习原理层的东西，也会很快成为技术大牛的。</p></li>
<li><p>基础知识扎实。和上面校招那一部分说的一样，基础知识决定一个人的上限。如果一直停留在表面应用业务的开发，不接触到底层计算机原理，那么即使你在nb闪闪发光的大公司里，你也是可有可无的一个人，价值慢慢会趋于0。而基础知识扎实，那么学起其他的业务层技术也根本不会成为问题，上限也会很高。</p></li>
<li><p>有执行力。执行力在某种方面说就是结果导向。见过很多聪明、对技术有热情的人，却总是钻牛角尖，容易陷在一个细节上出不来，这一点自己也不例外。但是有人会考虑到整个项目的进度，做好控制，先用快速的方案实现，后续再调整和优化；有些人却往往为了一个细节，纠结来纠结去，最后造成进度延误，这就是一种没有执行力的表现。当然，如果你效率高，那么你随便钻牛角尖，不然你应该以全局为重。</p></li>
</ol>


<p>以上是今年招聘的一些感悟，希望来年能招到更多符合期望的人才。</p>
]]></content>
  </entry>
  
</feed>
