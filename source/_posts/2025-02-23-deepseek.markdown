---
layout: post
title: "DeepSeek简单分享"
date: 2025-02-23 19:29:34 +0800
comments: true
categories: genai
---

本文内容来自一次内部分享。主要是对目前非常火的DeepSeek的一些自己的认知和理解。

<!-- more -->

## DeepSeek是什么？

DeepSeek是一个由中国人推出的媲美ChatGPT O1能力的开源推理大模型，其中文能力更强，而且由于背后公司数据的特点，在金融方面具有优势。这里所说的推理大模型是相对于之前的非推理模型，更加强化了推理、逻辑分析和决策能力，可以看做是把之前的CoT能力直接做到了模型里。DeepSeek本身是包括V3和R1两个模型，参数都达到6000亿，也就是现在市面上很多人说的满血版。而DeepSeek开源的几个蒸馏版本的模型其实本质还是qwen和llama，只是用了R1的推理数据做了微调。

## DeepSeek生态位

综合了各种榜单和一些评测，并基于公司实际使用的经验，对现在主流的大模型做了如下梯队排名：

<img src="/post_images/deepseek/dp_rank.png" width="450"/>

在选择模型时需要注意：

- 开源模型可以私有化部署提供无审查的服务
- 国内模型在中文上有优势

通过这个梯度，也可以看到DeepSeek并不是能力最强的，但R1确实是国内最好的推理模型。而非推理模型国内的通义千问是能力最强的。这里需要提到的一点就是Kimi其实也和DeepSeek差不多同一时间推出了推理模型的，能力也没有差太多，但由于不是完全开源的，所以被DeepSeek给完全盖住了。

## DeepSeek为什么这么火?

如第一部分所说，本质上DeepSeek是一个中国公司做到了O1水平并且开源了的推理大模型。具体来说，之所以它这么火有以下几点：

- ChatGPT o1出来后，给业界出了一道题，然后DeepSeek给解出来了，并且是以低成本的方式实现了，甚至还给开源了。
- 对于国内来说，由于zz原因，很长一段时间是无法使用国外的第一梯队大模型的。所以，有了DeepSeek这种能用的模型，自然是迅速出圈。
- 对于国外来说，则是高估了领先中国的速度，低估了中国的追赶速度。


## 为什么是DeepSeek?

国内外很多大模型厂商，为什么是DeepSeek做出来了呢？

- DeepSeek背后是幻方量化，这家公司号称多内私募量化四巨头，非常赚钱，有一年就捐了3个亿做慈善。虽然DeepSeek是相对独立的一家公司，但其中的关联肯定小不了，所以大概率是不缺钱的，也不是奔着赚钱去的。因此，可以类似高校一样单纯的做研究。与之相比，Kimi就有商业化的诉求，所以能看到Kimi在大量的投放广告。
- DeepSeek的招人门槛很高，虽然创始人是浙大的，但团队成员基本上是清北级别的。
- DeepSeek曾号称有国内最多的A100显卡。
- 创始人梁文峰是很有技术追求的一个人，不管是量化还是大模型，据各种报道，都是自己亲身在一线写代码、写论文的。
- 我自己的认知，其实OpenAI推出o1后，大家都在研究，都在探索，方法也都有区别，DeepSeek这次做出来是有一点运气成分。


这里还想提的是，春节期间所谓的国运一说，我觉得如果DeepSeek在不长的时间能追上甚至超过o3，那真的可以说国运了。

## DeepSeek的创新

DeepSeek由于受限于显卡的性能（H800），通过工程优化上的创新提升了算法效率，从而也大大降低了成本。

- DeepSeekMoE：采用了大量细粒度的专家，因此推理时，能大幅降低成本。
- 负载均衡优化：采用Auxiliary-loss-free算法提高了MoE路由的效率。
- 内存优化：重计算、使用CPU内存和参数共享
- 通信优化：DualPipe
- 计算优化：FP8混合精度训练
- 其他：MLA(多头潜在注意力)、MTP（多Token预测）、GPRO（强化学习算法）等
- NSA：原生稀疏注意力，长文本能力

## 使用

推理模型是有使用场景的，适合需要深度思考的场景，如设计、审查、推理、复杂计算等。如果让其做一些简单的任务，如实现代码，可能会思考来思考去，反而降低效率。结合推理模型+非推理模型是现在一种常用的方式，如DeepSeek R1 + Claude 3.5 sonnet就是使用R1来做方案设计，使用Claude来写代码。

不同于之前的非推理模型，推理模型的提示词跟侧重于描述清楚任务目标，过多的引导反而是干扰。

此外，通过DeepSeek对蒸馏模型的证明，一些行业模型也可以通过DeekSeek R1的推理数据来微调，实现蒸馏的效果。