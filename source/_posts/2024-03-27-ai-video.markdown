---
layout: post
title: "如何使用AI生成长视频？"
date: 2024-03-27 19:29:34 +0800
comments: true
categories: GenAI
---

今年最火的AI技术应该是OpenAI在春节期间发布的Sora了。相比起其他视频生成产品就3、4秒的时长，Sora是碾压式的存在。但Sora没有对外开放，所以要生成长视频，暂时也没有其他完整的好的方案。综合各种资料来看，目前最可行的方案应该就是：写剧本/分镜——>生图——>生视频->视频拼接，本质上就是通过多个短时长的视频组成一个完整的长视频。下面就详细讲述一下。

<!-- more -->

详细的步骤：

1. 脚本确认：拆分镜头，初步确定生成内容。这一步就是需要针对要生成的内容撰写剧本，并拆分成数个镜头。
2. 单帧图片
    - 使用Midjourney（V6的语义理解能力有明显提升），DALL-E 3（语义理解能力较好）进行文/图生图
    - 审查已生成图片中的细节问题，调整、更换合适的主题内容，并重新生成符合要求的图片
    - 使用PS处理图片中的不合理细节，添加未被AI生成的元素
    - 使用Stable Diffusion图生图进行图片放大和细节优化
    - 使用PS进行图片的最后优化
    - 人物不一致可以使用换脸进行统一
3. 图生视频
    - 使用RunWay/Pika/SVD/Animatediff实现图片生成短视频，可以综合利用各个视频服务的优点，如RunWay的运动笔刷、Pika的面部表情等，其中Pika还可以对局部视频进行重绘。
4. 视频合成
    - 使用剪映/iMove进行短视频片段合成与特效转场处理
    - 添加配音和配乐，根据卡点节奏进行视频剪辑与重新生成内容替换(如需要声音)

每一步使用的软件以及关键点如下：

1. 场景描述需要分镜，这里用GPT4来做场景拆解，场景的描述提示词模版如下：

    ```
    需要将一段场景的描述改写成一个时长30秒的分镜脚本，再根据每个分镜脚本的文字描述生成单张图片，之后通过图片生成视频，最后将视频进行拼接成最终的成品视频。

    场景描述如下：

    xxx

    分镜脚本结构如下：
    ‒ 序号：递增数字 
    ‒ 景别：远景/全景/中景/近景/特写 
    ‒ 风格：真实影像风格/日本动漫风格/水墨画风格等（在Dalle3里无法直接写作者的名字，比如新海诚，但Midjourney是可以的。） 
    ‒ 角色：具体到是什么样的角色，有什么特殊的颜色、道具、服饰等等。 
    ‒ 环境：森林、家、海边等等 
    ‒ 镜头移动：描述每个分镜中镜头的动作或变化 
    ‒ 比例：16:9/2.35:1等等

    分镜要求如下：
    1. 每个分镜时长4s
    2. xxx
    3. 内容和风格需要xxx

    每一个分镜后续会通过Midjourney进行图片生成。现在请给出每一个分镜脚本以及对应的Midjourney提示词，以Markdown Table的方式输出。
    ```
2. 图像需要保持一致性，包括人物和周围场景
    - DALL-E 3：一致性可以通过GenID
    - Midjourney V6: 最新版有了ref，一致性功能

3. 图生视频这一步，需要结合多种视频软件一起使用。每个软件的特点如下：
    - [Pixverse](https://pixverse.ai/): 免费无限生成，有一致性角色功能(效果一般)，可用于无限生成视频后择优选取
    - [Runway](https://app.runwayml.com/): 每次生成消耗5积分，做角色动作和部分运动镜头会好一点
    - [Pika](https://pika.art/): 每次生消耗10积分，做角色动作和面部表情
    - [Stable Video](https://www.stablevideo.com): 每次生成消耗10积分，适合生成风景视频

    换脸的话，可以使用roop或者facefusion，这里有其colab版本：<https://github.com/dream80/roop_colab>。

4. 视频拼接，可以使用剪映或者苹果电脑上的iMovie。

通过以上方案，基本可以实现长视频的生成，但目前AI生成视频的崩坏率极高，可控性差，所以需要生成很多视频，从中选取最符合预期的。